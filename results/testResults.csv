1,1989,Skilled financial planning: the cost of translating ideas into action,interviews and questionnaires
1,1989,Skilled financial planning: the cost of translating ideas into action,DNE: this research paper utilizes financial analysts as a data source and observational studies as the data gathering methodology.
1,1989,Skilled financial planning: the cost of translating ideas into action,DNE: this research paper utilizes interviews as a data source and think-aloud sessions as the data gathering methodology.
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,conceptual modeling
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,conceptual modeling
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,interviews
1,1989,Skilled financial planning: the cost of translating ideas into action,DNE: this research paper utilizes financial modeling systems as a data source and goms models as the data gathering methodology.
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,think-aloud sessions
1,1989,Skilled financial planning: the cost of translating ideas into action,DNE: this research paper's data source is mouse and keyboard interaction data.
1,1989,Skilled financial planning: the cost of translating ideas into action,DNE: this research paper's data source is mouse and keyboard interaction data.
1, 1989, Skilled financial planning: the cost of translating ideas into action, interviews, Research paper:
ABSTRACT We use GOMS models to predict error rates and mental times for translating financial concepts into equations in two widely used interface representations. The first of these common to spreadsheet packages is characterized by non mnemonic naming and absolute referencing of variables. The second common to nonprocedural command driven software is characterized by mnemonic naming conventions and relative referencing of variables. These predictions were tested in an experiment using experienced financial analysts. Although the interface that allows mnemonic and relative names called keyword takes longer overall it produces seventy five percent fewer simple errors and requires less mental effort. Given the overall serious cost of errors in financial models we conclude that interfaces having the keyword representation are far superior. INTRODUCTION A recurring prescription in user interface design states that the interface should use terms and show a display that fit the objects and actions in the users mental representation. Translating thought into action takes time and produces errors. If the interface requires a significant translation of the users mental representation we can expect an increase in mental time and errors. Card Moran and Newell analyzed how skilled users of text editing software translate the tasks to be performed into actions on the system. They did a similar analysis for the routine components of computer aided circuit design. Their research demonstrated that users break down tasks into small units of approximately 10330 seconds. It also showed that users performance in text editing and in the routine components of circuit design were similar both in terms of error frequency and command execution time. concepts into different kinds of interface representations is warranted. This research shows how interface features affect the performance of skilled financial model builders. Performance is measured in terms of execution times and the frequency of both simple and conceptual errors. The research was executed in four steps 1. Task Analysis. The task of building financial models was characterized by breaking it down into its essential operations independent of any given package. This characterization made evident the importance of describing cells referred to in formulas into different types. These cell types are used in different combinations inside common financial formulas in order to represent special time relationships among generic financial variables. 2. Analysis of the Interface Representations. Two common interface designs IFPS and Lotus 1 2 3 were analyzed for differences in how one refers to cells in formulas. The key differences are the mnemonic constraints of the cell naming conventions and whether the cells are referred to by their relative or absolute locations. 3. Cognitive Modeling. GOMS models were built to represent the steps that skilled financial model builders are likey to follow when writing model formulas. These cognitive models served as the basis for making predictions on the relative frequency of errors among cell types in different representations. They were also used for predicting and explaining the amount of mental time needed to translate financial concepts into formulas. 4. Performance Evaluation. We conducted an experiment with 31 experienced users of financial modeling systems and compared their performance with our predictions. Since financial modelling systems e.g. spreadsheet packages business modeling systems financial modelers etc. are the second most utilized end user software packages in business after word processors and are claimed to be the most frequently used among senior executives an analysis of the cost of translating financial TASK ANALYSIS The basic structure of financial modeling systems is the two dimensional matrix1. Model builders typically use one dimension usually columns to represent time periods and the other rows to represent financial variables such as categories of costs and revenues. The process of building these models is basically the same for all packages 1. Rows variables and columns time periods are labeled. This produces a grid of cells. 1Some packages allow adding one or more dimensions in order to include different business units products sales regions etc. 2. The content of each cell in the grid is specified. A cell may contain either numerical values or formulas. Formulas make reference to the content ofother cells and include mathematical andor logical operators and numerical constants. Most formulas make reference only to cells in the same column where the formula resides. For example the formulas to represent the relationships shown below only require cell references in the same column Assetst Current Assetst Fixed Assetst Profit t1t Revenues t1tCosts t1t Notice that the two relationships represent different concepts in time the first equation shows a relationship among variables at a given time t these are called level variables while the second shows a relationship between events within a period of time called flow variables. When financial relationships mix levels and flows the formulas that represent these relationships are likely to include cells that are not in the same column. Another important distinction for the financial planner is between income flows and cash flows. Income flows are the result of using the accrual basis of accounting. In this. view revenues are recognized when the transaction related to the earning process occurs that is when the sale is made or the service is rendered. An attempt is also made to match expenses with associated revenues. This method produces a good estimate of the profitability of the firm during a period of time. It does not however provide information about cash flows because income flows such as revenues occur before the receipt of cash. The timing of cash flows is crucial for decisions regarding the allocation and acquisition of funds. Therefore since income flows are important for assessing profitability and cash flows are indispensable for performing sound financial planning both are included in financial models. They are often confused by the beginning analyst and present a mental burden for the experienced one. An example with timing differences between income flows and cash flows is Receipts t1t .2Revenues t3t2 .8Revenues t2t1 The formula to represent this equation has to refer to cells in a different column and in a different row with respect to the cell where the equation resides. Every time there is a time lag between income flows and cash flows the formulas have to include cells in different rows and columns. These are difficult to think about yet they represent the basic task of financial planning. They are even more difficult to specify in current software. This research shows that the degree of difficulty a planner has in referencing cells in a formula is related to the kinds of cells those in the same row and column and those in different rows and columns. We refer to these types as type SR type SC and type DRC. The research also shows that how these cells are made available to the planner by the software package interface can add to the users task difficulty. INTERFACE REPRESENTATIONS References to the content of other cells cell references can be specified and represented using different cell naming conventions. There are two main differences in the cell naming conventions of financial modeling systems Mnemonic names vs Non mnemonic names Relative references vs Absolute references Mnemonic cell naming uses the name of the variables such as REVENUES or COSTS to specify rows. Non mnemonic cell naming only uses the row numbers. An absolute reference states the position of the cell by using its overall coordinates. A relative reference specifies the position of the cell by describing its location with respect to the cell where the formula resides. Four cell naming conventions result from combining these two cell naming dimensions. Figure 1 shows examples of each. REVENUES 1 COSTS 2 PROFITS 3 4 Year 1 A ..... Year 2 B J Mnemonic Absolute REVENUES 1 Relative PREVIOUS REVENUES Non mnemonic A1 C1R2 Figure 1. Cell naming conventions for referring to the cell Revenues in Year 1 at the arrow head when residing in Profit Year 2 the arrow foot This research investigates only two of these four naming conventions the two most widely used in financial modeling systems. IFPS Interactive Financial Planning System embodies the relative mnemonic cell naming convention here called keyword. Lotus 12n3 uses absolute non mnemonic cell naming here called positional. These two packages exhibit other differences such as style of interaction e.g. command language vs direct manipulation but it is only the users performance with respect to cell referencing that is studied here. COGNITIVE MODELS The next step of this work involved building GOMS models to simulate skilled performance. The models were built by observing experienced financial analysts in a pilot study. In this study the participants were asked to think aloud while building typical financial planning models. During the pilot study model building behavior occurred in two general phases the acquisition of the appropriate information and the translation of this information into formulas using one of the two interface representations. Skilled financial analysts exhibited the same type of problem solving behavior during the acquisition of information. This behavior is independent of interface representation because the acquisition process occurs before the representation is utilized. Problems in the acquisition phase will be referred to as conceptual errors in the performance evaluation section. In the translation phase the mental effort cost of translating financial concepts into formulas should be affected by the different interface representations. The GOMS models drawn from the pilot study specify different sequences of mental operations for the two representations studied here. An example of how the same task is executed with each representation is shown in Figure 2 the full fledged GOMS models are discussed in. errors caused by overload in working memory were expected to correlate with peak working memory loads called overload errors. These predictions assume that as the peak number of working memory resources utilized gets closer to its limit the likelihood of error behavior increases. Therefore error hypotheses can be built by comparing the peak working memory loads of different cell types in each representation. Figure 3 shows the working memory loads for cells type SC and DRC with the keyword representation when executing the GOMS model of Figure 2. GOAL WRITE OPERATOR CELL Keyword Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE LAG SPECIFY LAG POINTER SPECIFY KEYWORD WRITE LAG KEYWORD WRITE VARIABLE NAME ..if Jag Positional Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE COLUMN Sel GOAL GET COLUMN NAME SPECIFY COLUMN NAME WRITE COLUMN NAME GOAi CALCULATE COLUMN NAME SPECIFY LAG POINTER CALCUlATE COLUMN NAME WRITE COLUMN NAME SPECIFY ROW NUMBER WRITE ROW NUMBER ..ifno lag ..if Jag Figure 2. GOAL WRTIE OPERATOR CEIL The execution of this GOMS model translates information about the internal representation of the financial problem into cell types SC and DRC. Figure 2 shows that the positional representation requires more steps than the keyword representation because the model builder has to perform extra steps in order to translate variable names into cell coordinates. Performance predictions were made of the translation process from the GOMS models 1 Mental times for executing error free equations were estimated by simply counting the number of mental operations required for each cell reference and 2 The frequencies of simple Keyword Naming Cell type SC I  1  .. Operetor Pointer Pointer VarName Pointer VerName r ............................. Time Cell Type DRC ................................ .. ...  ........................................... Kev..ord Et S Log  Log  Kev...rd ........... tl t 1 Operator VorNome VorName VorNome Pointer Pointer Pointer Pointer Pointer VorNemef .................. Figure 3. Working Memory Loads for Cell Types SC and DRC for the Keyword Representation The horizontal axis in both charts represents time but each interval does not represent the same length of time. The intervals were chosen only to represent when the content of working memory changes. The vertical axis shows a rough estimation of the number of pieces of information kept in working memory for translating concepts into formulas. It does not show symbols for other goals that are also kept in working memory because they are the same for both cell types. Figure 3 shows that cells of type DRC have greater demand for working memory than cells of type SC. We expect higher overload error rates for DRC than for SC in the keyword representation because the model builder has to remember and translate the concept of time lag into the formula when writing cells of type DRC. The predictions made with the GOMS models for overload error rates are shown in Figure 4. Similar predictions were made for error free mental times see for details. A detailed task analysis is made for each cell type and each interface. The number of steps required is expected to correlate with the time spent performing the task. Overload Error Rates Predictions Positional Narnini Keyword Namini Type SR Type SC Type SR Type DRC Type SR Type DRC Type SR Type SC Type SR Type DRC Type SC Type DRC Both representations Positional Type SR Keyword Type SR Positional Type SC Keyword Type SC Positional Type DRC Keyword Type DRC Supported by empirical results Figure 4. Overload Error Rates Predictions In sum it was expected that subjects using the keyword interface would make fewer overload errors and need less mental time than subjects using the positional interface because of the additional translation effort required with the positional representation. In contrast execution time for the keyword subjects was expected to be longer because subjects are required to write more characters. Similar mental effort estimations were found by Olson and Nilsen in their study of cell referencing in positional style spreadsheet packages. They showed that substantially less mental effort was required for pointing to the cell as opposed to calculating the coordinates. This research expands the essence of these results by making both time and error predictions based on the estimation of the mental effort involved and by incorporating the characteristics of the task of building financial models into these estimations. PERFORMANCE EVALUATION The core of the comparison is the p.erformance of users on a keyword based interface versus users on a positional based interface. Keyword subjects were experienced FPS users positional subjects were experienced Lotus 12s3 users. Each subject was asked to write equations using different cell type combinations. The length of the equations was restricted to equations containing only two or three cell references. Subjects. Thirty one experienced users from eleven firms served as volunteer subjects. The subjects work fulltime in Fortune 500 financial and industrial firms in Michigan and Ohio. Participating organizations were asked for individuals with the following experience a at least two years working in accounting or finance in their organizations and b at least one year using either Lotus 12i3 or FPS. Twenty one subjects were Lotus users positional ten were keyword users FPS. Lotus users had an average experience of 7.3 years in accounting or finance whereas FPS users had 6.3. All subjects had more than one year experjence with their respective package. Ninety four percent of the subjects reported using the package at least twice a week with the remainder using it less often because they had recently been promoted to supervisory positions. Practice and Test Problems. The subjects were asked to solve the problem with pencil and paper. This eliminated the differences in interactive style presented by the two packages. Subjects read a description of the problem aloud from an instruction booklet and were then given a summary of data and a model skeleton. The summary of data detailed all the input data for building the model. The model skeleton was a worksheet with labeled columns and rows that had empty rows in which the participants wrote the missing equations or data. There were five practice problems and six test problems with 24 empty rows. Procedure. The experimental sessions were one hour and a half long and consisted of oral instructions practice problems test problems two tests of writing speed and a two part questionnaire. The whole experimental session was videotaped. During the test and practice problems subjects read a description of the problem and the objective for building the financial model. They then picked up the model skeleton and the summary of data. They were asked to write one variable at a time and to avoid jumping from row to row. After each problem subjects were given feedback on their solutions. Writing speed tests were administered to control for differences in subjects writing speeds. In the tests subjects were asked to write the correct answers for the test problems by copying them from a sheet of paper and by listening and transcribing items from a tape. As an additional experimental control background data on the subjects was collected. RESULTS AND DISCUSSION Errors. Errors were classified by two judges into two categories overload errors such as forgetting part of a cell reference or misremembering the cell name and conceptual errors such as specifying the overall formula incorrectly. Of the 120 errors five were judged to fall in either category and therefore were not included in further analysis. Of the 115 remammg errors 53 were overload errors and 62 conceptual. As predicted interface representation had an impact on overload error rates p.01 but not on conceptual errors p.96 analysis of variance of the logit transformation of error rates for both overload and conceptual errors. These results provide support for the claim that conceptual errors are expected to occur before the translation into the interface representation starts. In contrast the frequency of overload errors for positional subjects was 4.5 times greater than for keyword subjects 5.9 vs. 1.3 and almost the same for conceptual errors 5.4 vs. 4.2. Figure 5 shows the distribution of both types of errors for each cell type in each interface representation. For overload errors cell type DRC was more error prone than SR and SC as predicted p.05 and p.01 Tukey Kramer test but contrary to expectations there was no significant difference between SR and SC. The causes for this result discussed further in have to do with a misassumption in our GOMS models about how much effort was required in reusing a piece of information that was used in an earlier step. There was no interaction between cell type and the interface representation p.20. This means that cells that are difficult to translate in one representation are also difficult in the other. Finally the difference in error frequency between the two representations for each cell type was highly significant for SC p.01 slightly significant for SR p.10 and not significant for DRC as predicted by the cognitive models shown in Figure 4. Percentage of Errors Overload Errors I ISR 8 DFC Positional 81 Keyword Conceptual Errors Percentage of Errors I ISR 8 DR rmPositional Keyword Figure 5. Overload and Conceptual Error Rates For conceptual errors cell type SC has a significantly lower error rate than type SR and DRC p.01. Our task analysis shows that cells in the same column type SC represent straightforward relationships among both level and flow variables. On the other hand cells of type DRC are required for formulas that represent time lags and cells of type SR are included in formulas that mix levels and flows and in formulas that involve recursion. It was therefore expected that more conceptual difficulties would occur when formulas include these more complex financial relationships. If interface representation does not affect the conceptual process executed before translation then we should expect no interaction between representation and cell types for conceptual errors. Contrary to this reasoning the lower graph in Figure 5 shows a strong interaction between cell type and representation for conceptual errors. This interaction may be explained by other interface features of the two packages used in the experiment. IFPS encourages the user to think in terms of recursion by automatically propagating formulas this may explain why the frequency of conceptual errors is smaller for positional subjects in type SR which is usually used in recursive formulas. On the other hand IFPS makes it difficult to write equations with cells of type DRC by encouraging this same recursive thinking it is conceptually difficult to formulate in a single equation both initial and steady state conditions this may explain why IFPS subjects had so many conceptual problems with DRC compared to Lotus users. Times. Execution times for formulas with two and three references were between 7 and 35 seconds for both representations. These times are similar to the unit task times found by Card et al in their studies of text editing and circuit design. Figure 6 shows the average time needed to execute each cell reference with both representations. Even though execution time is longer for the keyword representation most of this time is spent writing because more has to be written. We were interested not only in total times but in how much mental effort was involved in each representation. We calculated mental time by subtracting from the execution time the raw writing time which was estimated from separate writing speed tests. The results show that the positional representation forces skilled financial analysts to spend a little more than 50 of the time in mental operations versus only 20 in the keyword notation. Average Time per Cell sec Writing Mental Positional 3.8 3.9 Keyword 8.9 2.2 Total 11.1 Figure 6. Error Free Execution Times General Statistics Mental times for formulas with two and three cell references were between 1 and 10 seconds for the keyword representation and between 3 and 22 seconds for the positional notation. The mental time spent in each cell type was estimated by regressing the total mental time per formula against the number of each cell type in each formula The results in Figure 7 show large differences in the mental effort required for each cell type in each representation and support the hypotheses generated by the GOMS models. Mental Times Regression coefficients standard error Positional Keyword Long SR Short SR SC DRC R2 N 3.34 .38 2.09 .12 9.25 .53 4.90 .96 1.25 .75 1.16 .19 4.88 .72 Figure 7. Mental Times It is important to clarify that the keyword representation has two methods for writing cells of type SR. In the short method the user only writes the keyword e.g. PREVIOUS while in the long method both the keyword and variable are specified. The mental times for these two methods indicated that the GOMS models can predict times and errors for these variations if the task is accurately represented. Finally mental times for each cell reference type and each representation have a high correspondence with overload error frequencies but not with conceptual error rates. This can be explained by speculating that in financial model building longer mental times are associated with increased effort and higher memory loads some of which break down and produce simple errors. Figure 8 shows both the overload error rates and the mental times in a logarithmic scale. IO Mental Time  Overload Error Rate 100 gTime sec. 10 0 below 0 Errors 0.1  0.1 Lotus Lotus Lotus  IFPS  IFPS  IFPS  SR S CFC SR S CFC The mental time and or rate for IFPS SR are composite of both short SR and long SR therefore they are not comparable to the mental times and overload error rates for the other cell references. Figure 8. Overload Error Rates vs Mental Times CONCLUSIONS Interface design decisions have grave consequences for the effort involved in difficult mental woik that is supposed to be made easier with software. In particular we found that for financial modeling systems an interface that requires the user to calculate the cell coordinates imposes a mental workload that produces errors and increases the time attributed to mental effort. Even though the total time to enter a mnemonic cell reference is longer it is not time involved with hard work. If one takes into account the cost of errors both the time to correct errors and the intangible cost of undetected errors the keyword representation is considered far superior. A second far more reaching outcome of this research is a demonstration that the GOMS model formalism is useful in predicting both time and error performance. It illustrates its potential in interface design. The user interface representations built into the GOMS models are abstractions of two common properties designed into financial modeling systems. They could readily have represented decisions of unbuilt software. The results of the experiment confirmed the cognitive load predictions of the GOMS models thereby supporting the worth of this analysis method for evaluating design decisions as well as differences in existing software packages. Finally the research in this paper extends the application of the GOMS model to a highly complex problem solving task. It demonstrates how it can be applied to the routine cognitive portions of the task and provides insight into how the users internal problem representation interacts with the external constraints of the software package. ACKNOWLEDGEMENTS This research was supported in part by a University of Michigan Rackham Grant. The authors would like to thank Alan G. Merten Raymond R. Reilly and J. Frank Yates for their valuable comments and suggestions throughout the execution of the research. Special thanks are due to the financial planners who took part in this study on a purely voluntary basis. Finally thanks to Diane L. Morrow Lerch for her help in data recording and analysis.
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token. This research paper's data gathering methodology or methodologies are [MASK].
1, 1989, Skilled financial planning: the cost of translating ideas into action, interviews, Research paper:
ABSTRACT We use GOMS models to predict error rates and mental times for translating financial concepts into equations in two widely used interface representations. The first of these common to spreadsheet packages is characterized by non mnemonic naming and absolute referencing of variables. The second common to nonprocedural command driven software is characterized by mnemonic naming conventions and relative referencing of variables. These predictions were tested in an experiment using experienced financial analysts. Although the interface that allows mnemonic and relative names called keyword takes longer overall it produces seventy five percent fewer simple errors and requires less mental effort. Given the overall serious cost of errors in financial models we conclude that interfaces having the keyword representation are far superior. INTRODUCTION A recurring prescription in user interface design states that the interface should use terms and show a display that fit the objects and actions in the users mental representation. Translating thought into action takes time and produces errors. If the interface requires a significant translation of the users mental representation we can expect an increase in mental time and errors. Card Moran and Newell analyzed how skilled users of text editing software translate the tasks to be performed into actions on the system. They did a similar analysis for the routine components of computer aided circuit design. Their research demonstrated that users break down tasks into small units of approximately 10330 seconds. It also showed that users performance in text editing and in the routine components of circuit design were similar both in terms of error frequency and command execution time. concepts into different kinds of interface representations is warranted. This research shows how interface features affect the performance of skilled financial model builders. Performance is measured in terms of execution times and the frequency of both simple and conceptual errors. The research was executed in four steps 1. Task Analysis. The task of building financial models was characterized by breaking it down into its essential operations independent of any given package. This characterization made evident the importance of describing cells referred to in formulas into different types. These cell types are used in different combinations inside common financial formulas in order to represent special time relationships among generic financial variables. 2. Analysis of the Interface Representations. Two common interface designs IFPS and Lotus 1 2 3 were analyzed for differences in how one refers to cells in formulas. The key differences are the mnemonic constraints of the cell naming conventions and whether the cells are referred to by their relative or absolute locations. 3. Cognitive Modeling. GOMS models were built to represent the steps that skilled financial model builders are likey to follow when writing model formulas. These cognitive models served as the basis for making predictions on the relative frequency of errors among cell types in different representations. They were also used for predicting and explaining the amount of mental time needed to translate financial concepts into formulas. 4. Performance Evaluation. We conducted an experiment with 31 experienced users of financial modeling systems and compared their performance with our predictions. Since financial modelling systems e.g. spreadsheet packages business modeling systems financial modelers etc. are the second most utilized end user software packages in business after word processors and are claimed to be the most frequently used among senior executives an analysis of the cost of translating financial TASK ANALYSIS The basic structure of financial modeling systems is the two dimensional matrix1. Model builders typically use one dimension usually columns to represent time periods and the other rows to represent financial variables such as categories of costs and revenues. The process of building these models is basically the same for all packages 1. Rows variables and columns time periods are labeled. This produces a grid of cells. 1Some packages allow adding one or more dimensions in order to include different business units products sales regions etc. 2. The content of each cell in the grid is specified. A cell may contain either numerical values or formulas. Formulas make reference to the content ofother cells and include mathematical andor logical operators and numerical constants. Most formulas make reference only to cells in the same column where the formula resides. For example the formulas to represent the relationships shown below only require cell references in the same column Assetst Current Assetst Fixed Assetst Profit t1t Revenues t1tCosts t1t Notice that the two relationships represent different concepts in time the first equation shows a relationship among variables at a given time t these are called level variables while the second shows a relationship between events within a period of time called flow variables. When financial relationships mix levels and flows the formulas that represent these relationships are likely to include cells that are not in the same column. Another important distinction for the financial planner is between income flows and cash flows. Income flows are the result of using the accrual basis of accounting. In this. view revenues are recognized when the transaction related to the earning process occurs that is when the sale is made or the service is rendered. An attempt is also made to match expenses with associated revenues. This method produces a good estimate of the profitability of the firm during a period of time. It does not however provide information about cash flows because income flows such as revenues occur before the receipt of cash. The timing of cash flows is crucial for decisions regarding the allocation and acquisition of funds. Therefore since income flows are important for assessing profitability and cash flows are indispensable for performing sound financial planning both are included in financial models. They are often confused by the beginning analyst and present a mental burden for the experienced one. An example with timing differences between income flows and cash flows is Receipts t1t .2Revenues t3t2 .8Revenues t2t1 The formula to represent this equation has to refer to cells in a different column and in a different row with respect to the cell where the equation resides. Every time there is a time lag between income flows and cash flows the formulas have to include cells in different rows and columns. These are difficult to think about yet they represent the basic task of financial planning. They are even more difficult to specify in current software. This research shows that the degree of difficulty a planner has in referencing cells in a formula is related to the kinds of cells those in the same row and column and those in different rows and columns. We refer to these types as type SR type SC and type DRC. The research also shows that how these cells are made available to the planner by the software package interface can add to the users task difficulty. INTERFACE REPRESENTATIONS References to the content of other cells cell references can be specified and represented using different cell naming conventions. There are two main differences in the cell naming conventions of financial modeling systems Mnemonic names vs Non mnemonic names Relative references vs Absolute references Mnemonic cell naming uses the name of the variables such as REVENUES or COSTS to specify rows. Non mnemonic cell naming only uses the row numbers. An absolute reference states the position of the cell by using its overall coordinates. A relative reference specifies the position of the cell by describing its location with respect to the cell where the formula resides. Four cell naming conventions result from combining these two cell naming dimensions. Figure 1 shows examples of each. REVENUES 1 COSTS 2 PROFITS 3 4 Year 1 A ..... Year 2 B J Mnemonic Absolute REVENUES 1 Relative PREVIOUS REVENUES Non mnemonic A1 C1R2 Figure 1. Cell naming conventions for referring to the cell Revenues in Year 1 at the arrow head when residing in Profit Year 2 the arrow foot This research investigates only two of these four naming conventions the two most widely used in financial modeling systems. IFPS Interactive Financial Planning System embodies the relative mnemonic cell naming convention here called keyword. Lotus 12n3 uses absolute non mnemonic cell naming here called positional. These two packages exhibit other differences such as style of interaction e.g. command language vs direct manipulation but it is only the users performance with respect to cell referencing that is studied here. COGNITIVE MODELS The next step of this work involved building GOMS models to simulate skilled performance. The models were built by observing experienced financial analysts in a pilot study. In this study the participants were asked to think aloud while building typical financial planning models. During the pilot study model building behavior occurred in two general phases the acquisition of the appropriate information and the translation of this information into formulas using one of the two interface representations. Skilled financial analysts exhibited the same type of problem solving behavior during the acquisition of information. This behavior is independent of interface representation because the acquisition process occurs before the representation is utilized. Problems in the acquisition phase will be referred to as conceptual errors in the performance evaluation section. In the translation phase the mental effort cost of translating financial concepts into formulas should be affected by the different interface representations. The GOMS models drawn from the pilot study specify different sequences of mental operations for the two representations studied here. An example of how the same task is executed with each representation is shown in Figure 2 the full fledged GOMS models are discussed in. errors caused by overload in working memory were expected to correlate with peak working memory loads called overload errors. These predictions assume that as the peak number of working memory resources utilized gets closer to its limit the likelihood of error behavior increases. Therefore error hypotheses can be built by comparing the peak working memory loads of different cell types in each representation. Figure 3 shows the working memory loads for cells type SC and DRC with the keyword representation when executing the GOMS model of Figure 2. GOAL WRITE OPERATOR CELL Keyword Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE LAG SPECIFY LAG POINTER SPECIFY KEYWORD WRITE LAG KEYWORD WRITE VARIABLE NAME ..if Jag Positional Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE COLUMN Sel GOAL GET COLUMN NAME SPECIFY COLUMN NAME WRITE COLUMN NAME GOAi CALCULATE COLUMN NAME SPECIFY LAG POINTER CALCUlATE COLUMN NAME WRITE COLUMN NAME SPECIFY ROW NUMBER WRITE ROW NUMBER ..ifno lag ..if Jag Figure 2. GOAL WRTIE OPERATOR CEIL The execution of this GOMS model translates information about the internal representation of the financial problem into cell types SC and DRC. Figure 2 shows that the positional representation requires more steps than the keyword representation because the model builder has to perform extra steps in order to translate variable names into cell coordinates. Performance predictions were made of the translation process from the GOMS models 1 Mental times for executing error free equations were estimated by simply counting the number of mental operations required for each cell reference and 2 The frequencies of simple Keyword Naming Cell type SC I  1  .. Operetor Pointer Pointer VarName Pointer VerName r ............................. Time Cell Type DRC ................................ .. ...  ........................................... Kev..ord Et S Log  Log  Kev...rd ........... tl t 1 Operator VorNome VorName VorNome Pointer Pointer Pointer Pointer Pointer VorNemef .................. Figure 3. Working Memory Loads for Cell Types SC and DRC for the Keyword Representation The horizontal axis in both charts represents time but each interval does not represent the same length of time. The intervals were chosen only to represent when the content of working memory changes. The vertical axis shows a rough estimation of the number of pieces of information kept in working memory for translating concepts into formulas. It does not show symbols for other goals that are also kept in working memory because they are the same for both cell types. Figure 3 shows that cells of type DRC have greater demand for working memory than cells of type SC. We expect higher overload error rates for DRC than for SC in the keyword representation because the model builder has to remember and translate the concept of time lag into the formula when writing cells of type DRC. The predictions made with the GOMS models for overload error rates are shown in Figure 4. Similar predictions were made for error free mental times see for details. A detailed task analysis is made for each cell type and each interface. The number of steps required is expected to correlate with the time spent performing the task. Overload Error Rates Predictions Positional Narnini Keyword Namini Type SR Type SC Type SR Type DRC Type SR Type DRC Type SR Type SC Type SR Type DRC Type SC Type DRC Both representations Positional Type SR Keyword Type SR Positional Type SC Keyword Type SC Positional Type DRC Keyword Type DRC Supported by empirical results Figure 4. Overload Error Rates Predictions In sum it was expected that subjects using the keyword interface would make fewer overload errors and need less mental time than subjects using the positional interface because of the additional translation effort required with the positional representation. In contrast execution time for the keyword subjects was expected to be longer because subjects are required to write more characters. Similar mental effort estimations were found by Olson and Nilsen in their study of cell referencing in positional style spreadsheet packages. They showed that substantially less mental effort was required for pointing to the cell as opposed to calculating the coordinates. This research expands the essence of these results by making both time and error predictions based on the estimation of the mental effort involved and by incorporating the characteristics of the task of building financial models into these estimations. PERFORMANCE EVALUATION The core of the comparison is the p.erformance of users on a keyword based interface versus users on a positional based interface. Keyword subjects were experienced FPS users positional subjects were experienced Lotus 12s3 users. Each subject was asked to write equations using different cell type combinations. The length of the equations was restricted to equations containing only two or three cell references. Subjects. Thirty one experienced users from eleven firms served as volunteer subjects. The subjects work fulltime in Fortune 500 financial and industrial firms in Michigan and Ohio. Participating organizations were asked for individuals with the following experience a at least two years working in accounting or finance in their organizations and b at least one year using either Lotus 12i3 or FPS. Twenty one subjects were Lotus users positional ten were keyword users FPS. Lotus users had an average experience of 7.3 years in accounting or finance whereas FPS users had 6.3. All subjects had more than one year experjence with their respective package. Ninety four percent of the subjects reported using the package at least twice a week with the remainder using it less often because they had recently been promoted to supervisory positions. Practice and Test Problems. The subjects were asked to solve the problem with pencil and paper. This eliminated the differences in interactive style presented by the two packages. Subjects read a description of the problem aloud from an instruction booklet and were then given a summary of data and a model skeleton. The summary of data detailed all the input data for building the model. The model skeleton was a worksheet with labeled columns and rows that had empty rows in which the participants wrote the missing equations or data. There were five practice problems and six test problems with 24 empty rows. Procedure. The experimental sessions were one hour and a half long and consisted of oral instructions practice problems test problems two tests of writing speed and a two part questionnaire. The whole experimental session was videotaped. During the test and practice problems subjects read a description of the problem and the objective for building the financial model. They then picked up the model skeleton and the summary of data. They were asked to write one variable at a time and to avoid jumping from row to row. After each problem subjects were given feedback on their solutions. Writing speed tests were administered to control for differences in subjects writing speeds. In the tests subjects were asked to write the correct answers for the test problems by copying them from a sheet of paper and by listening and transcribing items from a tape. As an additional experimental control background data on the subjects was collected. RESULTS AND DISCUSSION Errors. Errors were classified by two judges into two categories overload errors such as forgetting part of a cell reference or misremembering the cell name and conceptual errors such as specifying the overall formula incorrectly. Of the 120 errors five were judged to fall in either category and therefore were not included in further analysis. Of the 115 remammg errors 53 were overload errors and 62 conceptual. As predicted interface representation had an impact on overload error rates p.01 but not on conceptual errors p.96 analysis of variance of the logit transformation of error rates for both overload and conceptual errors. These results provide support for the claim that conceptual errors are expected to occur before the translation into the interface representation starts. In contrast the frequency of overload errors for positional subjects was 4.5 times greater than for keyword subjects 5.9 vs. 1.3 and almost the same for conceptual errors 5.4 vs. 4.2. Figure 5 shows the distribution of both types of errors for each cell type in each interface representation. For overload errors cell type DRC was more error prone than SR and SC as predicted p.05 and p.01 Tukey Kramer test but contrary to expectations there was no significant difference between SR and SC. The causes for this result discussed further in have to do with a misassumption in our GOMS models about how much effort was required in reusing a piece of information that was used in an earlier step. There was no interaction between cell type and the interface representation p.20. This means that cells that are difficult to translate in one representation are also difficult in the other. Finally the difference in error frequency between the two representations for each cell type was highly significant for SC p.01 slightly significant for SR p.10 and not significant for DRC as predicted by the cognitive models shown in Figure 4. Percentage of Errors Overload Errors I ISR 8 DFC Positional 81 Keyword Conceptual Errors Percentage of Errors I ISR 8 DR rmPositional Keyword Figure 5. Overload and Conceptual Error Rates For conceptual errors cell type SC has a significantly lower error rate than type SR and DRC p.01. Our task analysis shows that cells in the same column type SC represent straightforward relationships among both level and flow variables. On the other hand cells of type DRC are required for formulas that represent time lags and cells of type SR are included in formulas that mix levels and flows and in formulas that involve recursion. It was therefore expected that more conceptual difficulties would occur when formulas include these more complex financial relationships. If interface representation does not affect the conceptual process executed before translation then we should expect no interaction between representation and cell types for conceptual errors. Contrary to this reasoning the lower graph in Figure 5 shows a strong interaction between cell type and representation for conceptual errors. This interaction may be explained by other interface features of the two packages used in the experiment. IFPS encourages the user to think in terms of recursion by automatically propagating formulas this may explain why the frequency of conceptual errors is smaller for positional subjects in type SR which is usually used in recursive formulas. On the other hand IFPS makes it difficult to write equations with cells of type DRC by encouraging this same recursive thinking it is conceptually difficult to formulate in a single equation both initial and steady state conditions this may explain why IFPS subjects had so many conceptual problems with DRC compared to Lotus users. Times. Execution times for formulas with two and three references were between 7 and 35 seconds for both representations. These times are similar to the unit task times found by Card et al in their studies of text editing and circuit design. Figure 6 shows the average time needed to execute each cell reference with both representations. Even though execution time is longer for the keyword representation most of this time is spent writing because more has to be written. We were interested not only in total times but in how much mental effort was involved in each representation. We calculated mental time by subtracting from the execution time the raw writing time which was estimated from separate writing speed tests. The results show that the positional representation forces skilled financial analysts to spend a little more than 50 of the time in mental operations versus only 20 in the keyword notation. Average Time per Cell sec Writing Mental Positional 3.8 3.9 Keyword 8.9 2.2 Total 11.1 Figure 6. Error Free Execution Times General Statistics Mental times for formulas with two and three cell references were between 1 and 10 seconds for the keyword representation and between 3 and 22 seconds for the positional notation. The mental time spent in each cell type was estimated by regressing the total mental time per formula against the number of each cell type in each formula The results in Figure 7 show large differences in the mental effort required for each cell type in each representation and support the hypotheses generated by the GOMS models. Mental Times Regression coefficients standard error Positional Keyword Long SR Short SR SC DRC R2 N 3.34 .38 2.09 .12 9.25 .53 4.90 .96 1.25 .75 1.16 .19 4.88 .72 Figure 7. Mental Times It is important to clarify that the keyword representation has two methods for writing cells of type SR. In the short method the user only writes the keyword e.g. PREVIOUS while in the long method both the keyword and variable are specified. The mental times for these two methods indicated that the GOMS models can predict times and errors for these variations if the task is accurately represented. Finally mental times for each cell reference type and each representation have a high correspondence with overload error frequencies but not with conceptual error rates. This can be explained by speculating that in financial model building longer mental times are associated with increased effort and higher memory loads some of which break down and produce simple errors. Figure 8 shows both the overload error rates and the mental times in a logarithmic scale. IO Mental Time  Overload Error Rate 100 gTime sec. 10 0 below 0 Errors 0.1  0.1 Lotus Lotus Lotus  IFPS  IFPS  IFPS  SR S CFC SR S CFC The mental time and or rate for IFPS SR are composite of both short SR and long SR therefore they are not comparable to the mental times and overload error rates for the other cell references. Figure 8. Overload Error Rates vs Mental Times CONCLUSIONS Interface design decisions have grave consequences for the effort involved in difficult mental woik that is supposed to be made easier with software. In particular we found that for financial modeling systems an interface that requires the user to calculate the cell coordinates imposes a mental workload that produces errors and increases the time attributed to mental effort. Even though the total time to enter a mnemonic cell reference is longer it is not time involved with hard work. If one takes into account the cost of errors both the time to correct errors and the intangible cost of undetected errors the keyword representation is considered far superior. A second far more reaching outcome of this research is a demonstration that the GOMS model formalism is useful in predicting both time and error performance. It illustrates its potential in interface design. The user interface representations built into the GOMS models are abstractions of two common properties designed into financial modeling systems. They could readily have represented decisions of unbuilt software. The results of the experiment confirmed the cognitive load predictions of the GOMS models thereby supporting the worth of this analysis method for evaluating design decisions as well as differences in existing software packages. Finally the research in this paper extends the application of the GOMS model to a highly complex problem solving task. It demonstrates how it can be applied to the routine cognitive portions of the task and provides insight into how the users internal problem representation interacts with the external constraints of the software package. ACKNOWLEDGEMENTS This research was supported in part by a University of Michigan Rackham Grant. The authors would like to thank Alan G. Merten Raymond R. Reilly and J. Frank Yates for their valuable comments and suggestions throughout the execution of the research. Special thanks are due to the financial planners who took part in this study on a purely voluntary basis. Finally thanks to Diane L. Morrow Lerch for her help in data recording and analysis.
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token. The set of this research paper's data gathering methodologies contains [MASK].
1, 1989, Skilled financial planning: the cost of translating ideas into action, brainstorming and focus groups, Research paper:
ABSTRACT We use GOMS models to predict error rates and mental times for translating financial concepts into equations in two widely used interface representations. The first of these common to spreadsheet packages is characterized by non mnemonic naming and absolute referencing of variables. The second common to nonprocedural command driven software is characterized by mnemonic naming conventions and relative referencing of variables. These predictions were tested in an experiment using experienced financial analysts. Although the interface that allows mnemonic and relative names called keyword takes longer overall it produces seventy five percent fewer simple errors and requires less mental effort. Given the overall serious cost of errors in financial models we conclude that interfaces having the keyword representation are far superior. INTRODUCTION A recurring prescription in user interface design states that the interface should use terms and show a display that fit the objects and actions in the users mental representation. Translating thought into action takes time and produces errors. If the interface requires a significant translation of the users mental representation we can expect an increase in mental time and errors. Card Moran and Newell analyzed how skilled users of text editing software translate the tasks to be performed into actions on the system. They did a similar analysis for the routine components of computer aided circuit design. Their research demonstrated that users break down tasks into small units of approximately 10330 seconds. It also showed that users performance in text editing and in the routine components of circuit design were similar both in terms of error frequency and command execution time. concepts into different kinds of interface representations is warranted. This research shows how interface features affect the performance of skilled financial model builders. Performance is measured in terms of execution times and the frequency of both simple and conceptual errors. The research was executed in four steps 1. Task Analysis. The task of building financial models was characterized by breaking it down into its essential operations independent of any given package. This characterization made evident the importance of describing cells referred to in formulas into different types. These cell types are used in different combinations inside common financial formulas in order to represent special time relationships among generic financial variables. 2. Analysis of the Interface Representations. Two common interface designs IFPS and Lotus 1 2 3 were analyzed for differences in how one refers to cells in formulas. The key differences are the mnemonic constraints of the cell naming conventions and whether the cells are referred to by their relative or absolute locations. 3. Cognitive Modeling. GOMS models were built to represent the steps that skilled financial model builders are likey to follow when writing model formulas. These cognitive models served as the basis for making predictions on the relative frequency of errors among cell types in different representations. They were also used for predicting and explaining the amount of mental time needed to translate financial concepts into formulas. 4. Performance Evaluation. We conducted an experiment with 31 experienced users of financial modeling systems and compared their performance with our predictions. Since financial modelling systems e.g. spreadsheet packages business modeling systems financial modelers etc. are the second most utilized end user software packages in business after word processors and are claimed to be the most frequently used among senior executives an analysis of the cost of translating financial TASK ANALYSIS The basic structure of financial modeling systems is the two dimensional matrix1. Model builders typically use one dimension usually columns to represent time periods and the other rows to represent financial variables such as categories of costs and revenues. The process of building these models is basically the same for all packages 1. Rows variables and columns time periods are labeled. This produces a grid of cells. 1Some packages allow adding one or more dimensions in order to include different business units products sales regions etc. 2. The content of each cell in the grid is specified. A cell may contain either numerical values or formulas. Formulas make reference to the content ofother cells and include mathematical andor logical operators and numerical constants. Most formulas make reference only to cells in the same column where the formula resides. For example the formulas to represent the relationships shown below only require cell references in the same column Assetst Current Assetst Fixed Assetst Profit t1t Revenues t1tCosts t1t Notice that the two relationships represent different concepts in time the first equation shows a relationship among variables at a given time t these are called level variables while the second shows a relationship between events within a period of time called flow variables. When financial relationships mix levels and flows the formulas that represent these relationships are likely to include cells that are not in the same column. Another important distinction for the financial planner is between income flows and cash flows. Income flows are the result of using the accrual basis of accounting. In this. view revenues are recognized when the transaction related to the earning process occurs that is when the sale is made or the service is rendered. An attempt is also made to match expenses with associated revenues. This method produces a good estimate of the profitability of the firm during a period of time. It does not however provide information about cash flows because income flows such as revenues occur before the receipt of cash. The timing of cash flows is crucial for decisions regarding the allocation and acquisition of funds. Therefore since income flows are important for assessing profitability and cash flows are indispensable for performing sound financial planning both are included in financial models. They are often confused by the beginning analyst and present a mental burden for the experienced one. An example with timing differences between income flows and cash flows is Receipts t1t .2Revenues t3t2 .8Revenues t2t1 The formula to represent this equation has to refer to cells in a different column and in a different row with respect to the cell where the equation resides. Every time there is a time lag between income flows and cash flows the formulas have to include cells in different rows and columns. These are difficult to think about yet they represent the basic task of financial planning. They are even more difficult to specify in current software. This research shows that the degree of difficulty a planner has in referencing cells in a formula is related to the kinds of cells those in the same row and column and those in different rows and columns. We refer to these types as type SR type SC and type DRC. The research also shows that how these cells are made available to the planner by the software package interface can add to the users task difficulty. INTERFACE REPRESENTATIONS References to the content of other cells cell references can be specified and represented using different cell naming conventions. There are two main differences in the cell naming conventions of financial modeling systems Mnemonic names vs Non mnemonic names Relative references vs Absolute references Mnemonic cell naming uses the name of the variables such as REVENUES or COSTS to specify rows. Non mnemonic cell naming only uses the row numbers. An absolute reference states the position of the cell by using its overall coordinates. A relative reference specifies the position of the cell by describing its location with respect to the cell where the formula resides. Four cell naming conventions result from combining these two cell naming dimensions. Figure 1 shows examples of each. REVENUES 1 COSTS 2 PROFITS 3 4 Year 1 A ..... Year 2 B J Mnemonic Absolute REVENUES 1 Relative PREVIOUS REVENUES Non mnemonic A1 C1R2 Figure 1. Cell naming conventions for referring to the cell Revenues in Year 1 at the arrow head when residing in Profit Year 2 the arrow foot This research investigates only two of these four naming conventions the two most widely used in financial modeling systems. IFPS Interactive Financial Planning System embodies the relative mnemonic cell naming convention here called keyword. Lotus 12n3 uses absolute non mnemonic cell naming here called positional. These two packages exhibit other differences such as style of interaction e.g. command language vs direct manipulation but it is only the users performance with respect to cell referencing that is studied here. COGNITIVE MODELS The next step of this work involved building GOMS models to simulate skilled performance. The models were built by observing experienced financial analysts in a pilot study. In this study the participants were asked to think aloud while building typical financial planning models. During the pilot study model building behavior occurred in two general phases the acquisition of the appropriate information and the translation of this information into formulas using one of the two interface representations. Skilled financial analysts exhibited the same type of problem solving behavior during the acquisition of information. This behavior is independent of interface representation because the acquisition process occurs before the representation is utilized. Problems in the acquisition phase will be referred to as conceptual errors in the performance evaluation section. In the translation phase the mental effort cost of translating financial concepts into formulas should be affected by the different interface representations. The GOMS models drawn from the pilot study specify different sequences of mental operations for the two representations studied here. An example of how the same task is executed with each representation is shown in Figure 2 the full fledged GOMS models are discussed in. errors caused by overload in working memory were expected to correlate with peak working memory loads called overload errors. These predictions assume that as the peak number of working memory resources utilized gets closer to its limit the likelihood of error behavior increases. Therefore error hypotheses can be built by comparing the peak working memory loads of different cell types in each representation. Figure 3 shows the working memory loads for cells type SC and DRC with the keyword representation when executing the GOMS model of Figure 2. GOAL WRITE OPERATOR CELL Keyword Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE LAG SPECIFY LAG POINTER SPECIFY KEYWORD WRITE LAG KEYWORD WRITE VARIABLE NAME ..if Jag Positional Naming GOAL WRITE OPERATOR CEIL GOAL ACQUIRE INFORMATION ABOUf CEIL SPECIFY INFORMATION POINTER SPECIFY OPERATOR WRITE OPERATOR jf required SPECIFY VARIABLE NAME GOAL WRITE COLUMN Sel GOAL GET COLUMN NAME SPECIFY COLUMN NAME WRITE COLUMN NAME GOAi CALCULATE COLUMN NAME SPECIFY LAG POINTER CALCUlATE COLUMN NAME WRITE COLUMN NAME SPECIFY ROW NUMBER WRITE ROW NUMBER ..ifno lag ..if Jag Figure 2. GOAL WRTIE OPERATOR CEIL The execution of this GOMS model translates information about the internal representation of the financial problem into cell types SC and DRC. Figure 2 shows that the positional representation requires more steps than the keyword representation because the model builder has to perform extra steps in order to translate variable names into cell coordinates. Performance predictions were made of the translation process from the GOMS models 1 Mental times for executing error free equations were estimated by simply counting the number of mental operations required for each cell reference and 2 The frequencies of simple Keyword Naming Cell type SC I  1  .. Operetor Pointer Pointer VarName Pointer VerName r ............................. Time Cell Type DRC ................................ .. ...  ........................................... Kev..ord Et S Log  Log  Kev...rd ........... tl t 1 Operator VorNome VorName VorNome Pointer Pointer Pointer Pointer Pointer VorNemef .................. Figure 3. Working Memory Loads for Cell Types SC and DRC for the Keyword Representation The horizontal axis in both charts represents time but each interval does not represent the same length of time. The intervals were chosen only to represent when the content of working memory changes. The vertical axis shows a rough estimation of the number of pieces of information kept in working memory for translating concepts into formulas. It does not show symbols for other goals that are also kept in working memory because they are the same for both cell types. Figure 3 shows that cells of type DRC have greater demand for working memory than cells of type SC. We expect higher overload error rates for DRC than for SC in the keyword representation because the model builder has to remember and translate the concept of time lag into the formula when writing cells of type DRC. The predictions made with the GOMS models for overload error rates are shown in Figure 4. Similar predictions were made for error free mental times see for details. A detailed task analysis is made for each cell type and each interface. The number of steps required is expected to correlate with the time spent performing the task. Overload Error Rates Predictions Positional Narnini Keyword Namini Type SR Type SC Type SR Type DRC Type SR Type DRC Type SR Type SC Type SR Type DRC Type SC Type DRC Both representations Positional Type SR Keyword Type SR Positional Type SC Keyword Type SC Positional Type DRC Keyword Type DRC Supported by empirical results Figure 4. Overload Error Rates Predictions In sum it was expected that subjects using the keyword interface would make fewer overload errors and need less mental time than subjects using the positional interface because of the additional translation effort required with the positional representation. In contrast execution time for the keyword subjects was expected to be longer because subjects are required to write more characters. Similar mental effort estimations were found by Olson and Nilsen in their study of cell referencing in positional style spreadsheet packages. They showed that substantially less mental effort was required for pointing to the cell as opposed to calculating the coordinates. This research expands the essence of these results by making both time and error predictions based on the estimation of the mental effort involved and by incorporating the characteristics of the task of building financial models into these estimations. PERFORMANCE EVALUATION The core of the comparison is the p.erformance of users on a keyword based interface versus users on a positional based interface. Keyword subjects were experienced FPS users positional subjects were experienced Lotus 12s3 users. Each subject was asked to write equations using different cell type combinations. The length of the equations was restricted to equations containing only two or three cell references. Subjects. Thirty one experienced users from eleven firms served as volunteer subjects. The subjects work fulltime in Fortune 500 financial and industrial firms in Michigan and Ohio. Participating organizations were asked for individuals with the following experience a at least two years working in accounting or finance in their organizations and b at least one year using either Lotus 12i3 or FPS. Twenty one subjects were Lotus users positional ten were keyword users FPS. Lotus users had an average experience of 7.3 years in accounting or finance whereas FPS users had 6.3. All subjects had more than one year experjence with their respective package. Ninety four percent of the subjects reported using the package at least twice a week with the remainder using it less often because they had recently been promoted to supervisory positions. Practice and Test Problems. The subjects were asked to solve the problem with pencil and paper. This eliminated the differences in interactive style presented by the two packages. Subjects read a description of the problem aloud from an instruction booklet and were then given a summary of data and a model skeleton. The summary of data detailed all the input data for building the model. The model skeleton was a worksheet with labeled columns and rows that had empty rows in which the participants wrote the missing equations or data. There were five practice problems and six test problems with 24 empty rows. Procedure. The experimental sessions were one hour and a half long and consisted of oral instructions practice problems test problems two tests of writing speed and a two part questionnaire. The whole experimental session was videotaped. During the test and practice problems subjects read a description of the problem and the objective for building the financial model. They then picked up the model skeleton and the summary of data. They were asked to write one variable at a time and to avoid jumping from row to row. After each problem subjects were given feedback on their solutions. Writing speed tests were administered to control for differences in subjects writing speeds. In the tests subjects were asked to write the correct answers for the test problems by copying them from a sheet of paper and by listening and transcribing items from a tape. As an additional experimental control background data on the subjects was collected. RESULTS AND DISCUSSION Errors. Errors were classified by two judges into two categories overload errors such as forgetting part of a cell reference or misremembering the cell name and conceptual errors such as specifying the overall formula incorrectly. Of the 120 errors five were judged to fall in either category and therefore were not included in further analysis. Of the 115 remammg errors 53 were overload errors and 62 conceptual. As predicted interface representation had an impact on overload error rates p.01 but not on conceptual errors p.96 analysis of variance of the logit transformation of error rates for both overload and conceptual errors. These results provide support for the claim that conceptual errors are expected to occur before the translation into the interface representation starts. In contrast the frequency of overload errors for positional subjects was 4.5 times greater than for keyword subjects 5.9 vs. 1.3 and almost the same for conceptual errors 5.4 vs. 4.2. Figure 5 shows the distribution of both types of errors for each cell type in each interface representation. For overload errors cell type DRC was more error prone than SR and SC as predicted p.05 and p.01 Tukey Kramer test but contrary to expectations there was no significant difference between SR and SC. The causes for this result discussed further in have to do with a misassumption in our GOMS models about how much effort was required in reusing a piece of information that was used in an earlier step. There was no interaction between cell type and the interface representation p.20. This means that cells that are difficult to translate in one representation are also difficult in the other. Finally the difference in error frequency between the two representations for each cell type was highly significant for SC p.01 slightly significant for SR p.10 and not significant for DRC as predicted by the cognitive models shown in Figure 4. Percentage of Errors Overload Errors I ISR 8 DFC Positional 81 Keyword Conceptual Errors Percentage of Errors I ISR 8 DR rmPositional Keyword Figure 5. Overload and Conceptual Error Rates For conceptual errors cell type SC has a significantly lower error rate than type SR and DRC p.01. Our task analysis shows that cells in the same column type SC represent straightforward relationships among both level and flow variables. On the other hand cells of type DRC are required for formulas that represent time lags and cells of type SR are included in formulas that mix levels and flows and in formulas that involve recursion. It was therefore expected that more conceptual difficulties would occur when formulas include these more complex financial relationships. If interface representation does not affect the conceptual process executed before translation then we should expect no interaction between representation and cell types for conceptual errors. Contrary to this reasoning the lower graph in Figure 5 shows a strong interaction between cell type and representation for conceptual errors. This interaction may be explained by other interface features of the two packages used in the experiment. IFPS encourages the user to think in terms of recursion by automatically propagating formulas this may explain why the frequency of conceptual errors is smaller for positional subjects in type SR which is usually used in recursive formulas. On the other hand IFPS makes it difficult to write equations with cells of type DRC by encouraging this same recursive thinking it is conceptually difficult to formulate in a single equation both initial and steady state conditions this may explain why IFPS subjects had so many conceptual problems with DRC compared to Lotus users. Times. Execution times for formulas with two and three references were between 7 and 35 seconds for both representations. These times are similar to the unit task times found by Card et al in their studies of text editing and circuit design. Figure 6 shows the average time needed to execute each cell reference with both representations. Even though execution time is longer for the keyword representation most of this time is spent writing because more has to be written. We were interested not only in total times but in how much mental effort was involved in each representation. We calculated mental time by subtracting from the execution time the raw writing time which was estimated from separate writing speed tests. The results show that the positional representation forces skilled financial analysts to spend a little more than 50 of the time in mental operations versus only 20 in the keyword notation. Average Time per Cell sec Writing Mental Positional 3.8 3.9 Keyword 8.9 2.2 Total 11.1 Figure 6. Error Free Execution Times General Statistics Mental times for formulas with two and three cell references were between 1 and 10 seconds for the keyword representation and between 3 and 22 seconds for the positional notation. The mental time spent in each cell type was estimated by regressing the total mental time per formula against the number of each cell type in each formula The results in Figure 7 show large differences in the mental effort required for each cell type in each representation and support the hypotheses generated by the GOMS models. Mental Times Regression coefficients standard error Positional Keyword Long SR Short SR SC DRC R2 N 3.34 .38 2.09 .12 9.25 .53 4.90 .96 1.25 .75 1.16 .19 4.88 .72 Figure 7. Mental Times It is important to clarify that the keyword representation has two methods for writing cells of type SR. In the short method the user only writes the keyword e.g. PREVIOUS while in the long method both the keyword and variable are specified. The mental times for these two methods indicated that the GOMS models can predict times and errors for these variations if the task is accurately represented. Finally mental times for each cell reference type and each representation have a high correspondence with overload error frequencies but not with conceptual error rates. This can be explained by speculating that in financial model building longer mental times are associated with increased effort and higher memory loads some of which break down and produce simple errors. Figure 8 shows both the overload error rates and the mental times in a logarithmic scale. IO Mental Time  Overload Error Rate 100 gTime sec. 10 0 below 0 Errors 0.1  0.1 Lotus Lotus Lotus  IFPS  IFPS  IFPS  SR S CFC SR S CFC The mental time and or rate for IFPS SR are composite of both short SR and long SR therefore they are not comparable to the mental times and overload error rates for the other cell references. Figure 8. Overload Error Rates vs Mental Times CONCLUSIONS Interface design decisions have grave consequences for the effort involved in difficult mental woik that is supposed to be made easier with software. In particular we found that for financial modeling systems an interface that requires the user to calculate the cell coordinates imposes a mental workload that produces errors and increases the time attributed to mental effort. Even though the total time to enter a mnemonic cell reference is longer it is not time involved with hard work. If one takes into account the cost of errors both the time to correct errors and the intangible cost of undetected errors the keyword representation is considered far superior. A second far more reaching outcome of this research is a demonstration that the GOMS model formalism is useful in predicting both time and error performance. It illustrates its potential in interface design. The user interface representations built into the GOMS models are abstractions of two common properties designed into financial modeling systems. They could readily have represented decisions of unbuilt software. The results of the experiment confirmed the cognitive load predictions of the GOMS models thereby supporting the worth of this analysis method for evaluating design decisions as well as differences in existing software packages. Finally the research in this paper extends the application of the GOMS model to a highly complex problem solving task. It demonstrates how it can be applied to the routine cognitive portions of the task and provides insight into how the users internal problem representation interacts with the external constraints of the software package. ACKNOWLEDGEMENTS This research was supported in part by a University of Michigan Rackham Grant. The authors would like to thank Alan G. Merten Raymond R. Reilly and J. Frank Yates for their valuable comments and suggestions throughout the execution of the research. Special thanks are due to the financial planners who took part in this study on a purely voluntary basis. Finally thanks to Diane L. Morrow Lerch for her help in data recording and analysis.
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length. The set of this research paper's data gathering methodologies contains [MASK].
6454, 2018, Veritaps: Truth Estimation from Mobile Interaction, brainstorming and focus groups, Research paper:
ABSTRACT We introduce the concept of Veritaps a communication layer to help users identify truths and lies in mobile input. Existing lie detection research typically uses features not suitable for the breadth of mobile interaction. We explore the feasibility of detecting lies across all mobile touch interaction using sensor data from commodity smartphones. We report on three studies in which we collect discrete truth labelled mobile input using swipes and taps. The studies demonstrate the potential of using mobile interaction as a truth estimator by employing features such as touch pressure and the inter tap details of number entry for example. In our final study we report an F1  score of .98 for classifying truths and .57 for lies. Finally we sketch three potential future scenarios of using lie detection in mobile applications as a security measure during online login a trust layer during online sale negotiations and a tool for exploring self deception. H.5.m. Information Interfaces and Presentation e.g. HCI We frequently lie whether to advance our own aims or to protect others. Consequently we are also subject to many lies. Though this provides ample opportunity for practice humans are only slightly better than chance at detecting lies and exhibit a positive bias in assessing the truth. This deficiency has led to a century long interest in lie detection. Visual vocal and physiological features of communication have all been explored but to date natural language processing leads the way in identifying lies in digital communication. Through linguistic psychological and personal features research has demonstrated success in classifying dishonest prose such as spam and deceptive reviews. This work is licensed under a Creative Commons Attribution International 4.0 License. Copyright held by the ownerauthors.DOI httpsdoi.org10.11453173574.3174135 Paper 561 However writing prose covers only a small part of our digital input. As we increasingly use our mobile devices for digital communication our input also comes to include individual taps and swipes such as button clicks checkbox selection and number entry. This leaves much digital activity open for deceptive behaviour with our approximately chance level truth assessments. To this end we explore a content agnostic approach to mobile lie detection ignoring the content of the input i.e. input text and enabling lie detection across a much wider spectrum of input. To enable content agnostic lie detection we draw on research demonstrating that a variety of information is hidden in the details of mobile input such as stress boredom and affective states. We explore whether dishonesty and deception are similarly hidden. Research suggests the presence of physiological responses to lying such as increased handfinger activity. We hypothesize that these responses although subtle can be identified through smartphone sensors. We test this across three crowdsourced smartphone studies. In Study I we verify that lying on a smartphone exhibits similar behavioural cues to lying in conversation and that this can support the separation of honest and dishonest responses. In this study following the paradigm of Williams et al. participants are instructed to tell the truth or lie and the cues are identified through response time. Visible trends in other sensor data such as input speed motivate a second study using a more natural spontaneous lying paradigm. The results from Study II show that acceleration rotation and inter keypress duration can drive lie classification with an F1  score of .77. Finally we validate this result with an additional study where we explore our identified features from Study II with a dice paradigm. In Study III we show 98 precision and 97 recall for truths F1 .98 and 65 precision and 59 recall for lies F1 .57. Following the studies we sketch the concept of Veritaps an additional layer of communication to assist mobile device users in their own lie detection accuracy. Veritaps enables users to automatically share a belief state indicator alongside their input. With high accuracy Veritaps can label truthful input . We can also label inconclusive taps and swipes informing the user that they should use caution or seek further information in assessing this input. We illustrate the opportunities of Veritaps across a range of example scenarios including i automated lie analysis when completing online Page 1 Paper forms ii increased richness of trust in mobile messaging and iii as a prompt to prevent self deception. We present the following contributions 1. An exploration of lie detection across mobile devices regardless of the input content. 2. Results from three studies showing dishonesty affects user interaction with mobile devices. 3. Convincing classification rates of lies in mobile entry potentially improving a users ability to judge the veracity of others mobile input. 4. Veritaps a concept that allows users to share their belief states with other users and applications. accuracy in lie detection across three text corpuses. Ott et al. used linguistic features such as average word length or misspelling rate psychological features such as social or emotional clues and personal features such as references to money or religion to classify spam and deceptive reviews. Lying has also become a subject of exploration in crowdsourcing studies. Gino et al. asked participants to report the outcome of random events such as dice rolling or coin tossing. They identify lying across all of the input based on the deviation from the expected mean offering an insight into lying across an entire study. RELATED WORK Opportunities for Lie Detection in Smartphones Our work explores lie detection in mobile input. Specifically we are interested in classifying lying through sensor data rather than actual user input in order to make lie detection available for a broader range of mobile input types. While current research points towards physiological  and content based lie detection a common and robust strategy to lie detection has yet to be derived. We look for a commodity content agnostic approach to lie detection that can be used to identify deception in basic mobile input taps and swipes. We hypothesize that the bodily influences of deception can be measured using sensors available in consumer smartphones making commodity lie detecting feasible. Classifying Behaviors from Mobile Sensors Research shows that complex cognitive and affective phenomena can be inferred using commodity sensors. The linearity of swiping for example correlates with emotions during gameplay. Similarly speed acceleration and precision in touch input are indicative of affective states. Mobile activity can also provide insight into a users thinking where app activity battery level and time of day are strong correlates of boredom. Based on the idea of using mobile sensor data to support realtime inferences about human cognition we explore indicators of lying in mobile sensor data. Lie Detection Deceptive behavior carries a range of verbal and nonverbal cues and research has explored various strategies for using such cues to uncover deception. Among the most famous of these strategies is the polygraph. Polygraphs examine the subjects heart rate galvanic skin response respiration and blood pressure as physiological markers of deception. It is widely accepted however that the interpretation of physiological responses and thus polygraph results is a complex clinical task. The debate continues regarding the accuracy and applicability of polygraph testing. For example a large body of research assessing the validity of polygraph techniques uses mock crime scenarios which inherently lack the consequences of real crime scenarios and thus call into question the validity of their results. Other work has provided evidence on verbal visual and vocal cues to deception e.g.. Zuckerman et al. for example suggested that lying is a more cognitively complex task than telling the truth requiring liars to formulate internally and externally consistent events. These greater cognitive challenges result in greater response latency more hesitations increased pupil dilations and fewer heartbeats. More recently research has shown that lies include more complex imagery longer words and a greater number of pauses Paper 561 STUDY I SIMPLE LIES Research shows that lying takes longer than telling the truth. A common explanation is that the construction of a lie forces additional cognitive load compared to telling the truth and thus causes longer response times. Study I had two goals i to establish whether lying through touch interaction on mobile devices produces results that are consistent with verbal responses in a laboratory and ii to demonstrate the feasibility of separating honest and dishonest activity using mobile interaction data. We ran a mobile crowdsourced study of an experimental paradigm originally developed Williams et al.. The participants were asked to either lie or tell the truth about the color of the screen using common mobile UI elements. This paradigm offers an experimental procedure for studying both instructed and voluntary lies while maintaining an even distribution of lies and truths. This provides a simple method for initially investigating differences in interaction patterns between telling lies and truths using mobile devices. Task The experiment progressed as a series of random ordered trials each beginning with an objective TRUTH LIE or CHOICE. Directed trials where participants were told to LIE or tell the TRUTH presented a continue button and the CHOICE trials had two buttons prompting the user to choose between lying or telling the truth see Figures 1a and 1b. Upon establishing the objective participants were presented with a screen with a red or blue background. The participants objective was written as a visual reminder at the top of the screen. The UI controls button or slider appeared at the bottom of the screen see Figure 1d. The order of UI controls was randomised. Participants then had to activate the correct UI control according to a the color of the background b Page 2 Paper Figure 1 Example screens from the experimental application used for Study I. The figures show a a directed trial prescreen b a choice trial prescreen c a trial with sliders  where the participant should slide the RED slider in this case and d a trial with buttons  where the participant should select the RED button in this case. Results Lying took longer than telling the truth both when answering with a button and a slider and when being told whether to lie or when given the option to choose see Figure 2. Participants We recruited 100 participants from Mechanical Turk aged 19M59 M 31 33 females. Participants installed our experimental application on their own Android smart phones Android version 6.0 and followed onscreen instructions. Participants were reimbursed with 2.00 USD. To ensure only qualified participation we i required 90 HIT approval ii had participants pass a qualification test about the task before starting the HIT iii stored a unique device ID to avoid multiple participations and iv ensured that app and MTurk HIT participation count matched. Data Of the 100 participants ten never lied and one never told the truth in the choice trials and were therefore removed. The Paper 561 Truth The study used a 2 2 2 withinsubjects design. The independent variables were honesty of response lie vs. truth type of instruction directed vs. choice and UI button vs. slider. The dependent variable was response time. Each participant did a total of 192 trials with 64 from the directed to lie condition 64 from the directed to tell the truth condition and 64 from the choice condition. In half of the trials participants responded by tapping a button and the other half by dragging a slider. The order of trials was randomized. The study took 15 minutes on average. response time ms Design The task was similar to the original study with the exceptions that i instead of a labbased study participants were recruited online and completed the experiment on their own phones ii vocal responses were replaced with selections using buttons or sliders and iii the colors were changed to be visible for color blind red and blue instead of red and green. remaining 89 participants performed a total of 16671 trials. We removed i the first 10 trials per participant as warmup rounds ii 460 trials 2.9 that lasted more than 4 seconds and iii 623 incorrectly answered trials 3.9. The analysis is made on a resulting data set comprising 14788 trials. the text on the UI control and c the trials objective see Figure 1c. Trials were separated with a white screen for 1s. Participants were instructed to respond as quickly and accurately as possible. Participants were asked to lie and tell the truth half of the time each in the choice condition. Button Directed Slider Button Choice Slider Figure 2 Response times for telling truths and lies using two different UIs both for the directed and choice conditions. Error bars show 95 confidence intervals. It took on average longer to tell a lie for both UIs. Except for directed trials with the slider participants took significantly longer when lying on .5 see Table 1. The effect was larger when participants chose whether to lie or not. Summary The results show that constructing a lie is a cognitively harder task than simply telling the truth reflected by the increased response time when participants were asked to lie about the background color of a mobile UI. This corroborates the findings of Williams et al.. While the data sourced do not allow for an effective binary discrimination of truth and lies per Page 3 Paper Condition Directed Directed Choice Choice Button Slider Button Slider 3.037.00 .084.009 Cohens d 0.260.40 Table 1 Results from an ANOVA comparing truths and lies. Lying caused significantly longer responses for close to all conditions with the largest effect for choice trials. entry the results imply the feasibility of separating honest and dishonest activity using mobile interaction data specifically timing in this case. We further investigate if this difference can be observed for other parameters in Study II III. Although not statistically significant we observed that slider interactions were performed faster by 4.4 when telling the truth F1 175 2.16 p 0.14. Although mean response times pertaining to honest and dishonest behaviour were distinguishable in this study we were keen to explore whether additional features become more prominent with a spontaneous lying and b a more natural distribution of truths and lies i.e.. STUDY II ULTIMATUM GAME We ran a second study to analyze natural deceptive behavior. We employed a mobile version of the Ultimatum game a commonly studied task in behavioral economics. In this variant the participants are offered an incentive to lie. In the Ultimatum Game the first participant the proposer receives a sum of money and proposes a division of the money between themselves and the second participant the responder. The responder then either accepts the division giving both participants the proposed funds or rejects it altogether resulting in no payout for any of the participants. In the variant developed by Besancenot et al. which we use the proposer is given the opportunity to lie about the amount of allocated funds. Therefore for each trial the proposer to the responder i declares the amount that was allocated and ii proposes a division. This provides a monetary incentive for the participant to understate the provided funds enabling the study of naturally occurring dishonest behavior. and the proposed division of money. Additionally throughout the trials the mobile application collected data related to interaction with the UI using touch pressure accelerometer and gyro sensors. All participants had the role of the proposer. Participants were paired with an AI in the responder role presented as the human worker Mary with a fictional worker ID. Mary would simulate human latency when responding to proposals and would accept or reject proposals based on the available heuristics and basic economic and moral behavior greed was punished while fair divisions were rewarded. The AI was implemented as nine simple steps that would accept offers deemed favorable or refuse offers that were either directly too low 25 or too unfair 3P F where P is the proposal and F the declared funds. The AI would also reject offers when they repeatedly showed lower declared funds than expected from a random sample. If all steps passed a 75 chance of acceptance was returned to introduce some degree of unpredictable behavior. Mary did not know whether the participant was in fact honest or dishonest but instead reasoned based on the distribution of declared allocations from all trials. Mary accepted 86 of all proposals made very similar to human behaviour observed in other of the Ultimatum game studies. Procedure Upon installing and opening the experimental application participants were informed that they were playing the proposer and were paired with our AI under the guise of another crowdworker. For each of the 11 rounds an amount of US cents between 25 and 99 were allocated to the participant. The participant would then using num pads first state the amount of allocated funds about which they could lie and then propose a division see Figure 3a. Participants We recruited 41 participants from the USA from Mechanical Turk aged 2263 M 33 18 females 36 right handed. Participants were told that they were taking part in an economics experiment. Participants installed our experimental application on their own Android smart phones Android version 6.0 and followed onscreen instructions. Participants were reimbursed 1.00 USD in addition to the money collected throughout the experiment which ranged from 1.74 4.52 M 3.35. The experiment took at most 10 minutes. We employed the same qualification standards as for Study I. Design Each participant did 10 trials of proposals excluding a warmup round. The independent variable was funds allocated 2599. The dependent variables were declared allocated funds Paper 561 Figure 3 The experimental application used in Study II. Figures show a the screen where participants declare the allocated funds and propose a division and b a positive response from the AI Mary acting as another human worker. Page 4 Paper Shortly hereafter the participant would receive a notification of whether the responder Mary had accepted the division see Figure 3b. Participants collected money throughout the trials and were paid according to their final score to create a monetary incentive to lie. Feature Group Features Description Timing immediacy response t before first event entry duration Finger size touch area finger contact size Num pad key dynamics hold time tap precision see button holddown time distance to target center Button clicks hold time click area backspaces button holddown time quadrant activated number of deletions Done button taps precision hold time pressure click area number of times distance to target center button holddown time screen pressure quadrant activated Acceleration x y and z a for all axes Rotation   and p x2 y2 z2 around all axes Data An entry was defined as the window of time between when the proposal screen would appear see Figure 3a and until the participant hit OK. The resulting data set comprised 41 participants and 410 entries. Participants lied about the available funds on average 35.1 of time this was most prominent when the allocated funds were high. Seventeen participants never understated the available funds 59 lied at least once. Three participants understated at every entry. Participants discounted the actual endowment by 17.4 on average. The crowdsourced participants appear more loyal than laboratory participants Besancenot et al. found that on average 88.5 of the proposers discount the actual endowment by 20.5 in this study we observe that 41 of the participants never lied at all consistent with some feedback we received such as Signal Magnitude Table 2 Feature groups and specific features for each group. its best value at 1 and worst score at 0 and is defined as It seemed fair to me to split the money evenly. I dont believe in dishonesty so I did not want to lie Crowdworker Classification We built a binary truthlie classifier based on the data obtained. We defined a lie as an entry where the declared funds were lower than the allocated. Choice of Classifier We tried a range of classification algorithms including ensemble methods. An SVM with a radial basis function kernel provided the most promising classification accuracy. Hyper parameters were selected using grid search. The classifier was developed in Python using the ML library Scikit learn. for both a and precision recall precision recall where precision and recall relate to true positives TP false positives FP and false negatives FN as precision TP FP recall TP FN Using a randomized 5 fold cross validation we obtain precisions of 81 and 66 for truths and lies respectively. The rates for recall are 88 and 52 for truths and lies respectively. This yields an average F1  score of .77 .81 for truths and .66 for lies. These performances are well over both chance level .50 the baseline .65 and human performance. Feature Generation How Lies and Truths Differ Features were chosen based on previous work in classification of human factors using mobile devices e.g. such as speed precision rotation and acceleration sampled at 50 Hz. We also included features from empirical observations of deception e.g. such as immediacy and response length. Next we report on how interaction with the mobile UI differed between honest and dishonest entries in particular features that varied with the honesty of the interaction. We inspect the distribution of features using density plots blue areas represent the prevalence of honest entries red areas represent dishonest entries those with deflated declared funds. Note that a single feature seldom alone is enough to support classification. Instead combinations of features make up the decision which is not clear from a single features distribution. Feature Selection We clustered our features in related groups see Table 2 and handpicked the effective predictors for truth classification. The feature groups acceleration and num pad presented the most viable features for classifying truths and lies and were thus shown in our final classifier i.e. manual feature selection. Performance We measure how well our predictor works by reporting the average binary F1  score obtained over a randomized 5 fold cross validation. The F1  score can be interpreted as a weighted average of the precision and recall where an F1  score reaches Paper 561 Acceleration We observed that a low mean acceleration was most frequent among honest entries. This suggests that honest entries resulted in less hand movement by the device holding hand their nondominant hand. This was true both on the x axis and the z axis see Figure 4. This follows findings from an existing study of non phone deceit which showed that dishonesty causes increased handfinger activity. Page 5 Paper Dishonest detection. To validate these results and to assess whether the results generalize to other settings we ran a third study. Honest Figure 4 Mean acceleration on the z axis during an entry. Entries towards the low spectrum are predominately honest. Num pad For each entry an amount of cents between 25 and 99 was allocated requiring participants to input a two digit number in the declared input field using a num pad. We observe that the duration between the first key event and the second key event is higher for dishonest entries see Figure 5. This suggests that participants decide whether to lie and by how much per individual digit rather than per input. Honest Dishonest Figure 5 Duration between first and second num pad key event. Truthful entries show shorter durations between the first two num key presses. Our num pad dialog implementation could be dismissed by tapping outside of the num pad area instead of clicking OK. Additionally if after having entered a number the participants decided to correct their entry additional OK taps could be performed. The more taps on the OK button in the num pad the more likely an entry was to be honest see Figure 6 we almost exclusively observe dialog dismissal amongst dishonest entries and we almost only find honest entries for high number of taps on OK1 . Dishonest Honest Figure 6 Total number of taps on the OK button in the num pad. Honest entries tend to contain more taps on OK almost only dishonest entries closed the dialog without confirming OK almost only honest entries reopened the dialogue and pressed OK again. Summary Study II shows that the way people interact with their mobile UI can change with the level of honesty of the action. Specifically movement of the phone acceleration and num pad interactions varied. This increases our confidence in the feasibility of using sensor data to estimate the veracity of input. We built a classifier based on smartphone sensor data and achieved an average F1  score of .77. This classification accuracy shows that mobile sensor data can be a promising path towards lie 1 This may also suggest that honest users lied initially before correcting their input to the truth. Dishonest users may show reluctance to confirm their lie and thus avoid pressing OK. Further research is needed to verify this behaviour. Paper 561 STUDY III YATZY GAME Both Study I and Study II showed that we can observe differences in interaction data between lies and truths using mobile UIs. In Study I participants were instructed to lie and response time was the only distinguishing feature. In Study II participants were made aware that they could lie without punishment resulting in a higher proportion of lies than expected in everyday interaction. From this study a wider spectrum of mobile input became valuable features for classification. In order to validate the classification results from study II we ran a third study. This study still facilitated spontaneous lying but made no reference to dishonesty in its description. The study required participants to play a dice based game on a mobile device inspired by a widely used experimental task in dishonesty research. The task supported spontaneous lying and allowed for automatic labeling of discrete trials as either honest or dishonest. The participants were rewarded based on their reported score thereby making lying profitable. We did not encourage participants to lie and given that all participants passed an initial qualification test about the rules we can assume that participants were aware of their wrongdoings. Overstating scores could provoke both moral dissonance and fear of not having the crowdwork approved and thus not getting paid we hypothesize that this manifests itself in the participants mobile interaction. Task A commonly used task in studying deceit and dishonest behavior requires participants to report on the outcome of randomized events such as rolling a die or tossing a coin see for an overview. To encourage lying participants are rewarded relative to the reported outcomes. The actual outcomes of the events are only known to the participants. This paradigm supports inferences about deceit across all reports based on deviation from the expected mean but the individual reports cannot be labeled as honest or dishonest. To support the training of a classifier we used a dice rolling paradigm but made changes to allow for labeling of discrete events. Additionally we wished to collect data across a range of taps and swipes so as to cover a wider spectrum of typical mobile input. The application required participants to swipe through lists tap desired selections and tap numbers on a num pad. We developed a mobile dice game similar to the popular game Yatzy. The game consisted of 12 rounds of rolls with five dice. Each round required an initial roll and two potential rerolls of selected dice see Figure 7a. Participants then chose from a list of possible combinations such as sixes or three ofa kind and entered the score that a certain combination would yield see Figure 7b. This was typically the sum of the dice. There was a total of 12 combinations one for each round. Each combination could only be selected once. If the final dice of a round did not equate to a combination then any combination could be selected and a score of 0 should be entered. The game recorded both the participants actual score and their Page 6 Paper 0.50 below 150 points 1.00 between 150 and 200 points 2.00 more than 200 points Participants were briefed about the rules and scoring system of the game. Prior to taking part participants did a qualification test to ensure that they understood the rules. A help text was available throughout the game for assistance. After completing the experiment a debriefing screen explained the actual research agenda. In an attempt to differentiate 2 and 3 lies were defined as scoreenter scorereal 4. This was informed by the mean negative deviation from the real score i.e. when participants underreported their score M  3.4. Participants We recruited 51 participants from Mechanical Turk aged 2257 M 31.5 20 females. Participants were told that they were reviewing a mobile game before its launch. Participants were paid according to their score ranging from 0.50 US to 2.00 US to incentivize lying. We employed the same qualification standards as for Study I. Apparatus We developed the application for Android version 6.0 and higher. To obtain comparable data between participants we excluded tablets and other large screen devices. A pilot study identified touch pressure level as a good predictor of truthful input so for the final study we invited only participants who had phones with pressure sensors. This limited the phones to specific models from Google LG Motorola HTC and OnePlus. We also excluded mobile devices that could not report rotation or acceleration data. Results Figure 7 The experimental application used in Study III. Figures show a the home screen where participants roll and select dice selected dice are blue and see the score board and b the entry screen where combinations and amount of points are entered. The entry screen appears after finishing three rolls and pressing select combination. Fifty one participants took part in the study completing 561 unique entries with 44 labeled as lies 8 31 of the participants lied at least once. The average lie provided the participant with 15.6 surplus points. Conversely nine entries reported scores below the actual score with an average shortfall of 3.4 points. Our classification results show 98 precision and 97 recall for truths F1 .98 and 65 precision and 59 recall for lies F1 .57. What Constitutes an Entry Classification In order to train our classifier we labeled each entry as either a lie or a truth. When beginning a round the participants were presented with the home screen see Figure 7a. After rolling the dice the third time and pressing Select Combination they were presented with the entry screen see Figure 7b. We define an entry as the time frame from when participants were presented with the entry screen until and including they hit Done. During an entry the user had to pick a dice combination from a list and enter the amount of points that the combination and the dice roll amounted to. Swipes were recorded when scrolling the list of combinations taps were recorded when entering the amount of points on a num pad. IMU sensors recorded motion data throughout the entry. The classifier was built using the same approach as Study II. Data Cleaning We removed participants whose entries indicated that they did not understand the rules or deliberately rushed the game to optimize payment amounting to four participants. No participant lied on every single entry. We expected three possible outcomes of an entry in the game We removed entries with entered points lower than the actual score amount
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length. The set of this research paper's data gathering methodologies contains [MASK].
6455, 2018, How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games, brainstorming and focus groups, Research paper:
ABSTRACT How should an AI based explanation system explain an agents complex behavior to ordinary end users who have no background in AI Answering this question is an active research area for if an AI based explanation system could effectively explain intelligent agents behavior it could enable the end users to understand assess and appropriately trust or distrust the agents attempting to help them. To provide insights into this question we turned to human expert explainers in the realtime strategy domain shoutcasters to understand 1 how they foraged in an evolving strategy game in real time 2 how they assessed the players behaviors and 3 how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters foraging strategies for gleaning information necessary to assess and explain the players a characterization of the types of implicit questions shoutcasters answered and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. CCS Concepts Human centered computing Empirical studies in HCI Computing methodologies Intelligent agents Realtime strategy RTS games are becoming popular artificial intelligence AI research platforms. A number of factors have contributed to this trend. First RTS games are a challenge for AI because they involve realtime adversarial planning within sequential dynamic and partially observable environments. Second AI advancements made in the RTS domain can be mapped to real world combat mission must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. Paper 562 planning and execution such as an AI system trained to control a fleet of drones for missions in simulated environments. People without AI training will need to understand and ultimately assess the decisions of such a system based on what such intelligent systems recommend or decide to do on their own. For example imagine Jake a domain expert trying to make an educated decision about whether or not to use an intelligent agent. Ideally an interactive explanation system could help Jake assess whether and when the AI is making its decisions for the right reasons so as to ward off lucky guesses and legalethical concerns see. Scenarios like this are the motivation for a burgeoning area of research referred to as Explainable AI where an automated explanation device presents an AI systems decisions and actions in a form useful to the intended audience here Jake. There are recent research advances in explainable AI as we discuss in the Related Work section but only a few focus on explaining complex strategy environments like RTS games and fewer draw from expert explainers. To help fill this gap we conducted an investigation in the setting of StarCraft II a popular RTS game available to AI researchers. We looked to shoutcasters who are commentators for esports like RTS games. In StarCraft e sports two players compete while the shoutcasters provide realtime commentary. Shoutcasters are helpful to investigate for explaining AI agents in real time to people like Jake for two reasons. First they face an assessment task similar to Jakes. Specifically they must 1 discover the actions of the player 2 make sense of them and 3 assess them particularly if they discover good bad or unorthodox behavior. They must do all this while simultaneously constructing an explanation of their discoveries. Second shoutcasters are expert explainers. As communication professionals they are paid to inform an audience they cannot see or receive feedbackquestions from. Hoffman Klein researched five stages of explanation looking at how explanations are formed from observation of an event generating one or more possible explanations judging the plausibility of said explanations and either resolving or extending the explanation. Their findings help to illustrate the complexity of shoutcasters task due to its abductive nature of explaining the past and anticipating the future. In short shoutcasters must anticipate and answer the questions the audience are not able to ask all while passively watching the video stream. Page 1 Paper Because shoutcasters explain in parallel to gathering their information we guided part of our investigation using Information Foraging Theory IFT which explains how people go about their information seeking activities. It is based on naturalistic predator prey models in which the predator shoutcaster searches patches parts of the information environment to find prey evidence of players decision process by following the cues signposts in the environment that seem to point toward prey based on their scent predators guess at how related to the prey a cue is. IFT constructs have been used to explain and predict peoples informationseeking behavior in several domains such as understanding navigations through web sites or programming and software engineering environments. However to our knowledge it has not been used before to investigate explaining RTS environments like StarCraft. Using this framework we investigated the following research questions RQs. RQ1 RQ2 investigate the information needs of shoutcasters as human assessors like our target user Jake. Because shoutcasters may not have access to the playersagents internal reasoning this perspective is especially pertinent to model agnostic XAI which ignores internals to achieve generality e.g. RQ1 The What and the Where What information do shoutcasters seek to generate explanations and where do they find it RQ2 The How How do shoutcasters seek the information they seek We then used RQ3 RQ4 to investigate shoutcasters in their role as expert explainers RQ3 The Questions What implicit questions do shoutcasters answer and how do they form their answers RQ4 The Explanations What relationships and objects do shoutcasters use when building their explanations BACKGROUND AND RELATED WORK Another important underpinning for our work is what people want explained. Lim Dey conducted an influential investigation into information demanded from contextaware intelligent systems. They categorized users information needs into various intelligibility types and investigated which types provided the most benefit to user understanding. Among these types were What questions What did the system do Why questions Why did the system do X and so on. We draw upon these results in this paper to categorize the kinds of questions that shoutcasters explanations answered. Other research confirms that explanations containing certain intelligibility types make a difference in user attitude towards the system. For example findings by Cotter et al. showed that justifying why an algorithm works but not on how it works were helpful for increasing users confidence in the system but not for improving their trust. Other work shows that the relative importance of the intellibility types may vary with the domain for example findings by Castelli et al. in the domain of smart homes showed a strong interest in What questions but few of the other intellibility types. Constructing effective explanations of AI is not straightforward especially when the underlying AI system is complex. Both Kulesza et al. and Guestrin et al. point to a potential tradeoff between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain in a black box or model agnostic fashion predictions of any classifier in a faithful way and also approximate it locally with an interpretable model. They described a fidelity interpretability tradeoff in which making an explanation more faithful was likely to reduce its interpretability and vice versa. However humans manage this tradeoff by accounting for many factors such as the audiences current situation their background amount of time available etc. One goal of the current study is to understand how expert human explainers like our shoutcasters manage this tradeoff. Our work draws upon mental models in XAI. Mental models defined as internal representations that people build based on their experiences in the real world enable users like Jake our target user to predict system behavior. Kulesza et al. found those who adjusted their mental models most in response to explanations of AI a recommender system were best able to customize recommendations. Further participants who improved their mental models the most found debugging more worthwhile and engaging. In the domain of assessing RTS intelligent agents Kim et al. invited 20 experienced players to assess the skill levels of AI bots playing StarCraft. They observed that human rankings were different in several ways to a ranking computed from the bots competition win rate because humans weighed certain factors like decisionmaking skill more heavily. The mismatch between empirical results and perception scores may be because AI bots that are effective against each other proved less effective against humans. Building upon this finding Kulesza et al. then identified principles for explaining in a white box fashion to users how a machine learning system makes its predictions more transparent to the user. Participants quality of mental models increased by up to 52 in user studies with a prototype following these principles and along with these improvements came better ability to customize the intelligent agents. Kapoor et al. also showed that explaining AI increased user satisfaction and interacting with the explanations enabled users to construct classifiers that were more aligned with target preferences. Bostandjiev et al.s work on a music recommendation system found that explanation led to a remarkable increase in user satisfaction with their system. Cheung et al. studied StarCraft from a different perspective that of nonparticipant spectators. Their investigations produced a set of nine personas that helped to illuminate who these spectators are and why they watch. Since shoutcasters are one of the personas they discussed how shoutcasters affect the spectator experience and how they judiciously decide how and when to reveal different types of information both to entertain and inform the audience. Another contingent of researchers is working toward applying machine learning to automatically summarize different aspects of sports potentially assisting sportscasters or shoutcasters in their work. Two examples are automatically generating esports statistics and automatically extracting football play diagrams from raw video. Paper 562 Page 2 Paper Tournament Shoutcasters Players Game 2017 IEM Katowice Neeb vs Jjakji 2017 IEM Katowice Rotterdam and Maynarde Harstem vs TY 2017 GSL Season 1 Code S Artosis and tasteless 2016 WESG Finals Tenshi and Zeweig DeMuslim vs iGXY 2017 StarLeague S1 Premier Wolf and Brendan Innovation vs Dark 2016 KeSPA Cup Wolf and Brendan Maru vs Patience 2016 IEM Geonggi Kaelaris and Funka Byun vs Iasonu 2016 IEM Shanghai Rotterdam and Nathanias ShowTime vs Iasonu 3 2016 WCS Global Finals iNcontroL and Rotterdam Nerchio vs Elazer Snute vs ShowTime 10 2016 DreamHack Open Leipzig Rifkin and ZombieGrub Table 1. Summary of StarCraft 2 games studied. Please consult our supplementary materials for transcripts and links to videos. Finally future attempts to generate automated shoutcasting from replays using data such as the corpus reported in this paper could use dialog evaluation systems e.g. to assess the quality of the generated shoutcasters. The closest work to our own is Metoyer et al.s investigation into the vocabulary and language structure of explaining RTS games. In their study novices and experts acted in pairs the novice watched the expert play and asked questions while the expert thought aloud and answered them. They developed qualitative coding schemes of the content and structure of the explanations the expert players offered. In this paper we drew upon these coding schemes with slight modifications. The complete code sets are available in the online supplemental materials. Our work differs from all of these works in that our explainers are expert communicators about the game not participant players programmers or end users. Our work is also the first to apply IFT to XAI. METHODOLOGY In order to study high quality explanations and capable players we considered only games from professional tournaments denoted as Premier by TeamLiquid1 and also filtered out irrelevant utterances as described later in this section. We selected 10 matches from the Premier pool available with video on demand from professional StarCraft 2 tournaments from 2016 and 2017 Table 1. Professional matches have multiple games so we randomly selected one game from each match for analysis. 16 distinct shoutcasters2 appeared across the 10 videos with two casters3 commentating each time. The rest of this section explains our qualitative coding methodology. In general we measured rigor using an interrater reliability IRR rate of 80 over 20 of the data by 2 coders using the Jaccard index which is the size of the intersection of the codes divided by the size of the union. We exceeded 20 of the data when we needed more test iterations for agreement or when we had to subset the coding. If a codeset was too complex to do in one pass we coded subcategories. We simplified calculations with different data subset sizes to 20. First we coded for relevance. Shoutcasters should both inform and entertain so they fill dead air time with jokes. To filter out irrelevant utterances two researchers independently coded 1 httpwiki.teamliquid.netstarcraft2PremierTournaments 2 Shoutcasters confirmed each others quality in their consistency of utterance type and content Figure 5. 3 Here caster pair caster or pair for short differentiates our observed individuals from the population of shoutcasters as a whole. Paper 562 32 of statements in the corpus as relevant or irrelevant to explaining the game. We achieved a 95 interrater reliability. Then the researchers split up and coded the rest of the corpus. Research questions RQ1 and RQ2 investigated how the casters seek information onscreen so we used IFT constructs to discover the types of information casters sought and how they unearthed it. For RQ1 the patches in which they sought information we simply counted the casters navigations among patches. Changes in the display screen identified most of these4 for us automatically. For RQ2 how they went about their information foraging we coded the 110 instances of caster navigation by the context where it took place based on player actions Building Fighting Moving Scouting or simply caster navigation. Two researchers independently coded 21 of the data in this manner with IRR of 80. After achieving IRR one researcher coded the remainder of the data. For RQ3 implicit questions the shoutcasters answered we coded the casters utterances by the Lim Dey questions they answered. We added a judgment code to capture caster evaluation on the quality of actions. The complete code set will be detailed in the RQ3 Results section. Using this code set two researchers independently coded 34 of the 1024 explanations in the corpus with 80 interrater reliability Jaccard. After achieving IRR the researchers split up the remainder of the coding. For RQ4 explanation content we drew content coding rules from Metoyer et al.s analysis of explaining Wargus games and added some codes to account for differences in gameplay and study structure. For ease of presentation in this paper we use the terms numeric quantity and indefinite quantity instead of their terms identified discrete and indefinite quantity respectively. Two researchers independently coded the corpus one category at a time e.g. Objects Actions ... achieving an average of 78 IRR on more than 20 of the data in each category. One researcher then finished coding the rest of the corpus. Since all data sources are public we have provided all data and coding rules in supplementary materials to enable replicability and support further research. RESULTS RQ1 Results What information do shoutcasters seek to generate explanations and where do they find it We used two frameworks to investigate casters information seeking behaviors. We turned to the Performance Environ4 But if a caster points at something to bring the others attention to itbut does not have the mousethe viewer cant see it. Page 3 Paper Figure 1. A screenshot from an analyzed game modified to highlight the patches available to our casters HUD 1 bottom Information about current game state e.g. resources held income rate supply and upgrade status Minimap 2 lower left Zoomed out version of the main window Tab 3 top left Provides details on demand currently set on Production Workers killed 4 center left Shows that 9 Red workers have died recently Popup 5 center visualizations that compare player performance usually shown briefly. Regions 3 and 5 will be detailed in Figures 3 and 4. ment Actuators Sensors PEAS model to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory IFT to understand where casters did their information seeking beginning with the places their desired information could be found. These places are called information patches in IFT terminology. imap and Vision Toggles being among the most used patches in Table 2. The casters had superpowers with respect to Sensors and performance measures their interface allowed full observation of the environment whereas players could only partially observe it. The casters extensively used the Minimap and the Vision Toggle as they were the only ways for casters to peer through the players sensors. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets resources successes and failures e.g. Figure 1 region 4 showing that Blue has killed 9 of Reds workers and region 5 showing that Blue has killed 19 units to Reds 3 etc.. Table 2 shows that casters rarely consulted performance measures especially those that examined past game states. However they discussed basic performance measures available in the HUD Figure 1 region 1 which contained present state information e.g. resources held or upgrade status. Actuators were the means for the agents to interact with their environment such as building a unit. Figure 1 region 3 Production Tab shows some of the Actuators the player was using namely that Player Blue was building 5 types of objects whereas Red was building 8. Casters almost always kept visualizations of actions in progress on display. RTS actions had a duration meaning that when a player took an action time passed before its consequence had been realized. The Production tabs popularity was likely due to the fact that it is the only stable view of information about actuators and their associated actions. The Environment where the agent is situated is the game state map structures units etc. shown in Figure 1s main window. We label as Environment any of the patches in Table 2 such as the Units tab that show all the game state data regardless of whether the players or casters have observed it. In fact prior to the game in our corpus Pair 3 had this exchange which demonstrated their perception of the production tabs importance to doing their job Pair 3a What if we took someone who knows literally nothing about StarCraft just teach them a few phrases and what everything is on the production tab Pair 3bOh I would be out of a job. Sensors helped the agent collect information about the environment and corresponded to the local vision area provided by individual units themselves in our domain. Figure 1 region 2 Minimap shows a birds eye view of the portion of the environment observable by the Sensors. Casters used patches containing information about Sensors very often with Min  Paper 562 Implications for an interactive explainer Abstracting beyond the StarCraft components to the PEAS model revealed a pattern of the casters behaviors with implications for future explanation systems which we characterize Page 4 Pair 10 Pair 9 Pair 8 Pair 7 Pair 6 Pair 5 Pair 4 Pair 3 Pair 2 Pair 1 Usage Paper Patch Name State Agg. Units Lost popup Shows count and resource value of the units Past High 6 each player has lost. Units Lost tab Same as above but as a tab. Past High 5 Performance Income tab Provides resource gain rate. Present High 2 Income popup Shows each players resource gain rate and Present High 2 worker count. Army tab Shows supply and resource value of currently held Present High 1 nonworker units. Income Advantage graph Shows time series data comparing Past High 1 resource gain rate. Units Killed popup Essentially the opposite of the Units Lost Past High 1 popup Units tab Shows currently possessed units. Present Low Environment Upgrades tab Like Units tab but for upgrades to units and Present Low buildings. Structures tab Like Units tab but buildings. Present Low Actuator Production tab Shows the units structures and upgrades that Present Low Preferred by choice always on tab not counted are in progress i.e. have been started but are yet to finish. Minimap Shows zoomed out map view Present Med Too many to count Sensor Vision Toggle Shows only the vision available to one of the Present Low players. Table 2. This table illustrates description classification and usage rates of the patches and enrichment operations we observed casters using. Each patch is classified by 1. The part of the PEAS model that this patch illuminates best column 1 2. whether it examines past or present game states column 3 and 3. degree to which the patch aggregates data in its visualization column 4. The remaining columns show total usage counts as well as per caster pair usage. Note that there are additional patches passively available Main Window and HUD which do not have navigation properties. as keep your Sensors close but your Actuators closer. This aligns with other research showing that realtime visualization of agent actions can improve system transparency. However these results contrast with the explanation systems that tend to prioritize Performance measures. Our results instead suggest that an explanation system should prioritize useful readily accessible information about what an agent did or can do Actuators and of what it can see or has seen Sensors. Actuators Cue Goal assess scouting Environment Performance Sensors Cue Units separating fighting likely over Cue Units colocated impending combat likely Figure 2. The A E PS loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators and returns there throughout the foraging process. If a caster interrupted the loop they usually did so to return to the Actuators. RQ2 Results The How How do shoutcasters seek the information they seek Section RQ1 discussed the What and Where i.e. the content casters sought and locations where they sought it. We then considered how they decided to move among these places summarized in Table 2. The casters foraging moves seemed to follow a common foraging loop through the available information patch types an Actuators Environment Performance loop with occasional forays over to Sensors Figure 2. Specifically the casters tended to start at the always on Actuator related patches of current states actions in progress then when something triggered a change in their focus they checked the Environment for current game state information and occasionally Performance measures of past states. If they needed more information along the way they went to the Sensors to see through a players eyes. We will refer to this process as the A E PS loop. Information Foraging Theory IFT explains why people information predators leave one patch to move to another such when the casters left Actuator patches. According to IFT predators choose navigations as costbenefit decisions based on the value of information in the patch a predator is already in Paper 562 versus the value per cost of going to another patch. Staying in the same patch is generally the least expensive but when there is less value to be gained by staying versus moving to another patch the predator moves to the other patch. However the predator is not omniscient decisions are based upon the predators perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types and from the cues signposts in their information environment that point toward content available in other patches. Certain types of cues tended to trigger a move for the casters. Impending combat was the most common cue triggering a move from the Actuators type Production tab to the Environment type Units tab i.e. from A to E in the A E PS loop. In Figure 2 the co location of opposing units served as the cue. This cue indicated imminent combat which led to caster navigation to a new patch to illuminate the environment. In fact combat cues triggered navigations to the Units tab most frequently accounting for 30 of the 51 navigations there Table 2. Page 5 Paper Figure 3. The Units Lost tab left image shows the number of units lost and their total value in terms of resources spent for both players. In this example from Pair 2 we see that Blue Player top has lost merely 2800 minerals worth of units so far in this game while Red has lost more than 7000. The Units Killed popup right image allows shoutcasters to quickly compare player performance via a tug of war visualization. In this example from Pair 1 as we see that Blue Player left has killed 19 units while Red has killed merely 3. The main difference between these two styles of visualization is that the tab offers more options and information depth to drill down into. Figure 4. The Production tab showing the build actions currently in progress for each player. Each unitstructure type is represented by a glyph which serves as a link to navigate to that object provided a progress bar for duration and given the number of objects of that type. Thus we can see that Blue Player top row is building 5 different types of things while Red bottom row is building 4 types of things. The Structures Upgrades and Units tab look fairly similar to the Production tab. Interestingly this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations text or occasionally images in previous IFT investigations that label a navigation device like a hyperlink or button that leads to another information patch. In contrast cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However cues like this were considered cues because they provide users with concise information about content that is not immediately available. They suggested high value in another location in the case of combat the Units tab. Combat ending was a dynamic cue that triggered a move to a Performance measure. 10 of the 13 navigations to a past facing Performance measure via tab or popup occurred shortly after combat ended as a form of after action review Table 2. Occasionally the shoutcasters visited other Performance patches such as the Income Units Lost and Army tabs to demonstrate reasons why a player had accrued an ingame lead or the magnitude of that lead 7 navigations. However signs of completed fighting were the main cues for visiting a Performance patch. The most common detour out of the AE P part of the loop to a Sensor patch was to enrich the information environment via the Vision Toggle 36 navigations Table 2. The data did not reveal exactly what cues led to this move but the move itself had a common theme to assess scouting operations. The casters used the Vision Toggle to allow themselves to see the game through the eyes of only one of the players but their default behavior was to view the game with ALL Paper 562 Sensor was the second most common patch move. Besides the act of following cues IFT has another foraging operation enriching their information environment to make it more valuable or cost efficient. The aforementioned Vision Toggle was one example of this and another was when casters added on information visualizations derived from the raw data like Performance measure popups or other basic visualizations. Two examples of the data obtained through this enrichment are shown in Figure 3. These Performance measures gave the shoutcasters ata glance information about the ways one player was winning. The most commonly used tab for example the Units Lost tab Figure 3 showed the number of units lost and their total value in terms of resources spent. This measure achieves at a glance by aggregating all the data samples together by taking a sum derived values like this allow the visualization to scale to large data sets. However Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type as Figure 4 shows so type information was maintained with only positional data lost. This contrasts with the Minimap medium aggregation in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data HUD but these patches were also the only way to access past state information Table 2. Implications for an interactive explainer These results have several implications for automated explanation systems in this domain. First the A E PS loop and how the casters traversed it reveals priority and timing implications for automated explanation systems. For example the cues that led them to switch to different information patches could also be cues in an automated system about the need to avail different information at appropriate times. For example our casters showed a strong preference for actuator information as steady state visualization but preferred performance information upon conclusion of a subtask. Viewing the casters behaviors through the dual lens of PEAS IFT has implications for not only the kinds of patches that an explanation system would need to provide but also the cost to users of not providing these patches in a readily accessible format. For example PEAS IFT revealed a costly foraging problem for the casters due to the relative inaccessibility of some Actuator patches. There is no easily accessible mechanism in StarCraft by which they could navigate to an Actuator patch with fighting or scouting actions in progress. Instead the only way the casters could get access to these actions was via painstaking camera placement. The casters made countless navigations to move the Page 6 Paper 19 demanded a Why explanation. To contrast with our study only 27 of the casters 1024 utterances approximately 3 were Why answers. Discussion and implications for an interactive explainer Why so few Whys Should an automated explainer like our shoutcasters eschew Why explanations in favor of What Figure 5. Frequency of Lim Dey questions answered by casters with one line per caster pair. Y Axis represents percentages of the utterances which answered that category of question X Axis. Note how casters structured answers consistently. camera using the Minimap traditional scrolling or via tabs with links to the right unit or building. However despite all these navigation affordances sometimes the casters were unable to place the camera on all the actions they needed to see. For example at one point when Pair 4 had the camera on a fight at Xys base a second fight broke out at DeMuslims base which they completely missed Pair 4a surprised noticing something amiss Xy actually killed the 3rd base of DeMuslim. ...the pair tries to figure what must have happened... Pair 4b Oh my god youre right Alex. Pair 4a Yeah it was killed during all that action. RQ3 Results What implicit questions do shoutcasters answer and how do they form their answers We con
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length. The set of this research paper's data gathering methodologies contains [MASK].
6455, 2018, How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games, interviews, Research paper:
ABSTRACT How should an AI based explanation system explain an agents complex behavior to ordinary end users who have no background in AI Answering this question is an active research area for if an AI based explanation system could effectively explain intelligent agents behavior it could enable the end users to understand assess and appropriately trust or distrust the agents attempting to help them. To provide insights into this question we turned to human expert explainers in the realtime strategy domain shoutcasters to understand 1 how they foraged in an evolving strategy game in real time 2 how they assessed the players behaviors and 3 how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters foraging strategies for gleaning information necessary to assess and explain the players a characterization of the types of implicit questions shoutcasters answered and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. CCS Concepts Human centered computing Empirical studies in HCI Computing methodologies Intelligent agents Realtime strategy RTS games are becoming popular artificial intelligence AI research platforms. A number of factors have contributed to this trend. First RTS games are a challenge for AI because they involve realtime adversarial planning within sequential dynamic and partially observable environments. Second AI advancements made in the RTS domain can be mapped to real world combat mission must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. Paper 562 planning and execution such as an AI system trained to control a fleet of drones for missions in simulated environments. People without AI training will need to understand and ultimately assess the decisions of such a system based on what such intelligent systems recommend or decide to do on their own. For example imagine Jake a domain expert trying to make an educated decision about whether or not to use an intelligent agent. Ideally an interactive explanation system could help Jake assess whether and when the AI is making its decisions for the right reasons so as to ward off lucky guesses and legalethical concerns see. Scenarios like this are the motivation for a burgeoning area of research referred to as Explainable AI where an automated explanation device presents an AI systems decisions and actions in a form useful to the intended audience here Jake. There are recent research advances in explainable AI as we discuss in the Related Work section but only a few focus on explaining complex strategy environments like RTS games and fewer draw from expert explainers. To help fill this gap we conducted an investigation in the setting of StarCraft II a popular RTS game available to AI researchers. We looked to shoutcasters who are commentators for esports like RTS games. In StarCraft e sports two players compete while the shoutcasters provide realtime commentary. Shoutcasters are helpful to investigate for explaining AI agents in real time to people like Jake for two reasons. First they face an assessment task similar to Jakes. Specifically they must 1 discover the actions of the player 2 make sense of them and 3 assess them particularly if they discover good bad or unorthodox behavior. They must do all this while simultaneously constructing an explanation of their discoveries. Second shoutcasters are expert explainers. As communication professionals they are paid to inform an audience they cannot see or receive feedbackquestions from. Hoffman Klein researched five stages of explanation looking at how explanations are formed from observation of an event generating one or more possible explanations judging the plausibility of said explanations and either resolving or extending the explanation. Their findings help to illustrate the complexity of shoutcasters task due to its abductive nature of explaining the past and anticipating the future. In short shoutcasters must anticipate and answer the questions the audience are not able to ask all while passively watching the video stream. Page 1 Paper Because shoutcasters explain in parallel to gathering their information we guided part of our investigation using Information Foraging Theory IFT which explains how people go about their information seeking activities. It is based on naturalistic predator prey models in which the predator shoutcaster searches patches parts of the information environment to find prey evidence of players decision process by following the cues signposts in the environment that seem to point toward prey based on their scent predators guess at how related to the prey a cue is. IFT constructs have been used to explain and predict peoples informationseeking behavior in several domains such as understanding navigations through web sites or programming and software engineering environments. However to our knowledge it has not been used before to investigate explaining RTS environments like StarCraft. Using this framework we investigated the following research questions RQs. RQ1 RQ2 investigate the information needs of shoutcasters as human assessors like our target user Jake. Because shoutcasters may not have access to the playersagents internal reasoning this perspective is especially pertinent to model agnostic XAI which ignores internals to achieve generality e.g. RQ1 The What and the Where What information do shoutcasters seek to generate explanations and where do they find it RQ2 The How How do shoutcasters seek the information they seek We then used RQ3 RQ4 to investigate shoutcasters in their role as expert explainers RQ3 The Questions What implicit questions do shoutcasters answer and how do they form their answers RQ4 The Explanations What relationships and objects do shoutcasters use when building their explanations BACKGROUND AND RELATED WORK Another important underpinning for our work is what people want explained. Lim Dey conducted an influential investigation into information demanded from contextaware intelligent systems. They categorized users information needs into various intelligibility types and investigated which types provided the most benefit to user understanding. Among these types were What questions What did the system do Why questions Why did the system do X and so on. We draw upon these results in this paper to categorize the kinds of questions that shoutcasters explanations answered. Other research confirms that explanations containing certain intelligibility types make a difference in user attitude towards the system. For example findings by Cotter et al. showed that justifying why an algorithm works but not on how it works were helpful for increasing users confidence in the system but not for improving their trust. Other work shows that the relative importance of the intellibility types may vary with the domain for example findings by Castelli et al. in the domain of smart homes showed a strong interest in What questions but few of the other intellibility types. Constructing effective explanations of AI is not straightforward especially when the underlying AI system is complex. Both Kulesza et al. and Guestrin et al. point to a potential tradeoff between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain in a black box or model agnostic fashion predictions of any classifier in a faithful way and also approximate it locally with an interpretable model. They described a fidelity interpretability tradeoff in which making an explanation more faithful was likely to reduce its interpretability and vice versa. However humans manage this tradeoff by accounting for many factors such as the audiences current situation their background amount of time available etc. One goal of the current study is to understand how expert human explainers like our shoutcasters manage this tradeoff. Our work draws upon mental models in XAI. Mental models defined as internal representations that people build based on their experiences in the real world enable users like Jake our target user to predict system behavior. Kulesza et al. found those who adjusted their mental models most in response to explanations of AI a recommender system were best able to customize recommendations. Further participants who improved their mental models the most found debugging more worthwhile and engaging. In the domain of assessing RTS intelligent agents Kim et al. invited 20 experienced players to assess the skill levels of AI bots playing StarCraft. They observed that human rankings were different in several ways to a ranking computed from the bots competition win rate because humans weighed certain factors like decisionmaking skill more heavily. The mismatch between empirical results and perception scores may be because AI bots that are effective against each other proved less effective against humans. Building upon this finding Kulesza et al. then identified principles for explaining in a white box fashion to users how a machine learning system makes its predictions more transparent to the user. Participants quality of mental models increased by up to 52 in user studies with a prototype following these principles and along with these improvements came better ability to customize the intelligent agents. Kapoor et al. also showed that explaining AI increased user satisfaction and interacting with the explanations enabled users to construct classifiers that were more aligned with target preferences. Bostandjiev et al.s work on a music recommendation system found that explanation led to a remarkable increase in user satisfaction with their system. Cheung et al. studied StarCraft from a different perspective that of nonparticipant spectators. Their investigations produced a set of nine personas that helped to illuminate who these spectators are and why they watch. Since shoutcasters are one of the personas they discussed how shoutcasters affect the spectator experience and how they judiciously decide how and when to reveal different types of information both to entertain and inform the audience. Another contingent of researchers is working toward applying machine learning to automatically summarize different aspects of sports potentially assisting sportscasters or shoutcasters in their work. Two examples are automatically generating esports statistics and automatically extracting football play diagrams from raw video. Paper 562 Page 2 Paper Tournament Shoutcasters Players Game 2017 IEM Katowice Neeb vs Jjakji 2017 IEM Katowice Rotterdam and Maynarde Harstem vs TY 2017 GSL Season 1 Code S Artosis and tasteless 2016 WESG Finals Tenshi and Zeweig DeMuslim vs iGXY 2017 StarLeague S1 Premier Wolf and Brendan Innovation vs Dark 2016 KeSPA Cup Wolf and Brendan Maru vs Patience 2016 IEM Geonggi Kaelaris and Funka Byun vs Iasonu 2016 IEM Shanghai Rotterdam and Nathanias ShowTime vs Iasonu 3 2016 WCS Global Finals iNcontroL and Rotterdam Nerchio vs Elazer Snute vs ShowTime 10 2016 DreamHack Open Leipzig Rifkin and ZombieGrub Table 1. Summary of StarCraft 2 games studied. Please consult our supplementary materials for transcripts and links to videos. Finally future attempts to generate automated shoutcasting from replays using data such as the corpus reported in this paper could use dialog evaluation systems e.g. to assess the quality of the generated shoutcasters. The closest work to our own is Metoyer et al.s investigation into the vocabulary and language structure of explaining RTS games. In their study novices and experts acted in pairs the novice watched the expert play and asked questions while the expert thought aloud and answered them. They developed qualitative coding schemes of the content and structure of the explanations the expert players offered. In this paper we drew upon these coding schemes with slight modifications. The complete code sets are available in the online supplemental materials. Our work differs from all of these works in that our explainers are expert communicators about the game not participant players programmers or end users. Our work is also the first to apply IFT to XAI. METHODOLOGY In order to study high quality explanations and capable players we considered only games from professional tournaments denoted as Premier by TeamLiquid1 and also filtered out irrelevant utterances as described later in this section. We selected 10 matches from the Premier pool available with video on demand from professional StarCraft 2 tournaments from 2016 and 2017 Table 1. Professional matches have multiple games so we randomly selected one game from each match for analysis. 16 distinct shoutcasters2 appeared across the 10 videos with two casters3 commentating each time. The rest of this section explains our qualitative coding methodology. In general we measured rigor using an interrater reliability IRR rate of 80 over 20 of the data by 2 coders using the Jaccard index which is the size of the intersection of the codes divided by the size of the union. We exceeded 20 of the data when we needed more test iterations for agreement or when we had to subset the coding. If a codeset was too complex to do in one pass we coded subcategories. We simplified calculations with different data subset sizes to 20. First we coded for relevance. Shoutcasters should both inform and entertain so they fill dead air time with jokes. To filter out irrelevant utterances two researchers independently coded 1 httpwiki.teamliquid.netstarcraft2PremierTournaments 2 Shoutcasters confirmed each others quality in their consistency of utterance type and content Figure 5. 3 Here caster pair caster or pair for short differentiates our observed individuals from the population of shoutcasters as a whole. Paper 562 32 of statements in the corpus as relevant or irrelevant to explaining the game. We achieved a 95 interrater reliability. Then the researchers split up and coded the rest of the corpus. Research questions RQ1 and RQ2 investigated how the casters seek information onscreen so we used IFT constructs to discover the types of information casters sought and how they unearthed it. For RQ1 the patches in which they sought information we simply counted the casters navigations among patches. Changes in the display screen identified most of these4 for us automatically. For RQ2 how they went about their information foraging we coded the 110 instances of caster navigation by the context where it took place based on player actions Building Fighting Moving Scouting or simply caster navigation. Two researchers independently coded 21 of the data in this manner with IRR of 80. After achieving IRR one researcher coded the remainder of the data. For RQ3 implicit questions the shoutcasters answered we coded the casters utterances by the Lim Dey questions they answered. We added a judgment code to capture caster evaluation on the quality of actions. The complete code set will be detailed in the RQ3 Results section. Using this code set two researchers independently coded 34 of the 1024 explanations in the corpus with 80 interrater reliability Jaccard. After achieving IRR the researchers split up the remainder of the coding. For RQ4 explanation content we drew content coding rules from Metoyer et al.s analysis of explaining Wargus games and added some codes to account for differences in gameplay and study structure. For ease of presentation in this paper we use the terms numeric quantity and indefinite quantity instead of their terms identified discrete and indefinite quantity respectively. Two researchers independently coded the corpus one category at a time e.g. Objects Actions ... achieving an average of 78 IRR on more than 20 of the data in each category. One researcher then finished coding the rest of the corpus. Since all data sources are public we have provided all data and coding rules in supplementary materials to enable replicability and support further research. RESULTS RQ1 Results What information do shoutcasters seek to generate explanations and where do they find it We used two frameworks to investigate casters information seeking behaviors. We turned to the Performance Environ4 But if a caster points at something to bring the others attention to itbut does not have the mousethe viewer cant see it. Page 3 Paper Figure 1. A screenshot from an analyzed game modified to highlight the patches available to our casters HUD 1 bottom Information about current game state e.g. resources held income rate supply and upgrade status Minimap 2 lower left Zoomed out version of the main window Tab 3 top left Provides details on demand currently set on Production Workers killed 4 center left Shows that 9 Red workers have died recently Popup 5 center visualizations that compare player performance usually shown briefly. Regions 3 and 5 will be detailed in Figures 3 and 4. ment Actuators Sensors PEAS model to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory IFT to understand where casters did their information seeking beginning with the places their desired information could be found. These places are called information patches in IFT terminology. imap and Vision Toggles being among the most used patches in Table 2. The casters had superpowers with respect to Sensors and performance measures their interface allowed full observation of the environment whereas players could only partially observe it. The casters extensively used the Minimap and the Vision Toggle as they were the only ways for casters to peer through the players sensors. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets resources successes and failures e.g. Figure 1 region 4 showing that Blue has killed 9 of Reds workers and region 5 showing that Blue has killed 19 units to Reds 3 etc.. Table 2 shows that casters rarely consulted performance measures especially those that examined past game states. However they discussed basic performance measures available in the HUD Figure 1 region 1 which contained present state information e.g. resources held or upgrade status. Actuators were the means for the agents to interact with their environment such as building a unit. Figure 1 region 3 Production Tab shows some of the Actuators the player was using namely that Player Blue was building 5 types of objects whereas Red was building 8. Casters almost always kept visualizations of actions in progress on display. RTS actions had a duration meaning that when a player took an action time passed before its consequence had been realized. The Production tabs popularity was likely due to the fact that it is the only stable view of information about actuators and their associated actions. The Environment where the agent is situated is the game state map structures units etc. shown in Figure 1s main window. We label as Environment any of the patches in Table 2 such as the Units tab that show all the game state data regardless of whether the players or casters have observed it. In fact prior to the game in our corpus Pair 3 had this exchange which demonstrated their perception of the production tabs importance to doing their job Pair 3a What if we took someone who knows literally nothing about StarCraft just teach them a few phrases and what everything is on the production tab Pair 3bOh I would be out of a job. Sensors helped the agent collect information about the environment and corresponded to the local vision area provided by individual units themselves in our domain. Figure 1 region 2 Minimap shows a birds eye view of the portion of the environment observable by the Sensors. Casters used patches containing information about Sensors very often with Min  Paper 562 Implications for an interactive explainer Abstracting beyond the StarCraft components to the PEAS model revealed a pattern of the casters behaviors with implications for future explanation systems which we characterize Page 4 Pair 10 Pair 9 Pair 8 Pair 7 Pair 6 Pair 5 Pair 4 Pair 3 Pair 2 Pair 1 Usage Paper Patch Name State Agg. Units Lost popup Shows count and resource value of the units Past High 6 each player has lost. Units Lost tab Same as above but as a tab. Past High 5 Performance Income tab Provides resource gain rate. Present High 2 Income popup Shows each players resource gain rate and Present High 2 worker count. Army tab Shows supply and resource value of currently held Present High 1 nonworker units. Income Advantage graph Shows time series data comparing Past High 1 resource gain rate. Units Killed popup Essentially the opposite of the Units Lost Past High 1 popup Units tab Shows currently possessed units. Present Low Environment Upgrades tab Like Units tab but for upgrades to units and Present Low buildings. Structures tab Like Units tab but buildings. Present Low Actuator Production tab Shows the units structures and upgrades that Present Low Preferred by choice always on tab not counted are in progress i.e. have been started but are yet to finish. Minimap Shows zoomed out map view Present Med Too many to count Sensor Vision Toggle Shows only the vision available to one of the Present Low players. Table 2. This table illustrates description classification and usage rates of the patches and enrichment operations we observed casters using. Each patch is classified by 1. The part of the PEAS model that this patch illuminates best column 1 2. whether it examines past or present game states column 3 and 3. degree to which the patch aggregates data in its visualization column 4. The remaining columns show total usage counts as well as per caster pair usage. Note that there are additional patches passively available Main Window and HUD which do not have navigation properties. as keep your Sensors close but your Actuators closer. This aligns with other research showing that realtime visualization of agent actions can improve system transparency. However these results contrast with the explanation systems that tend to prioritize Performance measures. Our results instead suggest that an explanation system should prioritize useful readily accessible information about what an agent did or can do Actuators and of what it can see or has seen Sensors. Actuators Cue Goal assess scouting Environment Performance Sensors Cue Units separating fighting likely over Cue Units colocated impending combat likely Figure 2. The A E PS loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators and returns there throughout the foraging process. If a caster interrupted the loop they usually did so to return to the Actuators. RQ2 Results The How How do shoutcasters seek the information they seek Section RQ1 discussed the What and Where i.e. the content casters sought and locations where they sought it. We then considered how they decided to move among these places summarized in Table 2. The casters foraging moves seemed to follow a common foraging loop through the available information patch types an Actuators Environment Performance loop with occasional forays over to Sensors Figure 2. Specifically the casters tended to start at the always on Actuator related patches of current states actions in progress then when something triggered a change in their focus they checked the Environment for current game state information and occasionally Performance measures of past states. If they needed more information along the way they went to the Sensors to see through a players eyes. We will refer to this process as the A E PS loop. Information Foraging Theory IFT explains why people information predators leave one patch to move to another such when the casters left Actuator patches. According to IFT predators choose navigations as costbenefit decisions based on the value of information in the patch a predator is already in Paper 562 versus the value per cost of going to another patch. Staying in the same patch is generally the least expensive but when there is less value to be gained by staying versus moving to another patch the predator moves to the other patch. However the predator is not omniscient decisions are based upon the predators perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types and from the cues signposts in their information environment that point toward content available in other patches. Certain types of cues tended to trigger a move for the casters. Impending combat was the most common cue triggering a move from the Actuators type Production tab to the Environment type Units tab i.e. from A to E in the A E PS loop. In Figure 2 the co location of opposing units served as the cue. This cue indicated imminent combat which led to caster navigation to a new patch to illuminate the environment. In fact combat cues triggered navigations to the Units tab most frequently accounting for 30 of the 51 navigations there Table 2. Page 5 Paper Figure 3. The Units Lost tab left image shows the number of units lost and their total value in terms of resources spent for both players. In this example from Pair 2 we see that Blue Player top has lost merely 2800 minerals worth of units so far in this game while Red has lost more than 7000. The Units Killed popup right image allows shoutcasters to quickly compare player performance via a tug of war visualization. In this example from Pair 1 as we see that Blue Player left has killed 19 units while Red has killed merely 3. The main difference between these two styles of visualization is that the tab offers more options and information depth to drill down into. Figure 4. The Production tab showing the build actions currently in progress for each player. Each unitstructure type is represented by a glyph which serves as a link to navigate to that object provided a progress bar for duration and given the number of objects of that type. Thus we can see that Blue Player top row is building 5 different types of things while Red bottom row is building 4 types of things. The Structures Upgrades and Units tab look fairly similar to the Production tab. Interestingly this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations text or occasionally images in previous IFT investigations that label a navigation device like a hyperlink or button that leads to another information patch. In contrast cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However cues like this were considered cues because they provide users with concise information about content that is not immediately available. They suggested high value in another location in the case of combat the Units tab. Combat ending was a dynamic cue that triggered a move to a Performance measure. 10 of the 13 navigations to a past facing Performance measure via tab or popup occurred shortly after combat ended as a form of after action review Table 2. Occasionally the shoutcasters visited other Performance patches such as the Income Units Lost and Army tabs to demonstrate reasons why a player had accrued an ingame lead or the magnitude of that lead 7 navigations. However signs of completed fighting were the main cues for visiting a Performance patch. The most common detour out of the AE P part of the loop to a Sensor patch was to enrich the information environment via the Vision Toggle 36 navigations Table 2. The data did not reveal exactly what cues led to this move but the move itself had a common theme to assess scouting operations. The casters used the Vision Toggle to allow themselves to see the game through the eyes of only one of the players but their default behavior was to view the game with ALL Paper 562 Sensor was the second most common patch move. Besides the act of following cues IFT has another foraging operation enriching their information environment to make it more valuable or cost efficient. The aforementioned Vision Toggle was one example of this and another was when casters added on information visualizations derived from the raw data like Performance measure popups or other basic visualizations. Two examples of the data obtained through this enrichment are shown in Figure 3. These Performance measures gave the shoutcasters ata glance information about the ways one player was winning. The most commonly used tab for example the Units Lost tab Figure 3 showed the number of units lost and their total value in terms of resources spent. This measure achieves at a glance by aggregating all the data samples together by taking a sum derived values like this allow the visualization to scale to large data sets. However Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type as Figure 4 shows so type information was maintained with only positional data lost. This contrasts with the Minimap medium aggregation in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data HUD but these patches were also the only way to access past state information Table 2. Implications for an interactive explainer These results have several implications for automated explanation systems in this domain. First the A E PS loop and how the casters traversed it reveals priority and timing implications for automated explanation systems. For example the cues that led them to switch to different information patches could also be cues in an automated system about the need to avail different information at appropriate times. For example our casters showed a strong preference for actuator information as steady state visualization but preferred performance information upon conclusion of a subtask. Viewing the casters behaviors through the dual lens of PEAS IFT has implications for not only the kinds of patches that an explanation system would need to provide but also the cost to users of not providing these patches in a readily accessible format. For example PEAS IFT revealed a costly foraging problem for the casters due to the relative inaccessibility of some Actuator patches. There is no easily accessible mechanism in StarCraft by which they could navigate to an Actuator patch with fighting or scouting actions in progress. Instead the only way the casters could get access to these actions was via painstaking camera placement. The casters made countless navigations to move the Page 6 Paper 19 demanded a Why explanation. To contrast with our study only 27 of the casters 1024 utterances approximately 3 were Why answers. Discussion and implications for an interactive explainer Why so few Whys Should an automated explainer like our shoutcasters eschew Why explanations in favor of What Figure 5. Frequency of Lim Dey questions answered by casters with one line per caster pair. Y Axis represents percentages of the utterances which answered that category of question X Axis. Note how casters structured answers consistently. camera using the Minimap traditional scrolling or via tabs with links to the right unit or building. However despite all these navigation affordances sometimes the casters were unable to place the camera on all the actions they needed to see. For example at one point when Pair 4 had the camera on a fight at Xys base a second fight broke out at DeMuslims base which they completely missed Pair 4a surprised noticing something amiss Xy actually killed the 3rd base of DeMuslim. ...the pair tries to figure what must have happened... Pair 4b Oh my god youre right Alex. Pair 4a Yeah it was killed during all that action. RQ3 Results What implicit questions do shoutcasters answer and how do they form their answers We con
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK].
6455, 2018, How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games, interviews, Research paper:
ABSTRACT How should an AI based explanation system explain an agents complex behavior to ordinary end users who have no background in AI Answering this question is an active research area for if an AI based explanation system could effectively explain intelligent agents behavior it could enable the end users to understand assess and appropriately trust or distrust the agents attempting to help them. To provide insights into this question we turned to human expert explainers in the realtime strategy domain shoutcasters to understand 1 how they foraged in an evolving strategy game in real time 2 how they assessed the players behaviors and 3 how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters foraging strategies for gleaning information necessary to assess and explain the players a characterization of the types of implicit questions shoutcasters answered and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. CCS Concepts Human centered computing Empirical studies in HCI Computing methodologies Intelligent agents Realtime strategy RTS games are becoming popular artificial intelligence AI research platforms. A number of factors have contributed to this trend. First RTS games are a challenge for AI because they involve realtime adversarial planning within sequential dynamic and partially observable environments. Second AI advancements made in the RTS domain can be mapped to real world combat mission must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. Paper 562 planning and execution such as an AI system trained to control a fleet of drones for missions in simulated environments. People without AI training will need to understand and ultimately assess the decisions of such a system based on what such intelligent systems recommend or decide to do on their own. For example imagine Jake a domain expert trying to make an educated decision about whether or not to use an intelligent agent. Ideally an interactive explanation system could help Jake assess whether and when the AI is making its decisions for the right reasons so as to ward off lucky guesses and legalethical concerns see. Scenarios like this are the motivation for a burgeoning area of research referred to as Explainable AI where an automated explanation device presents an AI systems decisions and actions in a form useful to the intended audience here Jake. There are recent research advances in explainable AI as we discuss in the Related Work section but only a few focus on explaining complex strategy environments like RTS games and fewer draw from expert explainers. To help fill this gap we conducted an investigation in the setting of StarCraft II a popular RTS game available to AI researchers. We looked to shoutcasters who are commentators for esports like RTS games. In StarCraft e sports two players compete while the shoutcasters provide realtime commentary. Shoutcasters are helpful to investigate for explaining AI agents in real time to people like Jake for two reasons. First they face an assessment task similar to Jakes. Specifically they must 1 discover the actions of the player 2 make sense of them and 3 assess them particularly if they discover good bad or unorthodox behavior. They must do all this while simultaneously constructing an explanation of their discoveries. Second shoutcasters are expert explainers. As communication professionals they are paid to inform an audience they cannot see or receive feedbackquestions from. Hoffman Klein researched five stages of explanation looking at how explanations are formed from observation of an event generating one or more possible explanations judging the plausibility of said explanations and either resolving or extending the explanation. Their findings help to illustrate the complexity of shoutcasters task due to its abductive nature of explaining the past and anticipating the future. In short shoutcasters must anticipate and answer the questions the audience are not able to ask all while passively watching the video stream. Page 1 Paper Because shoutcasters explain in parallel to gathering their information we guided part of our investigation using Information Foraging Theory IFT which explains how people go about their information seeking activities. It is based on naturalistic predator prey models in which the predator shoutcaster searches patches parts of the information environment to find prey evidence of players decision process by following the cues signposts in the environment that seem to point toward prey based on their scent predators guess at how related to the prey a cue is. IFT constructs have been used to explain and predict peoples informationseeking behavior in several domains such as understanding navigations through web sites or programming and software engineering environments. However to our knowledge it has not been used before to investigate explaining RTS environments like StarCraft. Using this framework we investigated the following research questions RQs. RQ1 RQ2 investigate the information needs of shoutcasters as human assessors like our target user Jake. Because shoutcasters may not have access to the playersagents internal reasoning this perspective is especially pertinent to model agnostic XAI which ignores internals to achieve generality e.g. RQ1 The What and the Where What information do shoutcasters seek to generate explanations and where do they find it RQ2 The How How do shoutcasters seek the information they seek We then used RQ3 RQ4 to investigate shoutcasters in their role as expert explainers RQ3 The Questions What implicit questions do shoutcasters answer and how do they form their answers RQ4 The Explanations What relationships and objects do shoutcasters use when building their explanations BACKGROUND AND RELATED WORK Another important underpinning for our work is what people want explained. Lim Dey conducted an influential investigation into information demanded from contextaware intelligent systems. They categorized users information needs into various intelligibility types and investigated which types provided the most benefit to user understanding. Among these types were What questions What did the system do Why questions Why did the system do X and so on. We draw upon these results in this paper to categorize the kinds of questions that shoutcasters explanations answered. Other research confirms that explanations containing certain intelligibility types make a difference in user attitude towards the system. For example findings by Cotter et al. showed that justifying why an algorithm works but not on how it works were helpful for increasing users confidence in the system but not for improving their trust. Other work shows that the relative importance of the intellibility types may vary with the domain for example findings by Castelli et al. in the domain of smart homes showed a strong interest in What questions but few of the other intellibility types. Constructing effective explanations of AI is not straightforward especially when the underlying AI system is complex. Both Kulesza et al. and Guestrin et al. point to a potential tradeoff between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain in a black box or model agnostic fashion predictions of any classifier in a faithful way and also approximate it locally with an interpretable model. They described a fidelity interpretability tradeoff in which making an explanation more faithful was likely to reduce its interpretability and vice versa. However humans manage this tradeoff by accounting for many factors such as the audiences current situation their background amount of time available etc. One goal of the current study is to understand how expert human explainers like our shoutcasters manage this tradeoff. Our work draws upon mental models in XAI. Mental models defined as internal representations that people build based on their experiences in the real world enable users like Jake our target user to predict system behavior. Kulesza et al. found those who adjusted their mental models most in response to explanations of AI a recommender system were best able to customize recommendations. Further participants who improved their mental models the most found debugging more worthwhile and engaging. In the domain of assessing RTS intelligent agents Kim et al. invited 20 experienced players to assess the skill levels of AI bots playing StarCraft. They observed that human rankings were different in several ways to a ranking computed from the bots competition win rate because humans weighed certain factors like decisionmaking skill more heavily. The mismatch between empirical results and perception scores may be because AI bots that are effective against each other proved less effective against humans. Building upon this finding Kulesza et al. then identified principles for explaining in a white box fashion to users how a machine learning system makes its predictions more transparent to the user. Participants quality of mental models increased by up to 52 in user studies with a prototype following these principles and along with these improvements came better ability to customize the intelligent agents. Kapoor et al. also showed that explaining AI increased user satisfaction and interacting with the explanations enabled users to construct classifiers that were more aligned with target preferences. Bostandjiev et al.s work on a music recommendation system found that explanation led to a remarkable increase in user satisfaction with their system. Cheung et al. studied StarCraft from a different perspective that of nonparticipant spectators. Their investigations produced a set of nine personas that helped to illuminate who these spectators are and why they watch. Since shoutcasters are one of the personas they discussed how shoutcasters affect the spectator experience and how they judiciously decide how and when to reveal different types of information both to entertain and inform the audience. Another contingent of researchers is working toward applying machine learning to automatically summarize different aspects of sports potentially assisting sportscasters or shoutcasters in their work. Two examples are automatically generating esports statistics and automatically extracting football play diagrams from raw video. Paper 562 Page 2 Paper Tournament Shoutcasters Players Game 2017 IEM Katowice Neeb vs Jjakji 2017 IEM Katowice Rotterdam and Maynarde Harstem vs TY 2017 GSL Season 1 Code S Artosis and tasteless 2016 WESG Finals Tenshi and Zeweig DeMuslim vs iGXY 2017 StarLeague S1 Premier Wolf and Brendan Innovation vs Dark 2016 KeSPA Cup Wolf and Brendan Maru vs Patience 2016 IEM Geonggi Kaelaris and Funka Byun vs Iasonu 2016 IEM Shanghai Rotterdam and Nathanias ShowTime vs Iasonu 3 2016 WCS Global Finals iNcontroL and Rotterdam Nerchio vs Elazer Snute vs ShowTime 10 2016 DreamHack Open Leipzig Rifkin and ZombieGrub Table 1. Summary of StarCraft 2 games studied. Please consult our supplementary materials for transcripts and links to videos. Finally future attempts to generate automated shoutcasting from replays using data such as the corpus reported in this paper could use dialog evaluation systems e.g. to assess the quality of the generated shoutcasters. The closest work to our own is Metoyer et al.s investigation into the vocabulary and language structure of explaining RTS games. In their study novices and experts acted in pairs the novice watched the expert play and asked questions while the expert thought aloud and answered them. They developed qualitative coding schemes of the content and structure of the explanations the expert players offered. In this paper we drew upon these coding schemes with slight modifications. The complete code sets are available in the online supplemental materials. Our work differs from all of these works in that our explainers are expert communicators about the game not participant players programmers or end users. Our work is also the first to apply IFT to XAI. METHODOLOGY In order to study high quality explanations and capable players we considered only games from professional tournaments denoted as Premier by TeamLiquid1 and also filtered out irrelevant utterances as described later in this section. We selected 10 matches from the Premier pool available with video on demand from professional StarCraft 2 tournaments from 2016 and 2017 Table 1. Professional matches have multiple games so we randomly selected one game from each match for analysis. 16 distinct shoutcasters2 appeared across the 10 videos with two casters3 commentating each time. The rest of this section explains our qualitative coding methodology. In general we measured rigor using an interrater reliability IRR rate of 80 over 20 of the data by 2 coders using the Jaccard index which is the size of the intersection of the codes divided by the size of the union. We exceeded 20 of the data when we needed more test iterations for agreement or when we had to subset the coding. If a codeset was too complex to do in one pass we coded subcategories. We simplified calculations with different data subset sizes to 20. First we coded for relevance. Shoutcasters should both inform and entertain so they fill dead air time with jokes. To filter out irrelevant utterances two researchers independently coded 1 httpwiki.teamliquid.netstarcraft2PremierTournaments 2 Shoutcasters confirmed each others quality in their consistency of utterance type and content Figure 5. 3 Here caster pair caster or pair for short differentiates our observed individuals from the population of shoutcasters as a whole. Paper 562 32 of statements in the corpus as relevant or irrelevant to explaining the game. We achieved a 95 interrater reliability. Then the researchers split up and coded the rest of the corpus. Research questions RQ1 and RQ2 investigated how the casters seek information onscreen so we used IFT constructs to discover the types of information casters sought and how they unearthed it. For RQ1 the patches in which they sought information we simply counted the casters navigations among patches. Changes in the display screen identified most of these4 for us automatically. For RQ2 how they went about their information foraging we coded the 110 instances of caster navigation by the context where it took place based on player actions Building Fighting Moving Scouting or simply caster navigation. Two researchers independently coded 21 of the data in this manner with IRR of 80. After achieving IRR one researcher coded the remainder of the data. For RQ3 implicit questions the shoutcasters answered we coded the casters utterances by the Lim Dey questions they answered. We added a judgment code to capture caster evaluation on the quality of actions. The complete code set will be detailed in the RQ3 Results section. Using this code set two researchers independently coded 34 of the 1024 explanations in the corpus with 80 interrater reliability Jaccard. After achieving IRR the researchers split up the remainder of the coding. For RQ4 explanation content we drew content coding rules from Metoyer et al.s analysis of explaining Wargus games and added some codes to account for differences in gameplay and study structure. For ease of presentation in this paper we use the terms numeric quantity and indefinite quantity instead of their terms identified discrete and indefinite quantity respectively. Two researchers independently coded the corpus one category at a time e.g. Objects Actions ... achieving an average of 78 IRR on more than 20 of the data in each category. One researcher then finished coding the rest of the corpus. Since all data sources are public we have provided all data and coding rules in supplementary materials to enable replicability and support further research. RESULTS RQ1 Results What information do shoutcasters seek to generate explanations and where do they find it We used two frameworks to investigate casters information seeking behaviors. We turned to the Performance Environ4 But if a caster points at something to bring the others attention to itbut does not have the mousethe viewer cant see it. Page 3 Paper Figure 1. A screenshot from an analyzed game modified to highlight the patches available to our casters HUD 1 bottom Information about current game state e.g. resources held income rate supply and upgrade status Minimap 2 lower left Zoomed out version of the main window Tab 3 top left Provides details on demand currently set on Production Workers killed 4 center left Shows that 9 Red workers have died recently Popup 5 center visualizations that compare player performance usually shown briefly. Regions 3 and 5 will be detailed in Figures 3 and 4. ment Actuators Sensors PEAS model to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory IFT to understand where casters did their information seeking beginning with the places their desired information could be found. These places are called information patches in IFT terminology. imap and Vision Toggles being among the most used patches in Table 2. The casters had superpowers with respect to Sensors and performance measures their interface allowed full observation of the environment whereas players could only partially observe it. The casters extensively used the Minimap and the Vision Toggle as they were the only ways for casters to peer through the players sensors. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets resources successes and failures e.g. Figure 1 region 4 showing that Blue has killed 9 of Reds workers and region 5 showing that Blue has killed 19 units to Reds 3 etc.. Table 2 shows that casters rarely consulted performance measures especially those that examined past game states. However they discussed basic performance measures available in the HUD Figure 1 region 1 which contained present state information e.g. resources held or upgrade status. Actuators were the means for the agents to interact with their environment such as building a unit. Figure 1 region 3 Production Tab shows some of the Actuators the player was using namely that Player Blue was building 5 types of objects whereas Red was building 8. Casters almost always kept visualizations of actions in progress on display. RTS actions had a duration meaning that when a player took an action time passed before its consequence had been realized. The Production tabs popularity was likely due to the fact that it is the only stable view of information about actuators and their associated actions. The Environment where the agent is situated is the game state map structures units etc. shown in Figure 1s main window. We label as Environment any of the patches in Table 2 such as the Units tab that show all the game state data regardless of whether the players or casters have observed it. In fact prior to the game in our corpus Pair 3 had this exchange which demonstrated their perception of the production tabs importance to doing their job Pair 3a What if we took someone who knows literally nothing about StarCraft just teach them a few phrases and what everything is on the production tab Pair 3bOh I would be out of a job. Sensors helped the agent collect information about the environment and corresponded to the local vision area provided by individual units themselves in our domain. Figure 1 region 2 Minimap shows a birds eye view of the portion of the environment observable by the Sensors. Casters used patches containing information about Sensors very often with Min  Paper 562 Implications for an interactive explainer Abstracting beyond the StarCraft components to the PEAS model revealed a pattern of the casters behaviors with implications for future explanation systems which we characterize Page 4 Pair 10 Pair 9 Pair 8 Pair 7 Pair 6 Pair 5 Pair 4 Pair 3 Pair 2 Pair 1 Usage Paper Patch Name State Agg. Units Lost popup Shows count and resource value of the units Past High 6 each player has lost. Units Lost tab Same as above but as a tab. Past High 5 Performance Income tab Provides resource gain rate. Present High 2 Income popup Shows each players resource gain rate and Present High 2 worker count. Army tab Shows supply and resource value of currently held Present High 1 nonworker units. Income Advantage graph Shows time series data comparing Past High 1 resource gain rate. Units Killed popup Essentially the opposite of the Units Lost Past High 1 popup Units tab Shows currently possessed units. Present Low Environment Upgrades tab Like Units tab but for upgrades to units and Present Low buildings. Structures tab Like Units tab but buildings. Present Low Actuator Production tab Shows the units structures and upgrades that Present Low Preferred by choice always on tab not counted are in progress i.e. have been started but are yet to finish. Minimap Shows zoomed out map view Present Med Too many to count Sensor Vision Toggle Shows only the vision available to one of the Present Low players. Table 2. This table illustrates description classification and usage rates of the patches and enrichment operations we observed casters using. Each patch is classified by 1. The part of the PEAS model that this patch illuminates best column 1 2. whether it examines past or present game states column 3 and 3. degree to which the patch aggregates data in its visualization column 4. The remaining columns show total usage counts as well as per caster pair usage. Note that there are additional patches passively available Main Window and HUD which do not have navigation properties. as keep your Sensors close but your Actuators closer. This aligns with other research showing that realtime visualization of agent actions can improve system transparency. However these results contrast with the explanation systems that tend to prioritize Performance measures. Our results instead suggest that an explanation system should prioritize useful readily accessible information about what an agent did or can do Actuators and of what it can see or has seen Sensors. Actuators Cue Goal assess scouting Environment Performance Sensors Cue Units separating fighting likely over Cue Units colocated impending combat likely Figure 2. The A E PS loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators and returns there throughout the foraging process. If a caster interrupted the loop they usually did so to return to the Actuators. RQ2 Results The How How do shoutcasters seek the information they seek Section RQ1 discussed the What and Where i.e. the content casters sought and locations where they sought it. We then considered how they decided to move among these places summarized in Table 2. The casters foraging moves seemed to follow a common foraging loop through the available information patch types an Actuators Environment Performance loop with occasional forays over to Sensors Figure 2. Specifically the casters tended to start at the always on Actuator related patches of current states actions in progress then when something triggered a change in their focus they checked the Environment for current game state information and occasionally Performance measures of past states. If they needed more information along the way they went to the Sensors to see through a players eyes. We will refer to this process as the A E PS loop. Information Foraging Theory IFT explains why people information predators leave one patch to move to another such when the casters left Actuator patches. According to IFT predators choose navigations as costbenefit decisions based on the value of information in the patch a predator is already in Paper 562 versus the value per cost of going to another patch. Staying in the same patch is generally the least expensive but when there is less value to be gained by staying versus moving to another patch the predator moves to the other patch. However the predator is not omniscient decisions are based upon the predators perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types and from the cues signposts in their information environment that point toward content available in other patches. Certain types of cues tended to trigger a move for the casters. Impending combat was the most common cue triggering a move from the Actuators type Production tab to the Environment type Units tab i.e. from A to E in the A E PS loop. In Figure 2 the co location of opposing units served as the cue. This cue indicated imminent combat which led to caster navigation to a new patch to illuminate the environment. In fact combat cues triggered navigations to the Units tab most frequently accounting for 30 of the 51 navigations there Table 2. Page 5 Paper Figure 3. The Units Lost tab left image shows the number of units lost and their total value in terms of resources spent for both players. In this example from Pair 2 we see that Blue Player top has lost merely 2800 minerals worth of units so far in this game while Red has lost more than 7000. The Units Killed popup right image allows shoutcasters to quickly compare player performance via a tug of war visualization. In this example from Pair 1 as we see that Blue Player left has killed 19 units while Red has killed merely 3. The main difference between these two styles of visualization is that the tab offers more options and information depth to drill down into. Figure 4. The Production tab showing the build actions currently in progress for each player. Each unitstructure type is represented by a glyph which serves as a link to navigate to that object provided a progress bar for duration and given the number of objects of that type. Thus we can see that Blue Player top row is building 5 different types of things while Red bottom row is building 4 types of things. The Structures Upgrades and Units tab look fairly similar to the Production tab. Interestingly this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations text or occasionally images in previous IFT investigations that label a navigation device like a hyperlink or button that leads to another information patch. In contrast cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However cues like this were considered cues because they provide users with concise information about content that is not immediately available. They suggested high value in another location in the case of combat the Units tab. Combat ending was a dynamic cue that triggered a move to a Performance measure. 10 of the 13 navigations to a past facing Performance measure via tab or popup occurred shortly after combat ended as a form of after action review Table 2. Occasionally the shoutcasters visited other Performance patches such as the Income Units Lost and Army tabs to demonstrate reasons why a player had accrued an ingame lead or the magnitude of that lead 7 navigations. However signs of completed fighting were the main cues for visiting a Performance patch. The most common detour out of the AE P part of the loop to a Sensor patch was to enrich the information environment via the Vision Toggle 36 navigations Table 2. The data did not reveal exactly what cues led to this move but the move itself had a common theme to assess scouting operations. The casters used the Vision Toggle to allow themselves to see the game through the eyes of only one of the players but their default behavior was to view the game with ALL Paper 562 Sensor was the second most common patch move. Besides the act of following cues IFT has another foraging operation enriching their information environment to make it more valuable or cost efficient. The aforementioned Vision Toggle was one example of this and another was when casters added on information visualizations derived from the raw data like Performance measure popups or other basic visualizations. Two examples of the data obtained through this enrichment are shown in Figure 3. These Performance measures gave the shoutcasters ata glance information about the ways one player was winning. The most commonly used tab for example the Units Lost tab Figure 3 showed the number of units lost and their total value in terms of resources spent. This measure achieves at a glance by aggregating all the data samples together by taking a sum derived values like this allow the visualization to scale to large data sets. However Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type as Figure 4 shows so type information was maintained with only positional data lost. This contrasts with the Minimap medium aggregation in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data HUD but these patches were also the only way to access past state information Table 2. Implications for an interactive explainer These results have several implications for automated explanation systems in this domain. First the A E PS loop and how the casters traversed it reveals priority and timing implications for automated explanation systems. For example the cues that led them to switch to different information patches could also be cues in an automated system about the need to avail different information at appropriate times. For example our casters showed a strong preference for actuator information as steady state visualization but preferred performance information upon conclusion of a subtask. Viewing the casters behaviors through the dual lens of PEAS IFT has implications for not only the kinds of patches that an explanation system would need to provide but also the cost to users of not providing these patches in a readily accessible format. For example PEAS IFT revealed a costly foraging problem for the casters due to the relative inaccessibility of some Actuator patches. There is no easily accessible mechanism in StarCraft by which they could navigate to an Actuator patch with fighting or scouting actions in progress. Instead the only way the casters could get access to these actions was via painstaking camera placement. The casters made countless navigations to move the Page 6 Paper 19 demanded a Why explanation. To contrast with our study only 27 of the casters 1024 utterances approximately 3 were Why answers. Discussion and implications for an interactive explainer Why so few Whys Should an automated explainer like our shoutcasters eschew Why explanations in favor of What Figure 5. Frequency of Lim Dey questions answered by casters with one line per caster pair. Y Axis represents percentages of the utterances which answered that category of question X Axis. Note how casters structured answers consistently. camera using the Minimap traditional scrolling or via tabs with links to the right unit or building. However despite all these navigation affordances sometimes the casters were unable to place the camera on all the actions they needed to see. For example at one point when Pair 4 had the camera on a fight at Xys base a second fight broke out at DeMuslims base which they completely missed Pair 4a surprised noticing something amiss Xy actually killed the 3rd base of DeMuslim. ...the pair tries to figure what must have happened... Pair 4b Oh my god youre right Alex. Pair 4a Yeah it was killed during all that action. RQ3 Results What implicit questions do shoutcasters answer and how do they form their answers We con
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. Be confident in your assessment before adding a methodology to the set. The set of this research paper's data gathering methodologies contains [MASK].
6455, 2018, How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games, interviews, Research paper:
ABSTRACT How should an AI based explanation system explain an agents complex behavior to ordinary end users who have no background in AI Answering this question is an active research area for if an AI based explanation system could effectively explain intelligent agents behavior it could enable the end users to understand assess and appropriately trust or distrust the agents attempting to help them. To provide insights into this question we turned to human expert explainers in the realtime strategy domain shoutcasters to understand 1 how they foraged in an evolving strategy game in real time 2 how they assessed the players behaviors and 3 how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters foraging strategies for gleaning information necessary to assess and explain the players a characterization of the types of implicit questions shoutcasters answered and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. CCS Concepts Human centered computing Empirical studies in HCI Computing methodologies Intelligent agents Realtime strategy RTS games are becoming popular artificial intelligence AI research platforms. A number of factors have contributed to this trend. First RTS games are a challenge for AI because they involve realtime adversarial planning within sequential dynamic and partially observable environments. Second AI advancements made in the RTS domain can be mapped to real world combat mission must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. Paper 562 planning and execution such as an AI system trained to control a fleet of drones for missions in simulated environments. People without AI training will need to understand and ultimately assess the decisions of such a system based on what such intelligent systems recommend or decide to do on their own. For example imagine Jake a domain expert trying to make an educated decision about whether or not to use an intelligent agent. Ideally an interactive explanation system could help Jake assess whether and when the AI is making its decisions for the right reasons so as to ward off lucky guesses and legalethical concerns see. Scenarios like this are the motivation for a burgeoning area of research referred to as Explainable AI where an automated explanation device presents an AI systems decisions and actions in a form useful to the intended audience here Jake. There are recent research advances in explainable AI as we discuss in the Related Work section but only a few focus on explaining complex strategy environments like RTS games and fewer draw from expert explainers. To help fill this gap we conducted an investigation in the setting of StarCraft II a popular RTS game available to AI researchers. We looked to shoutcasters who are commentators for esports like RTS games. In StarCraft e sports two players compete while the shoutcasters provide realtime commentary. Shoutcasters are helpful to investigate for explaining AI agents in real time to people like Jake for two reasons. First they face an assessment task similar to Jakes. Specifically they must 1 discover the actions of the player 2 make sense of them and 3 assess them particularly if they discover good bad or unorthodox behavior. They must do all this while simultaneously constructing an explanation of their discoveries. Second shoutcasters are expert explainers. As communication professionals they are paid to inform an audience they cannot see or receive feedbackquestions from. Hoffman Klein researched five stages of explanation looking at how explanations are formed from observation of an event generating one or more possible explanations judging the plausibility of said explanations and either resolving or extending the explanation. Their findings help to illustrate the complexity of shoutcasters task due to its abductive nature of explaining the past and anticipating the future. In short shoutcasters must anticipate and answer the questions the audience are not able to ask all while passively watching the video stream. Page 1 Paper Because shoutcasters explain in parallel to gathering their information we guided part of our investigation using Information Foraging Theory IFT which explains how people go about their information seeking activities. It is based on naturalistic predator prey models in which the predator shoutcaster searches patches parts of the information environment to find prey evidence of players decision process by following the cues signposts in the environment that seem to point toward prey based on their scent predators guess at how related to the prey a cue is. IFT constructs have been used to explain and predict peoples informationseeking behavior in several domains such as understanding navigations through web sites or programming and software engineering environments. However to our knowledge it has not been used before to investigate explaining RTS environments like StarCraft. Using this framework we investigated the following research questions RQs. RQ1 RQ2 investigate the information needs of shoutcasters as human assessors like our target user Jake. Because shoutcasters may not have access to the playersagents internal reasoning this perspective is especially pertinent to model agnostic XAI which ignores internals to achieve generality e.g. RQ1 The What and the Where What information do shoutcasters seek to generate explanations and where do they find it RQ2 The How How do shoutcasters seek the information they seek We then used RQ3 RQ4 to investigate shoutcasters in their role as expert explainers RQ3 The Questions What implicit questions do shoutcasters answer and how do they form their answers RQ4 The Explanations What relationships and objects do shoutcasters use when building their explanations BACKGROUND AND RELATED WORK Another important underpinning for our work is what people want explained. Lim Dey conducted an influential investigation into information demanded from contextaware intelligent systems. They categorized users information needs into various intelligibility types and investigated which types provided the most benefit to user understanding. Among these types were What questions What did the system do Why questions Why did the system do X and so on. We draw upon these results in this paper to categorize the kinds of questions that shoutcasters explanations answered. Other research confirms that explanations containing certain intelligibility types make a difference in user attitude towards the system. For example findings by Cotter et al. showed that justifying why an algorithm works but not on how it works were helpful for increasing users confidence in the system but not for improving their trust. Other work shows that the relative importance of the intellibility types may vary with the domain for example findings by Castelli et al. in the domain of smart homes showed a strong interest in What questions but few of the other intellibility types. Constructing effective explanations of AI is not straightforward especially when the underlying AI system is complex. Both Kulesza et al. and Guestrin et al. point to a potential tradeoff between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain in a black box or model agnostic fashion predictions of any classifier in a faithful way and also approximate it locally with an interpretable model. They described a fidelity interpretability tradeoff in which making an explanation more faithful was likely to reduce its interpretability and vice versa. However humans manage this tradeoff by accounting for many factors such as the audiences current situation their background amount of time available etc. One goal of the current study is to understand how expert human explainers like our shoutcasters manage this tradeoff. Our work draws upon mental models in XAI. Mental models defined as internal representations that people build based on their experiences in the real world enable users like Jake our target user to predict system behavior. Kulesza et al. found those who adjusted their mental models most in response to explanations of AI a recommender system were best able to customize recommendations. Further participants who improved their mental models the most found debugging more worthwhile and engaging. In the domain of assessing RTS intelligent agents Kim et al. invited 20 experienced players to assess the skill levels of AI bots playing StarCraft. They observed that human rankings were different in several ways to a ranking computed from the bots competition win rate because humans weighed certain factors like decisionmaking skill more heavily. The mismatch between empirical results and perception scores may be because AI bots that are effective against each other proved less effective against humans. Building upon this finding Kulesza et al. then identified principles for explaining in a white box fashion to users how a machine learning system makes its predictions more transparent to the user. Participants quality of mental models increased by up to 52 in user studies with a prototype following these principles and along with these improvements came better ability to customize the intelligent agents. Kapoor et al. also showed that explaining AI increased user satisfaction and interacting with the explanations enabled users to construct classifiers that were more aligned with target preferences. Bostandjiev et al.s work on a music recommendation system found that explanation led to a remarkable increase in user satisfaction with their system. Cheung et al. studied StarCraft from a different perspective that of nonparticipant spectators. Their investigations produced a set of nine personas that helped to illuminate who these spectators are and why they watch. Since shoutcasters are one of the personas they discussed how shoutcasters affect the spectator experience and how they judiciously decide how and when to reveal different types of information both to entertain and inform the audience. Another contingent of researchers is working toward applying machine learning to automatically summarize different aspects of sports potentially assisting sportscasters or shoutcasters in their work. Two examples are automatically generating esports statistics and automatically extracting football play diagrams from raw video. Paper 562 Page 2 Paper Tournament Shoutcasters Players Game 2017 IEM Katowice Neeb vs Jjakji 2017 IEM Katowice Rotterdam and Maynarde Harstem vs TY 2017 GSL Season 1 Code S Artosis and tasteless 2016 WESG Finals Tenshi and Zeweig DeMuslim vs iGXY 2017 StarLeague S1 Premier Wolf and Brendan Innovation vs Dark 2016 KeSPA Cup Wolf and Brendan Maru vs Patience 2016 IEM Geonggi Kaelaris and Funka Byun vs Iasonu 2016 IEM Shanghai Rotterdam and Nathanias ShowTime vs Iasonu 3 2016 WCS Global Finals iNcontroL and Rotterdam Nerchio vs Elazer Snute vs ShowTime 10 2016 DreamHack Open Leipzig Rifkin and ZombieGrub Table 1. Summary of StarCraft 2 games studied. Please consult our supplementary materials for transcripts and links to videos. Finally future attempts to generate automated shoutcasting from replays using data such as the corpus reported in this paper could use dialog evaluation systems e.g. to assess the quality of the generated shoutcasters. The closest work to our own is Metoyer et al.s investigation into the vocabulary and language structure of explaining RTS games. In their study novices and experts acted in pairs the novice watched the expert play and asked questions while the expert thought aloud and answered them. They developed qualitative coding schemes of the content and structure of the explanations the expert players offered. In this paper we drew upon these coding schemes with slight modifications. The complete code sets are available in the online supplemental materials. Our work differs from all of these works in that our explainers are expert communicators about the game not participant players programmers or end users. Our work is also the first to apply IFT to XAI. METHODOLOGY In order to study high quality explanations and capable players we considered only games from professional tournaments denoted as Premier by TeamLiquid1 and also filtered out irrelevant utterances as described later in this section. We selected 10 matches from the Premier pool available with video on demand from professional StarCraft 2 tournaments from 2016 and 2017 Table 1. Professional matches have multiple games so we randomly selected one game from each match for analysis. 16 distinct shoutcasters2 appeared across the 10 videos with two casters3 commentating each time. The rest of this section explains our qualitative coding methodology. In general we measured rigor using an interrater reliability IRR rate of 80 over 20 of the data by 2 coders using the Jaccard index which is the size of the intersection of the codes divided by the size of the union. We exceeded 20 of the data when we needed more test iterations for agreement or when we had to subset the coding. If a codeset was too complex to do in one pass we coded subcategories. We simplified calculations with different data subset sizes to 20. First we coded for relevance. Shoutcasters should both inform and entertain so they fill dead air time with jokes. To filter out irrelevant utterances two researchers independently coded 1 httpwiki.teamliquid.netstarcraft2PremierTournaments 2 Shoutcasters confirmed each others quality in their consistency of utterance type and content Figure 5. 3 Here caster pair caster or pair for short differentiates our observed individuals from the population of shoutcasters as a whole. Paper 562 32 of statements in the corpus as relevant or irrelevant to explaining the game. We achieved a 95 interrater reliability. Then the researchers split up and coded the rest of the corpus. Research questions RQ1 and RQ2 investigated how the casters seek information onscreen so we used IFT constructs to discover the types of information casters sought and how they unearthed it. For RQ1 the patches in which they sought information we simply counted the casters navigations among patches. Changes in the display screen identified most of these4 for us automatically. For RQ2 how they went about their information foraging we coded the 110 instances of caster navigation by the context where it took place based on player actions Building Fighting Moving Scouting or simply caster navigation. Two researchers independently coded 21 of the data in this manner with IRR of 80. After achieving IRR one researcher coded the remainder of the data. For RQ3 implicit questions the shoutcasters answered we coded the casters utterances by the Lim Dey questions they answered. We added a judgment code to capture caster evaluation on the quality of actions. The complete code set will be detailed in the RQ3 Results section. Using this code set two researchers independently coded 34 of the 1024 explanations in the corpus with 80 interrater reliability Jaccard. After achieving IRR the researchers split up the remainder of the coding. For RQ4 explanation content we drew content coding rules from Metoyer et al.s analysis of explaining Wargus games and added some codes to account for differences in gameplay and study structure. For ease of presentation in this paper we use the terms numeric quantity and indefinite quantity instead of their terms identified discrete and indefinite quantity respectively. Two researchers independently coded the corpus one category at a time e.g. Objects Actions ... achieving an average of 78 IRR on more than 20 of the data in each category. One researcher then finished coding the rest of the corpus. Since all data sources are public we have provided all data and coding rules in supplementary materials to enable replicability and support further research. RESULTS RQ1 Results What information do shoutcasters seek to generate explanations and where do they find it We used two frameworks to investigate casters information seeking behaviors. We turned to the Performance Environ4 But if a caster points at something to bring the others attention to itbut does not have the mousethe viewer cant see it. Page 3 Paper Figure 1. A screenshot from an analyzed game modified to highlight the patches available to our casters HUD 1 bottom Information about current game state e.g. resources held income rate supply and upgrade status Minimap 2 lower left Zoomed out version of the main window Tab 3 top left Provides details on demand currently set on Production Workers killed 4 center left Shows that 9 Red workers have died recently Popup 5 center visualizations that compare player performance usually shown briefly. Regions 3 and 5 will be detailed in Figures 3 and 4. ment Actuators Sensors PEAS model to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory IFT to understand where casters did their information seeking beginning with the places their desired information could be found. These places are called information patches in IFT terminology. imap and Vision Toggles being among the most used patches in Table 2. The casters had superpowers with respect to Sensors and performance measures their interface allowed full observation of the environment whereas players could only partially observe it. The casters extensively used the Minimap and the Vision Toggle as they were the only ways for casters to peer through the players sensors. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets resources successes and failures e.g. Figure 1 region 4 showing that Blue has killed 9 of Reds workers and region 5 showing that Blue has killed 19 units to Reds 3 etc.. Table 2 shows that casters rarely consulted performance measures especially those that examined past game states. However they discussed basic performance measures available in the HUD Figure 1 region 1 which contained present state information e.g. resources held or upgrade status. Actuators were the means for the agents to interact with their environment such as building a unit. Figure 1 region 3 Production Tab shows some of the Actuators the player was using namely that Player Blue was building 5 types of objects whereas Red was building 8. Casters almost always kept visualizations of actions in progress on display. RTS actions had a duration meaning that when a player took an action time passed before its consequence had been realized. The Production tabs popularity was likely due to the fact that it is the only stable view of information about actuators and their associated actions. The Environment where the agent is situated is the game state map structures units etc. shown in Figure 1s main window. We label as Environment any of the patches in Table 2 such as the Units tab that show all the game state data regardless of whether the players or casters have observed it. In fact prior to the game in our corpus Pair 3 had this exchange which demonstrated their perception of the production tabs importance to doing their job Pair 3a What if we took someone who knows literally nothing about StarCraft just teach them a few phrases and what everything is on the production tab Pair 3bOh I would be out of a job. Sensors helped the agent collect information about the environment and corresponded to the local vision area provided by individual units themselves in our domain. Figure 1 region 2 Minimap shows a birds eye view of the portion of the environment observable by the Sensors. Casters used patches containing information about Sensors very often with Min  Paper 562 Implications for an interactive explainer Abstracting beyond the StarCraft components to the PEAS model revealed a pattern of the casters behaviors with implications for future explanation systems which we characterize Page 4 Pair 10 Pair 9 Pair 8 Pair 7 Pair 6 Pair 5 Pair 4 Pair 3 Pair 2 Pair 1 Usage Paper Patch Name State Agg. Units Lost popup Shows count and resource value of the units Past High 6 each player has lost. Units Lost tab Same as above but as a tab. Past High 5 Performance Income tab Provides resource gain rate. Present High 2 Income popup Shows each players resource gain rate and Present High 2 worker count. Army tab Shows supply and resource value of currently held Present High 1 nonworker units. Income Advantage graph Shows time series data comparing Past High 1 resource gain rate. Units Killed popup Essentially the opposite of the Units Lost Past High 1 popup Units tab Shows currently possessed units. Present Low Environment Upgrades tab Like Units tab but for upgrades to units and Present Low buildings. Structures tab Like Units tab but buildings. Present Low Actuator Production tab Shows the units structures and upgrades that Present Low Preferred by choice always on tab not counted are in progress i.e. have been started but are yet to finish. Minimap Shows zoomed out map view Present Med Too many to count Sensor Vision Toggle Shows only the vision available to one of the Present Low players. Table 2. This table illustrates description classification and usage rates of the patches and enrichment operations we observed casters using. Each patch is classified by 1. The part of the PEAS model that this patch illuminates best column 1 2. whether it examines past or present game states column 3 and 3. degree to which the patch aggregates data in its visualization column 4. The remaining columns show total usage counts as well as per caster pair usage. Note that there are additional patches passively available Main Window and HUD which do not have navigation properties. as keep your Sensors close but your Actuators closer. This aligns with other research showing that realtime visualization of agent actions can improve system transparency. However these results contrast with the explanation systems that tend to prioritize Performance measures. Our results instead suggest that an explanation system should prioritize useful readily accessible information about what an agent did or can do Actuators and of what it can see or has seen Sensors. Actuators Cue Goal assess scouting Environment Performance Sensors Cue Units separating fighting likely over Cue Units colocated impending combat likely Figure 2. The A E PS loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators and returns there throughout the foraging process. If a caster interrupted the loop they usually did so to return to the Actuators. RQ2 Results The How How do shoutcasters seek the information they seek Section RQ1 discussed the What and Where i.e. the content casters sought and locations where they sought it. We then considered how they decided to move among these places summarized in Table 2. The casters foraging moves seemed to follow a common foraging loop through the available information patch types an Actuators Environment Performance loop with occasional forays over to Sensors Figure 2. Specifically the casters tended to start at the always on Actuator related patches of current states actions in progress then when something triggered a change in their focus they checked the Environment for current game state information and occasionally Performance measures of past states. If they needed more information along the way they went to the Sensors to see through a players eyes. We will refer to this process as the A E PS loop. Information Foraging Theory IFT explains why people information predators leave one patch to move to another such when the casters left Actuator patches. According to IFT predators choose navigations as costbenefit decisions based on the value of information in the patch a predator is already in Paper 562 versus the value per cost of going to another patch. Staying in the same patch is generally the least expensive but when there is less value to be gained by staying versus moving to another patch the predator moves to the other patch. However the predator is not omniscient decisions are based upon the predators perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types and from the cues signposts in their information environment that point toward content available in other patches. Certain types of cues tended to trigger a move for the casters. Impending combat was the most common cue triggering a move from the Actuators type Production tab to the Environment type Units tab i.e. from A to E in the A E PS loop. In Figure 2 the co location of opposing units served as the cue. This cue indicated imminent combat which led to caster navigation to a new patch to illuminate the environment. In fact combat cues triggered navigations to the Units tab most frequently accounting for 30 of the 51 navigations there Table 2. Page 5 Paper Figure 3. The Units Lost tab left image shows the number of units lost and their total value in terms of resources spent for both players. In this example from Pair 2 we see that Blue Player top has lost merely 2800 minerals worth of units so far in this game while Red has lost more than 7000. The Units Killed popup right image allows shoutcasters to quickly compare player performance via a tug of war visualization. In this example from Pair 1 as we see that Blue Player left has killed 19 units while Red has killed merely 3. The main difference between these two styles of visualization is that the tab offers more options and information depth to drill down into. Figure 4. The Production tab showing the build actions currently in progress for each player. Each unitstructure type is represented by a glyph which serves as a link to navigate to that object provided a progress bar for duration and given the number of objects of that type. Thus we can see that Blue Player top row is building 5 different types of things while Red bottom row is building 4 types of things. The Structures Upgrades and Units tab look fairly similar to the Production tab. Interestingly this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations text or occasionally images in previous IFT investigations that label a navigation device like a hyperlink or button that leads to another information patch. In contrast cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However cues like this were considered cues because they provide users with concise information about content that is not immediately available. They suggested high value in another location in the case of combat the Units tab. Combat ending was a dynamic cue that triggered a move to a Performance measure. 10 of the 13 navigations to a past facing Performance measure via tab or popup occurred shortly after combat ended as a form of after action review Table 2. Occasionally the shoutcasters visited other Performance patches such as the Income Units Lost and Army tabs to demonstrate reasons why a player had accrued an ingame lead or the magnitude of that lead 7 navigations. However signs of completed fighting were the main cues for visiting a Performance patch. The most common detour out of the AE P part of the loop to a Sensor patch was to enrich the information environment via the Vision Toggle 36 navigations Table 2. The data did not reveal exactly what cues led to this move but the move itself had a common theme to assess scouting operations. The casters used the Vision Toggle to allow themselves to see the game through the eyes of only one of the players but their default behavior was to view the game with ALL Paper 562 Sensor was the second most common patch move. Besides the act of following cues IFT has another foraging operation enriching their information environment to make it more valuable or cost efficient. The aforementioned Vision Toggle was one example of this and another was when casters added on information visualizations derived from the raw data like Performance measure popups or other basic visualizations. Two examples of the data obtained through this enrichment are shown in Figure 3. These Performance measures gave the shoutcasters ata glance information about the ways one player was winning. The most commonly used tab for example the Units Lost tab Figure 3 showed the number of units lost and their total value in terms of resources spent. This measure achieves at a glance by aggregating all the data samples together by taking a sum derived values like this allow the visualization to scale to large data sets. However Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type as Figure 4 shows so type information was maintained with only positional data lost. This contrasts with the Minimap medium aggregation in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data HUD but these patches were also the only way to access past state information Table 2. Implications for an interactive explainer These results have several implications for automated explanation systems in this domain. First the A E PS loop and how the casters traversed it reveals priority and timing implications for automated explanation systems. For example the cues that led them to switch to different information patches could also be cues in an automated system about the need to avail different information at appropriate times. For example our casters showed a strong preference for actuator information as steady state visualization but preferred performance information upon conclusion of a subtask. Viewing the casters behaviors through the dual lens of PEAS IFT has implications for not only the kinds of patches that an explanation system would need to provide but also the cost to users of not providing these patches in a readily accessible format. For example PEAS IFT revealed a costly foraging problem for the casters due to the relative inaccessibility of some Actuator patches. There is no easily accessible mechanism in StarCraft by which they could navigate to an Actuator patch with fighting or scouting actions in progress. Instead the only way the casters could get access to these actions was via painstaking camera placement. The casters made countless navigations to move the Page 6 Paper 19 demanded a Why explanation. To contrast with our study only 27 of the casters 1024 utterances approximately 3 were Why answers. Discussion and implications for an interactive explainer Why so few Whys Should an automated explainer like our shoutcasters eschew Why explanations in favor of What Figure 5. Frequency of Lim Dey questions answered by casters with one line per caster pair. Y Axis represents percentages of the utterances which answered that category of question X Axis. Note how casters structured answers consistently. camera using the Minimap traditional scrolling or via tabs with links to the right unit or building. However despite all these navigation affordances sometimes the casters were unable to place the camera on all the actions they needed to see. For example at one point when Pair 4 had the camera on a fight at Xys base a second fight broke out at DeMuslims base which they completely missed Pair 4a surprised noticing something amiss Xy actually killed the 3rd base of DeMuslim. ...the pair tries to figure what must have happened... Pair 4b Oh my god youre right Alex. Pair 4a Yeah it was killed during all that action. RQ3 Results What implicit questions do shoutcasters answer and how do they form their answers We con
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your assessment before adding a methodology to the set.
6455, 2018, How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games, interviews, Research paper:
ABSTRACT How should an AI based explanation system explain an agents complex behavior to ordinary end users who have no background in AI Answering this question is an active research area for if an AI based explanation system could effectively explain intelligent agents behavior it could enable the end users to understand assess and appropriately trust or distrust the agents attempting to help them. To provide insights into this question we turned to human expert explainers in the realtime strategy domain shoutcasters to understand 1 how they foraged in an evolving strategy game in real time 2 how they assessed the players behaviors and 3 how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters foraging strategies for gleaning information necessary to assess and explain the players a characterization of the types of implicit questions shoutcasters answered and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. CCS Concepts Human centered computing Empirical studies in HCI Computing methodologies Intelligent agents Realtime strategy RTS games are becoming popular artificial intelligence AI research platforms. A number of factors have contributed to this trend. First RTS games are a challenge for AI because they involve realtime adversarial planning within sequential dynamic and partially observable environments. Second AI advancements made in the RTS domain can be mapped to real world combat mission must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. Paper 562 planning and execution such as an AI system trained to control a fleet of drones for missions in simulated environments. People without AI training will need to understand and ultimately assess the decisions of such a system based on what such intelligent systems recommend or decide to do on their own. For example imagine Jake a domain expert trying to make an educated decision about whether or not to use an intelligent agent. Ideally an interactive explanation system could help Jake assess whether and when the AI is making its decisions for the right reasons so as to ward off lucky guesses and legalethical concerns see. Scenarios like this are the motivation for a burgeoning area of research referred to as Explainable AI where an automated explanation device presents an AI systems decisions and actions in a form useful to the intended audience here Jake. There are recent research advances in explainable AI as we discuss in the Related Work section but only a few focus on explaining complex strategy environments like RTS games and fewer draw from expert explainers. To help fill this gap we conducted an investigation in the setting of StarCraft II a popular RTS game available to AI researchers. We looked to shoutcasters who are commentators for esports like RTS games. In StarCraft e sports two players compete while the shoutcasters provide realtime commentary. Shoutcasters are helpful to investigate for explaining AI agents in real time to people like Jake for two reasons. First they face an assessment task similar to Jakes. Specifically they must 1 discover the actions of the player 2 make sense of them and 3 assess them particularly if they discover good bad or unorthodox behavior. They must do all this while simultaneously constructing an explanation of their discoveries. Second shoutcasters are expert explainers. As communication professionals they are paid to inform an audience they cannot see or receive feedbackquestions from. Hoffman Klein researched five stages of explanation looking at how explanations are formed from observation of an event generating one or more possible explanations judging the plausibility of said explanations and either resolving or extending the explanation. Their findings help to illustrate the complexity of shoutcasters task due to its abductive nature of explaining the past and anticipating the future. In short shoutcasters must anticipate and answer the questions the audience are not able to ask all while passively watching the video stream. Page 1 Paper Because shoutcasters explain in parallel to gathering their information we guided part of our investigation using Information Foraging Theory IFT which explains how people go about their information seeking activities. It is based on naturalistic predator prey models in which the predator shoutcaster searches patches parts of the information environment to find prey evidence of players decision process by following the cues signposts in the environment that seem to point toward prey based on their scent predators guess at how related to the prey a cue is. IFT constructs have been used to explain and predict peoples informationseeking behavior in several domains such as understanding navigations through web sites or programming and software engineering environments. However to our knowledge it has not been used before to investigate explaining RTS environments like StarCraft. Using this framework we investigated the following research questions RQs. RQ1 RQ2 investigate the information needs of shoutcasters as human assessors like our target user Jake. Because shoutcasters may not have access to the playersagents internal reasoning this perspective is especially pertinent to model agnostic XAI which ignores internals to achieve generality e.g. RQ1 The What and the Where What information do shoutcasters seek to generate explanations and where do they find it RQ2 The How How do shoutcasters seek the information they seek We then used RQ3 RQ4 to investigate shoutcasters in their role as expert explainers RQ3 The Questions What implicit questions do shoutcasters answer and how do they form their answers RQ4 The Explanations What relationships and objects do shoutcasters use when building their explanations BACKGROUND AND RELATED WORK Another important underpinning for our work is what people want explained. Lim Dey conducted an influential investigation into information demanded from contextaware intelligent systems. They categorized users information needs into various intelligibility types and investigated which types provided the most benefit to user understanding. Among these types were What questions What did the system do Why questions Why did the system do X and so on. We draw upon these results in this paper to categorize the kinds of questions that shoutcasters explanations answered. Other research confirms that explanations containing certain intelligibility types make a difference in user attitude towards the system. For example findings by Cotter et al. showed that justifying why an algorithm works but not on how it works were helpful for increasing users confidence in the system but not for improving their trust. Other work shows that the relative importance of the intellibility types may vary with the domain for example findings by Castelli et al. in the domain of smart homes showed a strong interest in What questions but few of the other intellibility types. Constructing effective explanations of AI is not straightforward especially when the underlying AI system is complex. Both Kulesza et al. and Guestrin et al. point to a potential tradeoff between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain in a black box or model agnostic fashion predictions of any classifier in a faithful way and also approximate it locally with an interpretable model. They described a fidelity interpretability tradeoff in which making an explanation more faithful was likely to reduce its interpretability and vice versa. However humans manage this tradeoff by accounting for many factors such as the audiences current situation their background amount of time available etc. One goal of the current study is to understand how expert human explainers like our shoutcasters manage this tradeoff. Our work draws upon mental models in XAI. Mental models defined as internal representations that people build based on their experiences in the real world enable users like Jake our target user to predict system behavior. Kulesza et al. found those who adjusted their mental models most in response to explanations of AI a recommender system were best able to customize recommendations. Further participants who improved their mental models the most found debugging more worthwhile and engaging. In the domain of assessing RTS intelligent agents Kim et al. invited 20 experienced players to assess the skill levels of AI bots playing StarCraft. They observed that human rankings were different in several ways to a ranking computed from the bots competition win rate because humans weighed certain factors like decisionmaking skill more heavily. The mismatch between empirical results and perception scores may be because AI bots that are effective against each other proved less effective against humans. Building upon this finding Kulesza et al. then identified principles for explaining in a white box fashion to users how a machine learning system makes its predictions more transparent to the user. Participants quality of mental models increased by up to 52 in user studies with a prototype following these principles and along with these improvements came better ability to customize the intelligent agents. Kapoor et al. also showed that explaining AI increased user satisfaction and interacting with the explanations enabled users to construct classifiers that were more aligned with target preferences. Bostandjiev et al.s work on a music recommendation system found that explanation led to a remarkable increase in user satisfaction with their system. Cheung et al. studied StarCraft from a different perspective that of nonparticipant spectators. Their investigations produced a set of nine personas that helped to illuminate who these spectators are and why they watch. Since shoutcasters are one of the personas they discussed how shoutcasters affect the spectator experience and how they judiciously decide how and when to reveal different types of information both to entertain and inform the audience. Another contingent of researchers is working toward applying machine learning to automatically summarize different aspects of sports potentially assisting sportscasters or shoutcasters in their work. Two examples are automatically generating esports statistics and automatically extracting football play diagrams from raw video. Paper 562 Page 2 Paper Tournament Shoutcasters Players Game 2017 IEM Katowice Neeb vs Jjakji 2017 IEM Katowice Rotterdam and Maynarde Harstem vs TY 2017 GSL Season 1 Code S Artosis and tasteless 2016 WESG Finals Tenshi and Zeweig DeMuslim vs iGXY 2017 StarLeague S1 Premier Wolf and Brendan Innovation vs Dark 2016 KeSPA Cup Wolf and Brendan Maru vs Patience 2016 IEM Geonggi Kaelaris and Funka Byun vs Iasonu 2016 IEM Shanghai Rotterdam and Nathanias ShowTime vs Iasonu 3 2016 WCS Global Finals iNcontroL and Rotterdam Nerchio vs Elazer Snute vs ShowTime 10 2016 DreamHack Open Leipzig Rifkin and ZombieGrub Table 1. Summary of StarCraft 2 games studied. Please consult our supplementary materials for transcripts and links to videos. Finally future attempts to generate automated shoutcasting from replays using data such as the corpus reported in this paper could use dialog evaluation systems e.g. to assess the quality of the generated shoutcasters. The closest work to our own is Metoyer et al.s investigation into the vocabulary and language structure of explaining RTS games. In their study novices and experts acted in pairs the novice watched the expert play and asked questions while the expert thought aloud and answered them. They developed qualitative coding schemes of the content and structure of the explanations the expert players offered. In this paper we drew upon these coding schemes with slight modifications. The complete code sets are available in the online supplemental materials. Our work differs from all of these works in that our explainers are expert communicators about the game not participant players programmers or end users. Our work is also the first to apply IFT to XAI. METHODOLOGY In order to study high quality explanations and capable players we considered only games from professional tournaments denoted as Premier by TeamLiquid1 and also filtered out irrelevant utterances as described later in this section. We selected 10 matches from the Premier pool available with video on demand from professional StarCraft 2 tournaments from 2016 and 2017 Table 1. Professional matches have multiple games so we randomly selected one game from each match for analysis. 16 distinct shoutcasters2 appeared across the 10 videos with two casters3 commentating each time. The rest of this section explains our qualitative coding methodology. In general we measured rigor using an interrater reliability IRR rate of 80 over 20 of the data by 2 coders using the Jaccard index which is the size of the intersection of the codes divided by the size of the union. We exceeded 20 of the data when we needed more test iterations for agreement or when we had to subset the coding. If a codeset was too complex to do in one pass we coded subcategories. We simplified calculations with different data subset sizes to 20. First we coded for relevance. Shoutcasters should both inform and entertain so they fill dead air time with jokes. To filter out irrelevant utterances two researchers independently coded 1 httpwiki.teamliquid.netstarcraft2PremierTournaments 2 Shoutcasters confirmed each others quality in their consistency of utterance type and content Figure 5. 3 Here caster pair caster or pair for short differentiates our observed individuals from the population of shoutcasters as a whole. Paper 562 32 of statements in the corpus as relevant or irrelevant to explaining the game. We achieved a 95 interrater reliability. Then the researchers split up and coded the rest of the corpus. Research questions RQ1 and RQ2 investigated how the casters seek information onscreen so we used IFT constructs to discover the types of information casters sought and how they unearthed it. For RQ1 the patches in which they sought information we simply counted the casters navigations among patches. Changes in the display screen identified most of these4 for us automatically. For RQ2 how they went about their information foraging we coded the 110 instances of caster navigation by the context where it took place based on player actions Building Fighting Moving Scouting or simply caster navigation. Two researchers independently coded 21 of the data in this manner with IRR of 80. After achieving IRR one researcher coded the remainder of the data. For RQ3 implicit questions the shoutcasters answered we coded the casters utterances by the Lim Dey questions they answered. We added a judgment code to capture caster evaluation on the quality of actions. The complete code set will be detailed in the RQ3 Results section. Using this code set two researchers independently coded 34 of the 1024 explanations in the corpus with 80 interrater reliability Jaccard. After achieving IRR the researchers split up the remainder of the coding. For RQ4 explanation content we drew content coding rules from Metoyer et al.s analysis of explaining Wargus games and added some codes to account for differences in gameplay and study structure. For ease of presentation in this paper we use the terms numeric quantity and indefinite quantity instead of their terms identified discrete and indefinite quantity respectively. Two researchers independently coded the corpus one category at a time e.g. Objects Actions ... achieving an average of 78 IRR on more than 20 of the data in each category. One researcher then finished coding the rest of the corpus. Since all data sources are public we have provided all data and coding rules in supplementary materials to enable replicability and support further research. RESULTS RQ1 Results What information do shoutcasters seek to generate explanations and where do they find it We used two frameworks to investigate casters information seeking behaviors. We turned to the Performance Environ4 But if a caster points at something to bring the others attention to itbut does not have the mousethe viewer cant see it. Page 3 Paper Figure 1. A screenshot from an analyzed game modified to highlight the patches available to our casters HUD 1 bottom Information about current game state e.g. resources held income rate supply and upgrade status Minimap 2 lower left Zoomed out version of the main window Tab 3 top left Provides details on demand currently set on Production Workers killed 4 center left Shows that 9 Red workers have died recently Popup 5 center visualizations that compare player performance usually shown briefly. Regions 3 and 5 will be detailed in Figures 3 and 4. ment Actuators Sensors PEAS model to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory IFT to understand where casters did their information seeking beginning with the places their desired information could be found. These places are called information patches in IFT terminology. imap and Vision Toggles being among the most used patches in Table 2. The casters had superpowers with respect to Sensors and performance measures their interface allowed full observation of the environment whereas players could only partially observe it. The casters extensively used the Minimap and the Vision Toggle as they were the only ways for casters to peer through the players sensors. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets resources successes and failures e.g. Figure 1 region 4 showing that Blue has killed 9 of Reds workers and region 5 showing that Blue has killed 19 units to Reds 3 etc.. Table 2 shows that casters rarely consulted performance measures especially those that examined past game states. However they discussed basic performance measures available in the HUD Figure 1 region 1 which contained present state information e.g. resources held or upgrade status. Actuators were the means for the agents to interact with their environment such as building a unit. Figure 1 region 3 Production Tab shows some of the Actuators the player was using namely that Player Blue was building 5 types of objects whereas Red was building 8. Casters almost always kept visualizations of actions in progress on display. RTS actions had a duration meaning that when a player took an action time passed before its consequence had been realized. The Production tabs popularity was likely due to the fact that it is the only stable view of information about actuators and their associated actions. The Environment where the agent is situated is the game state map structures units etc. shown in Figure 1s main window. We label as Environment any of the patches in Table 2 such as the Units tab that show all the game state data regardless of whether the players or casters have observed it. In fact prior to the game in our corpus Pair 3 had this exchange which demonstrated their perception of the production tabs importance to doing their job Pair 3a What if we took someone who knows literally nothing about StarCraft just teach them a few phrases and what everything is on the production tab Pair 3bOh I would be out of a job. Sensors helped the agent collect information about the environment and corresponded to the local vision area provided by individual units themselves in our domain. Figure 1 region 2 Minimap shows a birds eye view of the portion of the environment observable by the Sensors. Casters used patches containing information about Sensors very often with Min  Paper 562 Implications for an interactive explainer Abstracting beyond the StarCraft components to the PEAS model revealed a pattern of the casters behaviors with implications for future explanation systems which we characterize Page 4 Pair 10 Pair 9 Pair 8 Pair 7 Pair 6 Pair 5 Pair 4 Pair 3 Pair 2 Pair 1 Usage Paper Patch Name State Agg. Units Lost popup Shows count and resource value of the units Past High 6 each player has lost. Units Lost tab Same as above but as a tab. Past High 5 Performance Income tab Provides resource gain rate. Present High 2 Income popup Shows each players resource gain rate and Present High 2 worker count. Army tab Shows supply and resource value of currently held Present High 1 nonworker units. Income Advantage graph Shows time series data comparing Past High 1 resource gain rate. Units Killed popup Essentially the opposite of the Units Lost Past High 1 popup Units tab Shows currently possessed units. Present Low Environment Upgrades tab Like Units tab but for upgrades to units and Present Low buildings. Structures tab Like Units tab but buildings. Present Low Actuator Production tab Shows the units structures and upgrades that Present Low Preferred by choice always on tab not counted are in progress i.e. have been started but are yet to finish. Minimap Shows zoomed out map view Present Med Too many to count Sensor Vision Toggle Shows only the vision available to one of the Present Low players. Table 2. This table illustrates description classification and usage rates of the patches and enrichment operations we observed casters using. Each patch is classified by 1. The part of the PEAS model that this patch illuminates best column 1 2. whether it examines past or present game states column 3 and 3. degree to which the patch aggregates data in its visualization column 4. The remaining columns show total usage counts as well as per caster pair usage. Note that there are additional patches passively available Main Window and HUD which do not have navigation properties. as keep your Sensors close but your Actuators closer. This aligns with other research showing that realtime visualization of agent actions can improve system transparency. However these results contrast with the explanation systems that tend to prioritize Performance measures. Our results instead suggest that an explanation system should prioritize useful readily accessible information about what an agent did or can do Actuators and of what it can see or has seen Sensors. Actuators Cue Goal assess scouting Environment Performance Sensors Cue Units separating fighting likely over Cue Units colocated impending combat likely Figure 2. The A E PS loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators and returns there throughout the foraging process. If a caster interrupted the loop they usually did so to return to the Actuators. RQ2 Results The How How do shoutcasters seek the information they seek Section RQ1 discussed the What and Where i.e. the content casters sought and locations where they sought it. We then considered how they decided to move among these places summarized in Table 2. The casters foraging moves seemed to follow a common foraging loop through the available information patch types an Actuators Environment Performance loop with occasional forays over to Sensors Figure 2. Specifically the casters tended to start at the always on Actuator related patches of current states actions in progress then when something triggered a change in their focus they checked the Environment for current game state information and occasionally Performance measures of past states. If they needed more information along the way they went to the Sensors to see through a players eyes. We will refer to this process as the A E PS loop. Information Foraging Theory IFT explains why people information predators leave one patch to move to another such when the casters left Actuator patches. According to IFT predators choose navigations as costbenefit decisions based on the value of information in the patch a predator is already in Paper 562 versus the value per cost of going to another patch. Staying in the same patch is generally the least expensive but when there is less value to be gained by staying versus moving to another patch the predator moves to the other patch. However the predator is not omniscient decisions are based upon the predators perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types and from the cues signposts in their information environment that point toward content available in other patches. Certain types of cues tended to trigger a move for the casters. Impending combat was the most common cue triggering a move from the Actuators type Production tab to the Environment type Units tab i.e. from A to E in the A E PS loop. In Figure 2 the co location of opposing units served as the cue. This cue indicated imminent combat which led to caster navigation to a new patch to illuminate the environment. In fact combat cues triggered navigations to the Units tab most frequently accounting for 30 of the 51 navigations there Table 2. Page 5 Paper Figure 3. The Units Lost tab left image shows the number of units lost and their total value in terms of resources spent for both players. In this example from Pair 2 we see that Blue Player top has lost merely 2800 minerals worth of units so far in this game while Red has lost more than 7000. The Units Killed popup right image allows shoutcasters to quickly compare player performance via a tug of war visualization. In this example from Pair 1 as we see that Blue Player left has killed 19 units while Red has killed merely 3. The main difference between these two styles of visualization is that the tab offers more options and information depth to drill down into. Figure 4. The Production tab showing the build actions currently in progress for each player. Each unitstructure type is represented by a glyph which serves as a link to navigate to that object provided a progress bar for duration and given the number of objects of that type. Thus we can see that Blue Player top row is building 5 different types of things while Red bottom row is building 4 types of things. The Structures Upgrades and Units tab look fairly similar to the Production tab. Interestingly this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations text or occasionally images in previous IFT investigations that label a navigation device like a hyperlink or button that leads to another information patch. In contrast cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However cues like this were considered cues because they provide users with concise information about content that is not immediately available. They suggested high value in another location in the case of combat the Units tab. Combat ending was a dynamic cue that triggered a move to a Performance measure. 10 of the 13 navigations to a past facing Performance measure via tab or popup occurred shortly after combat ended as a form of after action review Table 2. Occasionally the shoutcasters visited other Performance patches such as the Income Units Lost and Army tabs to demonstrate reasons why a player had accrued an ingame lead or the magnitude of that lead 7 navigations. However signs of completed fighting were the main cues for visiting a Performance patch. The most common detour out of the AE P part of the loop to a Sensor patch was to enrich the information environment via the Vision Toggle 36 navigations Table 2. The data did not reveal exactly what cues led to this move but the move itself had a common theme to assess scouting operations. The casters used the Vision Toggle to allow themselves to see the game through the eyes of only one of the players but their default behavior was to view the game with ALL Paper 562 Sensor was the second most common patch move. Besides the act of following cues IFT has another foraging operation enriching their information environment to make it more valuable or cost efficient. The aforementioned Vision Toggle was one example of this and another was when casters added on information visualizations derived from the raw data like Performance measure popups or other basic visualizations. Two examples of the data obtained through this enrichment are shown in Figure 3. These Performance measures gave the shoutcasters ata glance information about the ways one player was winning. The most commonly used tab for example the Units Lost tab Figure 3 showed the number of units lost and their total value in terms of resources spent. This measure achieves at a glance by aggregating all the data samples together by taking a sum derived values like this allow the visualization to scale to large data sets. However Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type as Figure 4 shows so type information was maintained with only positional data lost. This contrasts with the Minimap medium aggregation in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data HUD but these patches were also the only way to access past state information Table 2. Implications for an interactive explainer These results have several implications for automated explanation systems in this domain. First the A E PS loop and how the casters traversed it reveals priority and timing implications for automated explanation systems. For example the cues that led them to switch to different information patches could also be cues in an automated system about the need to avail different information at appropriate times. For example our casters showed a strong preference for actuator information as steady state visualization but preferred performance information upon conclusion of a subtask. Viewing the casters behaviors through the dual lens of PEAS IFT has implications for not only the kinds of patches that an explanation system would need to provide but also the cost to users of not providing these patches in a readily accessible format. For example PEAS IFT revealed a costly foraging problem for the casters due to the relative inaccessibility of some Actuator patches. There is no easily accessible mechanism in StarCraft by which they could navigate to an Actuator patch with fighting or scouting actions in progress. Instead the only way the casters could get access to these actions was via painstaking camera placement. The casters made countless navigations to move the Page 6 Paper 19 demanded a Why explanation. To contrast with our study only 27 of the casters 1024 utterances approximately 3 were Why answers. Discussion and implications for an interactive explainer Why so few Whys Should an automated explainer like our shoutcasters eschew Why explanations in favor of What Figure 5. Frequency of Lim Dey questions answered by casters with one line per caster pair. Y Axis represents percentages of the utterances which answered that category of question X Axis. Note how casters structured answers consistently. camera using the Minimap traditional scrolling or via tabs with links to the right unit or building. However despite all these navigation affordances sometimes the casters were unable to place the camera on all the actions they needed to see. For example at one point when Pair 4 had the camera on a fight at Xys base a second fight broke out at DeMuslims base which they completely missed Pair 4a surprised noticing something amiss Xy actually killed the 3rd base of DeMuslim. ...the pair tries to figure what must have happened... Pair 4b Oh my god youre right Alex. Pair 4a Yeah it was killed during all that action. RQ3 Results What implicit questions do shoutcasters answer and how do they form their answers We con
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, interviews, Research paper:
ABSTRACT Data driven decisionmaking consequential to individuals raises important questions of accountability and justice. Indeed European law provides individuals limited rights to meaningful information about the logic behind significant autonomous decisions such as loan approvals insurance quotes and CV filtering. We undertake three experimental studies examining peoples perceptions of justice in algorithmic decisionmaking under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decisionmaking appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness generalisation and indignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different stylesunder repeated exposure of one style scenario effects obscure any explanation effects. Our results suggests there may be no best approach to explaining algorithmic decisions and that reflection on their automated nature both implicates and mitigates justice dimensions. H.5.m. Information Interfaces and Presentation e.g. HCI Miscellaneous K.4.1 Computers and Society Public Policy Issues Important decisionsabout the allocation of jobs loans or insuranceare increasingly based on the predictions and classifications of computer models. By training such models on large datasets of previous customers or employees organisations seek to more accurately estimate the risks and rewards of different actions and act accordinglysometimes even automatically. Such practices are commonly referred to as algorithmic decisionmaking and have caused both celebration and consternation. must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. 2018 Copyright held by the ownerauthors. Publication rights licensed toHowever such decisions have significant consequences for those individuals directly affected by them. While human decision makers may exhibit errors of judgement and biases we can at least demand that they rationalise their decisions and thereby hold them accountable. A significant body of research in legal and organisational psychology suggests that people do not only care about whether the outcome of a decision benefits them but also whether it meets standards of justice. In particular information about decisionmaking processes plays an important role in justifying the decisions that are made. In their basic form algorithmic decisionmaking systems do not naturally provide information relevant to justice judgements and therefore threaten the capacity for accountability inherent in human decisionmaking. In relying on such systems we therefore run the risk of important decisions being made unaccountably. The challenge to justice has long been recognised in contexts where datadriven decisionmaking is used in government administration. The increasing use of predictive systems by both private and public bodies has prompted regulation aimed at rendering them more transparent. The European Unions General Data Protection Regulation GDPR and similarly the preceding 1995 Directive requires organisations deploying certain systems to provide affected individuals with meaningful information about the logic behind their outputs. Similarly the United States Equal Credit Opportunity Act ECOA requires lenders to provide statements of reasons when applications are denied loans. These legal requirements give rise to HCI challenges concerning how the outputs of such systems should be communicated to affected individuals and other stakeholders. These challenges are not entirely new. The ability to provide information about how a system derives its predictions or classifications has long been recognised in the AI expert systems and HCI communities. However recent work has addressed the novel challenges of complex machine learning models whose logic and outputs are otherwise harder to explain. So called model agnostic approaches aim to explain the outputs of any classifier regardless of the machine learning algorithm used to train it. The potential for as well as the limitations of such approaches to address these regulatory requirements have been noted. By implementing these explanation facilities within their algorithmic decisionmaking systems organisations may be able to meet their obligation to provide meaningful information about how their decisions are made. However evaluation of the human factors of such systems has primarily tested their utility in supporting expert decisionmaking for instance in clinical DOI httpsdoi.org10.11453173574.3173951 Paper 377 Page 1 Paper diagnostic settings. The kind of meaningful information required by experts is likely to differ from that required by individuals affected by automated credit scores insurance pricing or hiring decisions for the purposes of accountability as envisaged by the relevant regulatory regimes. This study aims to provide a preliminary exploration of the ways in which a range of novel explanation approaches might serve regulatory goals of rendering algorithmic decisionmaking more fair accountable and transparent. In particular what effects do explanations have on peoples perceptions of algorithmic decisions with respect to these regulatory aims Do different explanation approaches meet these aims in different ways To explore these questions we conducted a series of studies both labbased and online examining peoples responses to a series of scenarios involving algorithmic decisions accompanied by different explanation styles inspired by recent work on fairness accountability and transparency in machine learning. BACKGROUND The questions raised above are connected to multiple distinct areas of research including the psychology of justice perceptions regarding decision making the design of interpretable models in machine learning and HCI and interdisciplinary work at the intersection of machine learning law and social sciences. Interpreting intelligent systems The ability to interpret a systems outputs has long been a core theme of research into intelligent systems. As a general principle systems should be able account for their own operation in ways that help users understand how their tasks are being accomplished. Early knowledge representation and reasoning systems typically aimed to meet this requirement by producing explanations for their outputs for instance by presenting derivations from rules in rule based architecture or graphical representations of nodes and edges in the case of Bayesian networks. Explanation facilities could be further augmented by explicitly modeling the knowledge engineering and design process and revealing information about them to end users. Inspired by different theoretical models of argumentation various explanation types have been explored and tested with the style of explanation provided affecting the extent to which endusers understand the decisions of expert systems. Various research suggests that usersagain primarily experts may in certain cases systematically over rely on the outputs of intelligent systems while in others they may systematically distrust systems despite reassurances as to their accuracy. However the provision of explanations can affect levels of trust and acceptance of algorithmic decisions. In some cases explanation increases trust and reliance but in others an explanation may have the opposite effect if the level of detail it contains is deemed insufficient. The increasing popularity of machine learning models as decision making tools presents novel challenges for explanation as it may not be as straightforward to extract the embedded knowledge behind their outputs. This problem is particularly pronounced for complex models like many layered neural networks but even simple decision tree models can quickly become uninterpretable if they have too many branches. As users form mental models Paper 377 Recent work on interpretable machine learning has focused on providing primarily pedagogical local explanations for the outputs of ML models pedagogical in the sense that the explanations teach something about how the model works rather than attempting to represent it directly and local in that they focus on how a specific output was derived. Approaches such as Local Interpretable Model Agnostic Explanations LIME and Quantitative Input Influence QII enable an enduser to see a list of features which contributed to the output along with the strength and direction of the contribution to the outcome class.1 Case based explanations provide examples from the models training data which most closely resemble the output in question while demographic explanations provide statistics on the outcome classes for relevant demographic variables. While these different explanation styles suggest promising ways to explain the outputs of any classifier explanation qualityor interpretabilitydoes not have a formal definition or a standard evaluation methodology shared amongst machine learning researchers. Explanation serves different functions in different contexts and its evaluation is therefore context  and purpose dependent. To this end various human subjects experiments have been undertaken exploring whether particular explanations result in better performance on a given end task in comparison to a no explanation baseline. Contexts studied include aiding medical experts in diagnosis increasing test performance of users of personalised learning environments or enabling network security analysts to correctly identify incidents e.g.. The use of various explanation styles has been been widely explored in the field of recommender systems. Tintarev and Mastho identify seven purposes for recommender system explanations namely transparency scrutability trust effectiveness persuasiveness efficiency and satisfaction. Explanations can also be used to justify or describe. While early recommender system explanation approaches provided a uniform explanation style for single source collaborative filtering more recent work explores how to derive explanations for recommender systems based on hybrid multiple sources and matrix factorisation. However a reported need for explanation may not always correspond to differences in behaviour or performance in a study of news recommender systems endusers expressed a desire for explanations but the number of news items they opened did not change when provided with reasons for their recommendations. The use of local pedagogical explanation facilities has not yet been tested in the contexts affected by the aforementioned emerging regulatory requirements such as the automated evaluation of people for loans hiring and insurance. While various tools designed to help decision makers identify and 1 Layer wise Relevance PropagationDeep Taylor Expansion decomposes the innards of deep learning systems to provide similar results to LIME without directionality and is also relevant here. Page 2 Paper correct discrimination in data mining exist these are aimed at data scientists implementing systems rather providing information to the individual decision subjects affected by them. The potential for local pedagogical explanation systems to provide justice related information fulfilling the policy goals of transparency accountability and fairness has recently been noted by computer scientists and law scholars. It has been suggested that organisations might rely upon these explanation facilities to fulfill legal duties to provide meaningful information about the logic of specific system outputs to affected individuals. Perceptions of justice regarding decisionmaking In order to explore how such explanation facilities might serve the aims of justice in algorithmic decisionmaking a clearer understanding of the psychological aspects involved in justice perceptions is required. To this end it is worth considering the extant literature on perceptions of justice regarding human decisionmaking. Decisions made about people with significant effects such as court decisions hiring firing and promotion in a workplace or the allocation of financial products are often contentious and require a higher burden of accountability than other decisions they should meet the standards of bureaucratic justice. Early work in this area emphasised that perceived levels of justice of a decision outcome are separate from purely self serving rationalisations of a decision outcome an individual might be negatively affected by a decision whilst still thinking it is just. According to Colquitt and others justice perceptions can be broken down into several aspects or antecedents. Procedural justice concerns the processes logic and deliberation behind a decision. Distributive justice concerns the allocation of positive and negative outcomes in a decision context and whether they are distributed equitably or deservedly amongst the affected population given their circumstances performance or contributions. Interactional justice concerns the extent to which the affected individual is treated with dignity and respect by the decision makers. Finally informational justice pertains to the information and explanations provided for decisions are they candid thorough and tailored to individual needs These aspects of justice are distinct but correlated. Receiving a thorough explanation informational justice is important in helping people to assess whether the decisionmaking procedure is just procedural justice. In turn decisions perceived to be procedurally just are more likely to be perceived as distributively just. Such findings provide a rationale for giving individuals rights to information about significant decisions requiring decision makers to explain their decisions should promote informational justice which should help people assess procedural justice and distributive justice in turn. This rationale might plausibly apply to the regulatory regime around algorithmic decisionmaking. Requiring organisations to explain the logic behind their algorithmic decisionmaking systems informational justice enables affected individuals to assess whether the logic of the system is just procedural justice which in turn might moderate their assessments of fairness of the decision outcomes distributive justice. In so far as these notions of justice capture the aims of the regulatory requirements they may provide appropriate ways of measuring the adequacy of different explanation systems for these purposes. Paper 377 Key questions and contributions This work builds upon and contributes to these disparate areas of research. Work on perceptions of justice reveals much about the role of explanations in ethical assessments of human decisions but the applicability of these findings to algorithmic decisions and the range of proposed explanation systems proposed for them remains largely unexplored. A notable exception is Kizilcec who studied the effect of levels of transparency on perceptions of procedural justice. However the nuances of the different explanation approaches recently developed in interpretable ML have not yet been explored in relation to perceptions of justice. We also do not yet know much the extent to which findings on correlations between perceptions of justice from human decisionmaking contexts hold in the context of algorithmic decisions. Furthermore as research into practically achievable explanations for machine learning systems accelerates researchers are turning to consider which explanation styles are more desirable rather than simply possible to implement. This itself is a loaded question an explanation style that artificially inflates certain notions of justice by playing on cognitive biases might suffer from the same anti paternalist critiques levied against proponents of nudge philosophy. Understanding how explanation styles might influence and interact with justice perceptions is therefore an important preliminary to further development of ML explanation tools. With these issues in mind our key questions are 1. How do explanations for algorithmic decisions affect justice perceptions regarding algorithmic decisions In particular do the positive correlations observed between informational procedural and distributive justice in human decisionmaking settings also hold in algorithmic decisionmaking settings 2. How do different styles of explanation affect such justice perceptions STUDY DESIGN AND METHODOLOGY In investigating these questions we combine several methodological aspects of the related work from justice perceptions and intelligent systems research. Many of the previously mentioned machine learning model explanation systems have not been subjected to user evaluations and those which have often only test peoples ability to predict a models outputs or answer questions about its logic e.g.. For present purposes we are not concerned about how faithfully an explanation facility imparts actual model logic to a user.2 Rather we are interested in how the provision of various kinds of information about a models outputs might affect justice perceptions as measured in the psychological literature. Much of this research is based on field surveys of individuals with personal experience of being affected by decisions e.g. although controlled experiments involving simulated decisionmaking and scenario based methods have also been used. Given our focus on the effects of information provision and explanation styles on perceptions of fairness we conducted a set of experimental studies to elicit peoples responses to a range of 2 Although we acknowledge this is an important challenge if explanations are to be more than just comforting stories as cautioned by Lipton. Page 3 Paper plausible algorithmic decision scenarios and associated explanations. While this may lack the ecological validity of field surveys it gives us the ability to engage people with a wide range of novel explanation styles covered in 2.1 and have yet to be implemented alongside real machine learning systems currently deployed in the wild to any significant degree. By testing out proposed explanation styles with fictional scenarios we hope that designers can gain insight into user concerns and perhaps create more meaningful explanation interfaces to meet the upcoming legal requirements. Scenarios and application contexts We selected 5 application contexts in which to situate the fictional scenarios. The main criteria for context selection were a relatively common interaction for the target population a large proportion has experienced or will experience it at some point in their life likely to involve machine learning models for algorithmic decisionmaking at present or in the near future have significant economic or practical effects on the decision subject and would possibly fall under the aforementioned GDPR regulation on automated decisions.3 After consulting relevant literature on how ML systems are being applied in each context e.g. we settled on the following contexts 1. Applying for a personal financial loan 2. Applying for a promotion at work 3. Car insurance premiums dynamically priced based on personal details and driving behaviour measured by a telematic sensor as described in e.g. 4. Passengers on overbooked airline flights being selected for re routing 5. Freezing of bank account due to activity suspected as associated with money laundering For each context we created fictional cases in which an individual has had a decision made about them automatically. These were inspired by now classic algorithmic war stories see e.g.. Scenarios and cases were adapted where necessary to fit the context from which the inperson participants would be recruited a city in the United Kingdom. Because explanations are most likely to be requested in response to a decision which has a negative outcome for the individual and considerations of procedural justice have been found to be more strongly felt in such cases each scenario ended in a negative outcome for the individual. Explanation styles In order to derive a set of explanation styles to test we reviewed a range of research including both technical work on interpretable machine learning models and legal discussions on the requirements of existing and forthcoming regulations on transparency of algorithmic decisions e.g.. Our aim was to find explanation styles which could plausibly meet or exceed the regulatory requirements regarding transparency of automated decisions in particular the requirement that organisations provide meaningful information about the logic involved in an automated decision GDPR art 15h. We aimed to condense the many 3 The hurdle that a decision must be based solely on automated processing is here read less restrictively than legal analysis has indicated. Paper 377 categories of promising explanation style 1. Input Influence Presents a list of input variables alongside a quantitative measure of their influence positive or negative on a decision e.g. 2. Sensitivity For each input variable used in a decision sensitivity analysis shows how much the value of that variable would have to differ in order to change the output class not to be confused with the notion of sensitivity used in ML evaluation e.g.. 3. Case based Presents a case from the models training data which is most similar to the decision being explained e.g. 4. Demographic Presents aggregate statistics on the outcome classes for people in the same demographic categories as the decision subject such as age gender income level or occupation e.g. These explanation styles were tested and refined in a small scale informal design phase. Each explanation system imparts different information which is presented in various ways in the literature including text graphs and bullet points. In order to control for these differences in representation we chose to use purely textual explanations. We informally tested each explanation style by varying the wording and information dimensions. We found that people generally did not pay attention to explanations independent of scenario descriptions. Through iterative testing we found that the use of dialog boxes focused attention on explanations see Figure 2. These were deployed and investigated in depth in an inperson lab study and two online experiments. All phases of each study were approved by relevant University ethical review procedures. Justice constructs The justice constructs used in all three studies were based on those developed in the psychology of justice research summarised by Colquitt et al. Participants were asked to rate their agreement with five statements on a 5point Likert scale see statements at the bottom of Figure 1. Statements were drawn and adapted from previous studies of human decisionmaking to fit scenarios in which the decision was being made by an automated algorithmic system. Preliminary testing found that certain questions from Colquitts scale particularly those relating to interactional justice such as Have you been able to express your views and feelings during those procedures were difficult to interpret. This was partly due to the hypothetical nature of the scenarios and partly due to being too human specific to apply to a computerbased interaction. The remaining questions pertained to informational procedural and distributive justice. Statement 2 understanding aimed to capture informational justice statements 3 and 4 appropriateness of factors fair process pertained to procedural justice and statement 5 deserved outcome addressed distributive Page 4 Paper Car insurance scenario A car insurance company provides customers with personalised prices based on their attributes and driving behaviours measured through a telematic sensor installed in the car. Their system for setting prices is based on a computer model which predicts how likely an applicant is to have an accident and make a claim. The computer makes its predictions based on data the system has collected about thousands of other drivers. Based on its ongoing analysis of the drivers chances of having an accident the system will automatically set their monthly premium. The cheapest premium tier given to those drivers who are judged by the system as the safest is 20 per month. Sarah Sarah is a customer of the insurance company. She is 35 years old Has been driving for 17 years Been in an accident once which was not her fault Drives 800 miles a month on average Exceeds the speed limit on average once every two months 20 of Sarahs driving takes place at night Based on this information the computer system has not qualified Sarah for the cheapest tier of insurance premium. The insurer provides Sarah with the following information about the computers decision. This decision was based on thousands of similar cases from the past. For example a similar case to yours is a previous customer Claire. She was 38 years old with 18 years of driving experience drove 850 miles per month occasionally exceeded the speed limit and 25 of her trips took place at night. Claire was involved in one accident in the following year. Please rate your agreement with the following statements 1. Agreement I agree with the decision 2. Understanding I understand the process by which the decision was made 3. Appropriateness of factors The factors considered in the decision were appropriate 4. Fair process The decisionmaking process was fair 5. Deserved outcome The individual deserved this outcome given their circumstances or behaviour Figure 1. Example of a scenario car insurance case description Sarah explanation Case based and questions dimensions. We also found that people tended to interpret these justice construct statements as a proxy for the simpler question of whether or not they agreed with the decision to avoid this an additional statement about agreement 1 agreement was also included prior to the others. Phase 1 Lab study The first study consisted of inperson semistructured interviews focused around a series of fictionalised cases as described above. The purpose of this study was to gain indepth insights into how people interpret evaluate and reason about algorithmic decisionmaking in the range of contexts and explanation styles mentioned above. Participants were asked to consider their agreement with the five measures described in section 3.3 and Figure 1. Agreement was indicated on a 5point Likert scale ranging from strongly agree to strongly disagree. Participants were asked to verbalise their responses to each case and explain their thought processes for each response. Each case was presented with one of the 4 explanation styles described above or a control condition in which no explanation was provided. Participants were assigned pseudonymous identifiers to be associated with data generated during the experiment. Audio recordings taken from the interviews and thinkaloud processes were transcribed segmented and labelled by each case and explanation style and any personally identifying information was redacted. Thematic analysis was undertaken to identify common themes in participants interpretation evaluation and reasoning in response to cases. Thematic codes were independently developed by three coders after which a single set was jointly agreed upon. The data were then recoded using the new set of codes. Phase 2 Online studies Following on from the lab study two online studies were conducted in order to generate quantitative data to test the following hypotheses 1. Do different explanation styles result in differences in perceived levels of justice 2. Do the positive correlations observed between informational procedural and distributive justice in human decisionmaking settings also hold in algorithmic decisionmaking settings Both online studies consisted of pared down versions of the protocols used in phase 1. Participants were recruited via the study platform Prolific Academic4 and filtered to include only individuals over 18 based in the UK in order to maintain similarity with the participants recruited in phase 1. A betweensubjects design was used in order to test differences between responses under different explanation conditions. On the basis of described responses to the various explanation styles in the lab study it was hypothesised that the ability to make direct comparisons between explanation styles might affect justice perceptions. We therefore also devised a followup withinsubjects experiment to test whether exposure to multiple explanation styles for a single case would have different effects than repeated exposure to a single explanation style. 19 participants were recruited through paper flyers placed in various permitted locations in a small city in the UK including cafes and restaurants churches libraries shops and museums and through official social media accounts of the research lab and personal accounts of the researchers. Participants were then invited to the lab where the study occurred which was set up with a standard PC keyboard and mouse and a 27 inch display with a screen resolution of 1920x1200. Between subjects study Participants were presented with 3 cases for each of the 5 contexts 15 cases in total via a webbased interface. Cases were presented with a brief description of the individual the decision outcome the explanation except in the control condition and an illustrative image depicting the context. The cases were used to prompt reflection on various aspects of the decision and any explanatory information provided using a concurrent thinkaloud process. The withinsubjects study involved 65 participants. Only the loan and insurance cases were included in this design as the other Paper 377 325 participants were randomly assigned to 1 of 5 conditions n 65 in each group. Each group was presented with a selection of 12 cases used in experiment 1 accompanied by one of the 4 different explanation styles or a control condition featuring no explanation.5 Participants were asked to indicate their agreement with the 5 measures described in phase 1 using the same Likert scale. Within subjects study 4 www.prolific.ac 5 The airline context was excluded at this stage because experiment 1 had revealed that participants almost universally judged the cases to be very unfair and undeserved thus preventing any useful conclusions to be drawn from this context. Page 5 Paper Input influencebased explanation Demographicbased explanation Casebased explanation Sensitivitybased explanation Our predictive model assessed your personal information and driving behaviour in order to predict your chances of having an accident. The more s ors the more positively or negatively that factor impacted your predicted chance of accidents. Unimportant factors are indicated. Your age Driving experience Level of adherence to speed limit Number of trips taken at night Miles per month 29 of female drivers qualified for the cheapest tier. 31 of drivers in your age group qualified for the cheapest tier. 35 of drivers with 17 years of experience qualified for the cheapest tier. 15 of drivers who have been on one accident which was not their fault qualified for the cheapest tier. 26 of drivers who regularly travel at night qualified for the cheapest tier. 21 of drivers who exceed the speed limit once ever two months qualified for the cheapest tier This decision was based on thousands of similar cases from the past. For example a similar case to yours is a previous customer Claire. She was 38 years old with 18 years of driving experience drove 850 miles per month occasionally exceeded the speed limit and 25 of her trips took place at night. Claire was involved in one accident in the following year. If 10 or less of your driving took place at night you would have qualified for the cheapest tier. If your average miles per month were 700 or less you would have qualified for the cheapest tier. ok ok ok ok Figure 2. Illustrative examples of the different explanation types presented to participants input influence sensitivity demographic and case based. scenarios could not be adapted to fit into a realistic hypothetical scenario in which four different explanations would be presented for 
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, interviews, Research paper:
ABSTRACT Data driven decisionmaking consequential to individuals raises important questions of accountability and justice. Indeed European law provides individuals limited rights to meaningful information about the logic behind significant autonomous decisions such as loan approvals insurance quotes and CV filtering. We undertake three experimental studies examining peoples perceptions of justice in algorithmic decisionmaking under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decisionmaking appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness generalisation and indignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different stylesunder repeated exposure of one style scenario effects obscure any explanation effects. Our results suggests there may be no best approach to explaining algorithmic decisions and that reflection on their automated nature both implicates and mitigates justice dimensions. H.5.m. Information Interfaces and Presentation e.g. HCI Miscellaneous K.4.1 Computers and Society Public Policy Issues Important decisionsabout the allocation of jobs loans or insuranceare increasingly based on the predictions and classifications of computer models. By training such models on large datasets of previous customers or employees organisations seek to more accurately estimate the risks and rewards of different actions and act accordinglysometimes even automatically. Such practices are commonly referred to as algorithmic decisionmaking and have caused both celebration and consternation. must be honored. Abstracting with credit is permitted. To copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee. Request permissions from permissionsacm.org. 2018 Copyright held by the ownerauthors. Publication rights licensed toHowever such decisions have significant consequences for those individuals directly affected by them. While human decision makers may exhibit errors of judgement and biases we can at least demand that they rationalise their decisions and thereby hold them accountable. A significant body of research in legal and organisational psychology suggests that people do not only care about whether the outcome of a decision benefits them but also whether it meets standards of justice. In particular information about decisionmaking processes plays an important role in justifying the decisions that are made. In their basic form algorithmic decisionmaking systems do not naturally provide information relevant to justice judgements and therefore threaten the capacity for accountability inherent in human decisionmaking. In relying on such systems we therefore run the risk of important decisions being made unaccountably. The challenge to justice has long been recognised in contexts where datadriven decisionmaking is used in government administration. The increasing use of predictive systems by both private and public bodies has prompted regulation aimed at rendering them more transparent. The European Unions General Data Protection Regulation GDPR and similarly the preceding 1995 Directive requires organisations deploying certain systems to provide affected individuals with meaningful information about the logic behind their outputs. Similarly the United States Equal Credit Opportunity Act ECOA requires lenders to provide statements of reasons when applications are denied loans. These legal requirements give rise to HCI challenges concerning how the outputs of such systems should be communicated to affected individuals and other stakeholders. These challenges are not entirely new. The ability to provide information about how a system derives its predictions or classifications has long been recognised in the AI expert systems and HCI communities. However recent work has addressed the novel challenges of complex machine learning models whose logic and outputs are otherwise harder to explain. So called model agnostic approaches aim to explain the outputs of any classifier regardless of the machine learning algorithm used to train it. The potential for as well as the limitations of such approaches to address these regulatory requirements have been noted. By implementing these explanation facilities within their algorithmic decisionmaking systems organisations may be able to meet their obligation to provide meaningful information about how their decisions are made. However evaluation of the human factors of such systems has primarily tested their utility in supporting expert decisionmaking for instance in clinical DOI httpsdoi.org10.11453173574.3173951 Paper 377 Page 1 Paper diagnostic settings. The kind of meaningful information required by experts is likely to differ from that required by individuals affected by automated credit scores insurance pricing or hiring decisions for the purposes of accountability as envisaged by the relevant regulatory regimes. This study aims to provide a preliminary exploration of the ways in which a range of novel explanation approaches might serve regulatory goals of rendering algorithmic decisionmaking more fair accountable and transparent. In particular what effects do explanations have on peoples perceptions of algorithmic decisions with respect to these regulatory aims Do different explanation approaches meet these aims in different ways To explore these questions we conducted a series of studies both labbased and online examining peoples responses to a series of scenarios involving algorithmic decisions accompanied by different explanation styles inspired by recent work on fairness accountability and transparency in machine learning. BACKGROUND The questions raised above are connected to multiple distinct areas of research including the psychology of justice perceptions regarding decision making the design of interpretable models in machine learning and HCI and interdisciplinary work at the intersection of machine learning law and social sciences. Interpreting intelligent systems The ability to interpret a systems outputs has long been a core theme of research into intelligent systems. As a general principle systems should be able account for their own operation in ways that help users understand how their tasks are being accomplished. Early knowledge representation and reasoning systems typically aimed to meet this requirement by producing explanations for their outputs for instance by presenting derivations from rules in rule based architecture or graphical representations of nodes and edges in the case of Bayesian networks. Explanation facilities could be further augmented by explicitly modeling the knowledge engineering and design process and revealing information about them to end users. Inspired by different theoretical models of argumentation various explanation types have been explored and tested with the style of explanation provided affecting the extent to which endusers understand the decisions of expert systems. Various research suggests that usersagain primarily experts may in certain cases systematically over rely on the outputs of intelligent systems while in others they may systematically distrust systems despite reassurances as to their accuracy. However the provision of explanations can affect levels of trust and acceptance of algorithmic decisions. In some cases explanation increases trust and reliance but in others an explanation may have the opposite effect if the level of detail it contains is deemed insufficient. The increasing popularity of machine learning models as decision making tools presents novel challenges for explanation as it may not be as straightforward to extract the embedded knowledge behind their outputs. This problem is particularly pronounced for complex models like many layered neural networks but even simple decision tree models can quickly become uninterpretable if they have too many branches. As users form mental models Paper 377 Recent work on interpretable machine learning has focused on providing primarily pedagogical local explanations for the outputs of ML models pedagogical in the sense that the explanations teach something about how the model works rather than attempting to represent it directly and local in that they focus on how a specific output was derived. Approaches such as Local Interpretable Model Agnostic Explanations LIME and Quantitative Input Influence QII enable an enduser to see a list of features which contributed to the output along with the strength and direction of the contribution to the outcome class.1 Case based explanations provide examples from the models training data which most closely resemble the output in question while demographic explanations provide statistics on the outcome classes for relevant demographic variables. While these different explanation styles suggest promising ways to explain the outputs of any classifier explanation qualityor interpretabilitydoes not have a formal definition or a standard evaluation methodology shared amongst machine learning researchers. Explanation serves different functions in different contexts and its evaluation is therefore context  and purpose dependent. To this end various human subjects experiments have been undertaken exploring whether particular explanations result in better performance on a given end task in comparison to a no explanation baseline. Contexts studied include aiding medical experts in diagnosis increasing test performance of users of personalised learning environments or enabling network security analysts to correctly identify incidents e.g.. The use of various explanation styles has been been widely explored in the field of recommender systems. Tintarev and Mastho identify seven purposes for recommender system explanations namely transparency scrutability trust effectiveness persuasiveness efficiency and satisfaction. Explanations can also be used to justify or describe. While early recommender system explanation approaches provided a uniform explanation style for single source collaborative filtering more recent work explores how to derive explanations for recommender systems based on hybrid multiple sources and matrix factorisation. However a reported need for explanation may not always correspond to differences in behaviour or performance in a study of news recommender systems endusers expressed a desire for explanations but the number of news items they opened did not change when provided with reasons for their recommendations. The use of local pedagogical explanation facilities has not yet been tested in the contexts affected by the aforementioned emerging regulatory requirements such as the automated evaluation of people for loans hiring and insurance. While various tools designed to help decision makers identify and 1 Layer wise Relevance PropagationDeep Taylor Expansion decomposes the innards of deep learning systems to provide similar results to LIME without directionality and is also relevant here. Page 2 Paper correct discrimination in data mining exist these are aimed at data scientists implementing systems rather providing information to the individual decision subjects affected by them. The potential for local pedagogical explanation systems to provide justice related information fulfilling the policy goals of transparency accountability and fairness has recently been noted by computer scientists and law scholars. It has been suggested that organisations might rely upon these explanation facilities to fulfill legal duties to provide meaningful information about the logic of specific system outputs to affected individuals. Perceptions of justice regarding decisionmaking In order to explore how such explanation facilities might serve the aims of justice in algorithmic decisionmaking a clearer understanding of the psychological aspects involved in justice perceptions is required. To this end it is worth considering the extant literature on perceptions of justice regarding human decisionmaking. Decisions made about people with significant effects such as court decisions hiring firing and promotion in a workplace or the allocation of financial products are often contentious and require a higher burden of accountability than other decisions they should meet the standards of bureaucratic justice. Early work in this area emphasised that perceived levels of justice of a decision outcome are separate from purely self serving rationalisations of a decision outcome an individual might be negatively affected by a decision whilst still thinking it is just. According to Colquitt and others justice perceptions can be broken down into several aspects or antecedents. Procedural justice concerns the processes logic and deliberation behind a decision. Distributive justice concerns the allocation of positive and negative outcomes in a decision context and whether they are distributed equitably or deservedly amongst the affected population given their circumstances performance or contributions. Interactional justice concerns the extent to which the affected individual is treated with dignity and respect by the decision makers. Finally informational justice pertains to the information and explanations provided for decisions are they candid thorough and tailored to individual needs These aspects of justice are distinct but correlated. Receiving a thorough explanation informational justice is important in helping people to assess whether the decisionmaking procedure is just procedural justice. In turn decisions perceived to be procedurally just are more likely to be perceived as distributively just. Such findings provide a rationale for giving individuals rights to information about significant decisions requiring decision makers to explain their decisions should promote informational justice which should help people assess procedural justice and distributive justice in turn. This rationale might plausibly apply to the regulatory regime around algorithmic decisionmaking. Requiring organisations to explain the logic behind their algorithmic decisionmaking systems informational justice enables affected individuals to assess whether the logic of the system is just procedural justice which in turn might moderate their assessments of fairness of the decision outcomes distributive justice. In so far as these notions of justice capture the aims of the regulatory requirements they may provide appropriate ways of measuring the adequacy of different explanation systems for these purposes. Paper 377 Key questions and contributions This work builds upon and contributes to these disparate areas of research. Work on perceptions of justice reveals much about the role of explanations in ethical assessments of human decisions but the applicability of these findings to algorithmic decisions and the range of proposed explanation systems proposed for them remains largely unexplored. A notable exception is Kizilcec who studied the effect of levels of transparency on perceptions of procedural justice. However the nuances of the different explanation approaches recently developed in interpretable ML have not yet been explored in relation to perceptions of justice. We also do not yet know much the extent to which findings on correlations between perceptions of justice from human decisionmaking contexts hold in the context of algorithmic decisions. Furthermore as research into practically achievable explanations for machine learning systems accelerates researchers are turning to consider which explanation styles are more desirable rather than simply possible to implement. This itself is a loaded question an explanation style that artificially inflates certain notions of justice by playing on cognitive biases might suffer from the same anti paternalist critiques levied against proponents of nudge philosophy. Understanding how explanation styles might influence and interact with justice perceptions is therefore an important preliminary to further development of ML explanation tools. With these issues in mind our key questions are 1. How do explanations for algorithmic decisions affect justice perceptions regarding algorithmic decisions In particular do the positive correlations observed between informational procedural and distributive justice in human decisionmaking settings also hold in algorithmic decisionmaking settings 2. How do different styles of explanation affect such justice perceptions STUDY DESIGN AND METHODOLOGY In investigating these questions we combine several methodological aspects of the related work from justice perceptions and intelligent systems research. Many of the previously mentioned machine learning model explanation systems have not been subjected to user evaluations and those which have often only test peoples ability to predict a models outputs or answer questions about its logic e.g.. For present purposes we are not concerned about how faithfully an explanation facility imparts actual model logic to a user.2 Rather we are interested in how the provision of various kinds of information about a models outputs might affect justice perceptions as measured in the psychological literature. Much of this research is based on field surveys of individuals with personal experience of being affected by decisions e.g. although controlled experiments involving simulated decisionmaking and scenario based methods have also been used. Given our focus on the effects of information provision and explanation styles on perceptions of fairness we conducted a set of experimental studies to elicit peoples responses to a range of 2 Although we acknowledge this is an important challenge if explanations are to be more than just comforting stories as cautioned by Lipton. Page 3 Paper plausible algorithmic decision scenarios and associated explanations. While this may lack the ecological validity of field surveys it gives us the ability to engage people with a wide range of novel explanation styles covered in 2.1 and have yet to be implemented alongside real machine learning systems currently deployed in the wild to any significant degree. By testing out proposed explanation styles with fictional scenarios we hope that designers can gain insight into user concerns and perhaps create more meaningful explanation interfaces to meet the upcoming legal requirements. Scenarios and application contexts We selected 5 application contexts in which to situate the fictional scenarios. The main criteria for context selection were a relatively common interaction for the target population a large proportion has experienced or will experience it at some point in their life likely to involve machine learning models for algorithmic decisionmaking at present or in the near future have significant economic or practical effects on the decision subject and would possibly fall under the aforementioned GDPR regulation on automated decisions.3 After consulting relevant literature on how ML systems are being applied in each context e.g. we settled on the following contexts 1. Applying for a personal financial loan 2. Applying for a promotion at work 3. Car insurance premiums dynamically priced based on personal details and driving behaviour measured by a telematic sensor as described in e.g. 4. Passengers on overbooked airline flights being selected for re routing 5. Freezing of bank account due to activity suspected as associated with money laundering For each context we created fictional cases in which an individual has had a decision made about them automatically. These were inspired by now classic algorithmic war stories see e.g.. Scenarios and cases were adapted where necessary to fit the context from which the inperson participants would be recruited a city in the United Kingdom. Because explanations are most likely to be requested in response to a decision which has a negative outcome for the individual and considerations of procedural justice have been found to be more strongly felt in such cases each scenario ended in a negative outcome for the individual. Explanation styles In order to derive a set of explanation styles to test we reviewed a range of research including both technical work on interpretable machine learning models and legal discussions on the requirements of existing and forthcoming regulations on transparency of algorithmic decisions e.g.. Our aim was to find explanation styles which could plausibly meet or exceed the regulatory requirements regarding transparency of automated decisions in particular the requirement that organisations provide meaningful information about the logic involved in an automated decision GDPR art 15h. We aimed to condense the many 3 The hurdle that a decision must be based solely on automated processing is here read less restrictively than legal analysis has indicated. Paper 377 categories of promising explanation style 1. Input Influence Presents a list of input variables alongside a quantitative measure of their influence positive or negative on a decision e.g. 2. Sensitivity For each input variable used in a decision sensitivity analysis shows how much the value of that variable would have to differ in order to change the output class not to be confused with the notion of sensitivity used in ML evaluation e.g.. 3. Case based Presents a case from the models training data which is most similar to the decision being explained e.g. 4. Demographic Presents aggregate statistics on the outcome classes for people in the same demographic categories as the decision subject such as age gender income level or occupation e.g. These explanation styles were tested and refined in a small scale informal design phase. Each explanation system imparts different information which is presented in various ways in the literature including text graphs and bullet points. In order to control for these differences in representation we chose to use purely textual explanations. We informally tested each explanation style by varying the wording and information dimensions. We found that people generally did not pay attention to explanations independent of scenario descriptions. Through iterative testing we found that the use of dialog boxes focused attention on explanations see Figure 2. These were deployed and investigated in depth in an inperson lab study and two online experiments. All phases of each study were approved by relevant University ethical review procedures. Justice constructs The justice constructs used in all three studies were based on those developed in the psychology of justice research summarised by Colquitt et al. Participants were asked to rate their agreement with five statements on a 5point Likert scale see statements at the bottom of Figure 1. Statements were drawn and adapted from previous studies of human decisionmaking to fit scenarios in which the decision was being made by an automated algorithmic system. Preliminary testing found that certain questions from Colquitts scale particularly those relating to interactional justice such as Have you been able to express your views and feelings during those procedures were difficult to interpret. This was partly due to the hypothetical nature of the scenarios and partly due to being too human specific to apply to a computerbased interaction. The remaining questions pertained to informational procedural and distributive justice. Statement 2 understanding aimed to capture informational justice statements 3 and 4 appropriateness of factors fair process pertained to procedural justice and statement 5 deserved outcome addressed distributive Page 4 Paper Car insurance scenario A car insurance company provides customers with personalised prices based on their attributes and driving behaviours measured through a telematic sensor installed in the car. Their system for setting prices is based on a computer model which predicts how likely an applicant is to have an accident and make a claim. The computer makes its predictions based on data the system has collected about thousands of other drivers. Based on its ongoing analysis of the drivers chances of having an accident the system will automatically set their monthly premium. The cheapest premium tier given to those drivers who are judged by the system as the safest is 20 per month. Sarah Sarah is a customer of the insurance company. She is 35 years old Has been driving for 17 years Been in an accident once which was not her fault Drives 800 miles a month on average Exceeds the speed limit on average once every two months 20 of Sarahs driving takes place at night Based on this information the computer system has not qualified Sarah for the cheapest tier of insurance premium. The insurer provides Sarah with the following information about the computers decision. This decision was based on thousands of similar cases from the past. For example a similar case to yours is a previous customer Claire. She was 38 years old with 18 years of driving experience drove 850 miles per month occasionally exceeded the speed limit and 25 of her trips took place at night. Claire was involved in one accident in the following year. Please rate your agreement with the following statements 1. Agreement I agree with the decision 2. Understanding I understand the process by which the decision was made 3. Appropriateness of factors The factors considered in the decision were appropriate 4. Fair process The decisionmaking process was fair 5. Deserved outcome The individual deserved this outcome given their circumstances or behaviour Figure 1. Example of a scenario car insurance case description Sarah explanation Case based and questions dimensions. We also found that people tended to interpret these justice construct statements as a proxy for the simpler question of whether or not they agreed with the decision to avoid this an additional statement about agreement 1 agreement was also included prior to the others. Phase 1 Lab study The first study consisted of inperson semistructured interviews focused around a series of fictionalised cases as described above. The purpose of this study was to gain indepth insights into how people interpret evaluate and reason about algorithmic decisionmaking in the range of contexts and explanation styles mentioned above. Participants were asked to consider their agreement with the five measures described in section 3.3 and Figure 1. Agreement was indicated on a 5point Likert scale ranging from strongly agree to strongly disagree. Participants were asked to verbalise their responses to each case and explain their thought processes for each response. Each case was presented with one of the 4 explanation styles described above or a control condition in which no explanation was provided. Participants were assigned pseudonymous identifiers to be associated with data generated during the experiment. Audio recordings taken from the interviews and thinkaloud processes were transcribed segmented and labelled by each case and explanation style and any personally identifying information was redacted. Thematic analysis was undertaken to identify common themes in participants interpretation evaluation and reasoning in response to cases. Thematic codes were independently developed by three coders after which a single set was jointly agreed upon. The data were then recoded using the new set of codes. Phase 2 Online studies Following on from the lab study two online studies were conducted in order to generate quantitative data to test the following hypotheses 1. Do different explanation styles result in differences in perceived levels of justice 2. Do the positive correlations observed between informational procedural and distributive justice in human decisionmaking settings also hold in algorithmic decisionmaking settings Both online studies consisted of pared down versions of the protocols used in phase 1. Participants were recruited via the study platform Prolific Academic4 and filtered to include only individuals over 18 based in the UK in order to maintain similarity with the participants recruited in phase 1. A betweensubjects design was used in order to test differences between responses under different explanation conditions. On the basis of described responses to the various explanation styles in the lab study it was hypothesised that the ability to make direct comparisons between explanation styles might affect justice perceptions. We therefore also devised a followup withinsubjects experiment to test whether exposure to multiple explanation styles for a single case would have different effects than repeated exposure to a single explanation style. 19 participants were recruited through paper flyers placed in various permitted locations in a small city in the UK including cafes and restaurants churches libraries shops and museums and through official social media accounts of the research lab and personal accounts of the researchers. Participants were then invited to the lab where the study occurred which was set up with a standard PC keyboard and mouse and a 27 inch display with a screen resolution of 1920x1200. Between subjects study Participants were presented with 3 cases for each of the 5 contexts 15 cases in total via a webbased interface. Cases were presented with a brief description of the individual the decision outcome the explanation except in the control condition and an illustrative image depicting the context. The cases were used to prompt reflection on various aspects of the decision and any explanatory information provided using a concurrent thinkaloud process. The withinsubjects study involved 65 participants. Only the loan and insurance cases were included in this design as the other Paper 377 325 participants were randomly assigned to 1 of 5 conditions n 65 in each group. Each group was presented with a selection of 12 cases used in experiment 1 accompanied by one of the 4 different explanation styles or a control condition featuring no explanation.5 Participants were asked to indicate their agreement with the 5 measures described in phase 1 using the same Likert scale. Within subjects study 4 www.prolific.ac 5 The airline context was excluded at this stage because experiment 1 had revealed that participants almost universally judged the cases to be very unfair and undeserved thus preventing any useful conclusions to be drawn from this context. Page 5 Paper Input influencebased explanation Demographicbased explanation Casebased explanation Sensitivitybased explanation Our predictive model assessed your personal information and driving behaviour in order to predict your chances of having an accident. The more s ors the more positively or negatively that factor impacted your predicted chance of accidents. Unimportant factors are indicated. Your age Driving experience Level of adherence to speed limit Number of trips taken at night Miles per month 29 of female drivers qualified for the cheapest tier. 31 of drivers in your age group qualified for the cheapest tier. 35 of drivers with 17 years of experience qualified for the cheapest tier. 15 of drivers who have been on one accident which was not their fault qualified for the cheapest tier. 26 of drivers who regularly travel at night qualified for the cheapest tier. 21 of drivers who exceed the speed limit once ever two months qualified for the cheapest tier This decision was based on thousands of similar cases from the past. For example a similar case to yours is a previous customer Claire. She was 38 years old with 18 years of driving experience drove 850 miles per month occasionally exceeded the speed limit and 25 of her trips took place at night. Claire was involved in one accident in the following year. If 10 or less of your driving took place at night you would have qualified for the cheapest tier. If your average miles per month were 700 or less you would have qualified for the cheapest tier. ok ok ok ok Figure 2. Illustrative examples of the different explanation types presented to participants input influence sensitivity demographic and case based. scenarios could not be adapted to fit into a realistic hypothetical scenario in which four different explanations would be presented for 
Data gathering methodology taxonomy: brainstorming and focus groups, interviews, online surveys and polls, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, social media analysis, website analysis, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, brainstorming and focus groups, 
Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.

6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, interviews, Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, interviews, Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, ['interviews', 'online surveys and polls', 'questionnaires', 'think-aloud sessions', ''], Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews, online surveys and polls, questionnaires, think-aloud sessions, , ", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK].
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK].
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,think-aloud sessions,,", Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", [['Token: The, logprob: -0.005309535, linearprob: 99.47', 'Token: Classification, logprob: -5.974712, linearprob: 0.25', 'Token:  The, logprob: -7.469745, linearprob: 0.06', 'Token: Based, logprob: -7.7467456, linearprob: 0.04', 'Token: "The, logprob: -7.819879, linearprob: 0.04'], ['Token:  set, logprob: -7.231183e-05, linearprob: 99.99', 'Token:  data, logprob: -10.070559, linearprob: 0.0', 'Token:  research, logprob: -10.5865555, linearprob: 0.0', 'Token:  classification, logprob: -13.1126, linearprob: 0.0', 'Token:  paper, logprob: -14.091829, linearprob: 0.0'], ['Token:  of, logprob: -1.3856493e-06, linearprob: 100.0', 'Token:  for, logprob: -14.191886, linearprob: 0.0', 'Token:  containing, logprob: -15.627276, linearprob: 0.0', 'Token:  this, logprob: -15.8104315, linearprob: 0.0', 'Token:  data, logprob: -16.59639, linearprob: 0.0'], ['Token:  this, logprob: -2.4749568e-05, linearprob: 100.0', 'Token:  data, logprob: -11.070122, linearprob: 0.0', 'Token:  research, logprob: -12.616474, linearprob: 0.0', 'Token:  the, logprob: -12.704845, linearprob: 0.0', 'Token:  methodologies, logprob: -13.1517315, linearprob: 0.0'], ['Token:  research, logprob: -3.1281633e-07, linearprob: 100.0', 'Token:  re, logprob: -16.311758, linearprob: 0.0', 'Token:  res, logprob: -16.838037, linearprob: 0.0', 'Token:  Research, logprob: -17.145853, linearprob: 0.0', 'Token:  paper, logprob: -17.42683, linearprob: 0.0'], ['Token:  paper, logprob: -1.378283e-05, linearprob: 100.0', 'Token:  papers, logprob: -11.206938, linearprob: 0.0', 'Token:  pap, logprob: -16.352772, linearprob: 0.0', 'Token:  p, logprob: -17.316196, linearprob: 0.0', "Token: 's, logprob: -17.5731, linearprob: 0.0"], ["Token: 's, logprob: -1.2233183e-05, linearprob: 100.0", 'Token: ’s, logprob: -11.324767, linearprob: 0.0', "Token: \\', logprob: -16.951853, linearprob: 0.0", 'Token: ‘s, logprob: -17.942629, linearprob: 0.0', 'Token: ´s, logprob: -17.95062, linearprob: 0.0'], ['Token:  data, logprob: -4.3202e-07, linearprob: 100.0', 'Token:  Data, logprob: -15.989336, linearprob: 0.0', 'Token:  gathering, logprob: -16.234758, linearprob: 0.0', 'Token:  , logprob: -16.34314, linearprob: 0.0', 'Token:  dat, logprob: -17.846912, linearprob: 0.0'], ['Token:  gathering, logprob: -0.00033367777, linearprob: 99.97', 'Token: -g, logprob: -8.009314, linearprob: 0.03', 'Token:  g, logprob: -14.497168, linearprob: 0.0', 'Token:  gather, logprob: -14.98284, linearprob: 0.0', 'Token:  gathered, logprob: -15.804808, linearprob: 0.0'], ['Token:  methodologies, logprob: -6.8854926e-05, linearprob: 99.99', 'Token:  methodology, logprob: -9.820597, linearprob: 0.01', 'Token:  methods, logprob: -11.468374, linearprob: 0.0', 'Token:  method, logprob: -13.058228, linearprob: 0.0', 'Token:  meth, logprob: -14.510746, linearprob: 0.0'], ['Token:  contains, logprob: -9.72254e-05, linearprob: 99.99', 'Token:  contain, logprob: -9.478054, linearprob: 0.01', 'Token:  includes, logprob: -11.370745, linearprob: 0.0', 'Token:  contained, logprob: -12.577023, linearprob: 0.0', 'Token:  may, logprob: -13.834984, linearprob: 0.0'], ['Token:  interviews, logprob: -0.6520915, linearprob: 52.1', 'Token:  brainstorm, logprob: -0.77390623, linearprob: 46.12', 'Token:  focus, logprob: -4.795673, linearprob: 0.83', 'Token:  online, logprob: -5.055797, linearprob: 0.64', 'Token:  question, logprob: -6.69227, linearprob: 0.12'], ['Token: ,, logprob: -0.0070147947, linearprob: 99.3', 'Token:  and, logprob: -4.9877696, linearprob: 0.68', 'Token: ., logprob: -8.74312, linearprob: 0.02', 'Token:  [, logprob: -12.396238, linearprob: 0.0', 'Token:  as, logprob: -13.709982, linearprob: 0.0'], ['Token:  online, logprob: -0.6718775, linearprob: 51.07', 'Token:  think, logprob: -1.3835303, linearprob: 25.07', 'Token:  lab, logprob: -1.8015982, linearprob: 16.5', 'Token:  question, logprob: -3.8718324, linearprob: 2.08', 'Token:  in, logprob: -4.0416894, linearprob: 1.76'], ['Token:  surveys, logprob: -7.89631e-07, linearprob: 100.0', 'Token:  survey, logprob: -14.142451, linearprob: 0.0', 'Token:  studies, logprob: -17.449091, linearprob: 0.0', 'Token:  experiments, logprob: -17.984976, linearprob: 0.0', 'Token:  sur, logprob: -18.099329, linearprob: 0.0'], ['Token:  and, logprob: -0.049020372, linearprob: 95.22', 'Token: ,, logprob: -3.0400138, linearprob: 4.78', 'Token: /p, logprob: -12.908979, linearprob: 0.0', 'Token: ,p, logprob: -14.87677, linearprob: 0.0', 'Token:  &, logprob: -15.282038, linearprob: 0.0'], ['Token:  polls, logprob: -1.1279553e-05, linearprob: 100.0', 'Token:  question, logprob: -11.511311, linearprob: 0.0', 'Token:  polling, logprob: -14.310423, linearprob: 0.0', 'Token:  poll, logprob: -15.189335, linearprob: 0.0', 'Token:  focus, logprob: -15.865662, linearprob: 0.0'], ['Token: ,, logprob: -0.0012933317, linearprob: 99.87', 'Token: ., logprob: -6.6556416, linearprob: 0.13', 'Token:  and, logprob: -12.4835205, linearprob: 0.0', 'Token: .\n, logprob: -14.514419, linearprob: 0.0', 'Token: .\n\n, logprob: -14.932302, linearprob: 0.0'], ['Token:  question, logprob: -0.28093326, linearprob: 75.51', 'Token:  think, logprob: -1.687208, linearprob: 18.5', 'Token:  and, logprob: -3.076311, linearprob: 4.61', 'Token:  conceptual, logprob: -4.4899693, linearprob: 1.12', 'Token:  focus, logprob: -7.174963, linearprob: 0.08'], ['Token: naires, logprob: -7.226629e-06, linearprob: 100.0', 'Token: aires, logprob: -12.128685, linearprob: 0.0', 'Token: n, logprob: -13.704836, linearprob: 0.0', 'Token: na, logprob: -15.313597, linearprob: 0.0', 'Token: aries, logprob: -15.542629, linearprob: 0.0'], ['Token: ,, logprob: -0.052195694, linearprob: 94.91', 'Token: ., logprob: -2.9816277, linearprob: 5.07', 'Token:  and, logprob: -9.200862, linearprob: 0.01', 'Token: .\n, logprob: -11.058512, linearprob: 0.0', 'Token:  as, logprob: -12.31844, linearprob: 0.0'], ['Token:  think, logprob: -0.67139345, linearprob: 51.1', 'Token:  and, logprob: -0.7667609, linearprob: 46.45', 'Token:  conceptual, logprob: -4.1724215, linearprob: 1.54', 'Token:  brainstorm, logprob: -5.779458, linearprob: 0.31', 'Token:  focus, logprob: -5.9284716, linearprob: 0.27'], ['Token: -al, logprob: -2.1650272e-05, linearprob: 100.0', 'Token: al, logprob: -11.088185, linearprob: 0.0', 'Token:  aloud, logprob: -12.468804, linearprob: 0.0', 'Token: -, logprob: -13.466283, linearprob: 0.0', 'Token: -out, logprob: -15.083877, linearprob: 0.0'], ['Token: oud, logprob: -2.3392786e-06, linearprob: 100.0', 'Token: ound, logprob: -13.209295, linearprob: 0.0', 'Token: ould, logprob: -15.645962, linearprob: 0.0', 'Token: ong, logprob: -15.988115, linearprob: 0.0', 'Token: oad, logprob: -16.645168, linearprob: 0.0'], ['Token:  sessions, logprob: -3.2929079e-06, linearprob: 100.0', 'Token: sessions, logprob: -13.248258, linearprob: 0.0', 'Token:  session, logprob: -14.100753, linearprob: 0.0', 'Token: s, logprob: -14.380668, linearprob: 0.0', 'Token: ,, logprob: -16.845459, linearprob: 0.0'], ['Token: ., logprob: -0.20282486, linearprob: 81.64', 'Token: ,, logprob: -1.6995575, linearprob: 18.28', 'Token: <|end|>, logprob: -7.4364533, linearprob: 0.06', 'Token: .\n, logprob: -9.035452, linearprob: 0.01', 'Token:  and, logprob: -10.312227, linearprob: 0.0']],Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,online surveys and polls,questionnaires,think-aloud sessions,,", Token: The, logprob: -0.008024168, linearprob: 99.2Token: Classification, logprob: -5.8876495, linearprob: 0.28Token: Based, logprob: -6.715332, linearprob: 0.12Token: "The, logprob: -7.0063877, linearprob: 0.09Token:  The, logprob: -7.3565025, linearprob: 0.06Token:  set, logprob: -5.669615e-05, linearprob: 99.99Token:  data, logprob: -10.310082, linearprob: 0.0Token:  research, logprob: -10.879796, linearprob: 0.0Token:  classification, logprob: -13.157874, linearprob: 0.0Token:  paper, logprob: -14.243243, linearprob: 0.0Token:  of, logprob: -1.7432603e-06, linearprob: 100.0Token:  for, logprob: -13.877274, linearprob: 0.0Token:  containing, logprob: -15.651192, linearprob: 0.0Token:  this, logprob: -15.660219, linearprob: 0.0Token:  , logprob: -16.386656, linearprob: 0.0Token:  this, logprob: -2.7014437e-05, linearprob: 100.0Token:  data, logprob: -11.021641, linearprob: 0.0Token:  the, logprob: -12.562574, linearprob: 0.0Token:  research, logprob: -12.670404, linearprob: 0.0Token:  methodologies, logprob: -12.787523, linearprob: 0.0Token:  research, logprob: -3.1281633e-07, linearprob: 100.0Token:  re, logprob: -16.365124, linearprob: 0.0Token:  res, logprob: -17.146893, linearprob: 0.0Token:  Research, logprob: -17.21376, linearprob: 0.0Token:  paper, logprob: -17.45417, linearprob: 0.0Token:  paper, logprob: -1.2113979e-05, linearprob: 100.0Token:  papers, logprob: -11.338168, linearprob: 0.0Token:  pap, logprob: -16.90676, linearprob: 0.0Token: 's, logprob: -17.262724, linearprob: 0.0Token:  p, logprob: -17.390152, linearprob: 0.0Token: 's, logprob: -1.700133e-05, linearprob: 100.0Token: ’s, logprob: -10.99567, linearprob: 0.0Token: \', logprob: -16.866928, linearprob: 0.0Token: ‘s, logprob: -17.78636, linearprob: 0.0Token: ´s, logprob: -17.985933, linearprob: 0.0Token:  data, logprob: -4.3202e-07, linearprob: 100.0Token:  Data, logprob: -16.000257, linearprob: 0.0Token:  gathering, logprob: -16.09251, linearprob: 0.0Token:  , logprob: -16.527765, linearprob: 0.0Token:  dat, logprob: -18.091341, linearprob: 0.0Token:  gathering, logprob: -0.0003390383, linearprob: 99.97Token: -g, logprob: -7.9935107, linearprob: 0.03Token:  g, logprob: -14.399206, linearprob: 0.0Token:  gather, logprob: -14.938192, linearprob: 0.0Token:  gathered, logprob: -15.773014, linearprob: 0.0Token:  methodologies, logprob: -6.730528e-05, linearprob: 99.99Token:  methodology, logprob: -9.83352, linearprob: 0.01Token:  methods, logprob: -11.455577, linearprob: 0.0Token:  method, logprob: -13.22923, linearprob: 0.0Token:  contains, logprob: -14.957164, linearprob: 0.0Token:  contains, logprob: -0.00011236426, linearprob: 99.99Token:  contain, logprob: -9.312235, linearprob: 0.01Token:  includes, logprob: -11.278282, linearprob: 0.0Token:  contained, logprob: -12.793161, linearprob: 0.0Token:  may, logprob: -13.655836, linearprob: 0.0Token:  interviews, logprob: -0.42433935, linearprob: 65.42Token:  brainstorm, logprob: -1.130909, linearprob: 32.27Token:  focus, logprob: -4.328703, linearprob: 1.32Token:  online, logprob: -5.1286926, linearprob: 0.59Token:  question, logprob: -6.644827, linearprob: 0.13Token: ,, logprob: -0.0038718083, linearprob: 99.61Token:  and, logprob: -5.579729, linearprob: 0.38Token: ., logprob: -9.392643, linearprob: 0.01Token:  [, logprob: -12.831306, linearprob: 0.0Token:  as, logprob: -14.177099, linearprob: 0.0Token:  online, logprob: -0.6754886, linearprob: 50.89Token:  think, logprob: -1.1931278, linearprob: 30.33Token:  lab, logprob: -2.3062525, linearprob: 9.96Token:  question, logprob: -3.545535, linearprob: 2.89Token:  conceptual, logprob: -4.0858183, linearprob: 1.68Token:  surveys, logprob: -5.5122365e-07, linearprob: 100.0Token:  survey, logprob: -14.638148, linearprob: 0.0Token:  studies, logprob: -17.986013, linearprob: 0.0Token:  sur, logprob: -18.458326, linearprob: 0.0Token:  experiments, logprob: -18.677477, linearprob: 0.0Token:  and, logprob: -0.035947673, linearprob: 96.47Token: ,, logprob: -3.3437326, linearprob: 3.53Token: /p, logprob: -12.731873, linearprob: 0.0Token: ,p, logprob: -14.953129, linearprob: 0.0Token:  &, logprob: -15.357637, linearprob: 0.0Token:  polls, logprob: -1.0325924e-05, linearprob: 100.0Token:  question, logprob: -11.587213, linearprob: 0.0Token:  polling, logprob: -14.507806, linearprob: 0.0Token:  poll, logprob: -15.328359, linearprob: 0.0Token:  surveys, logprob: -15.749141, linearprob: 0.0Token: ,, logprob: -0.0003649018, linearprob: 99.96Token: ., logprob: -7.9293427, linearprob: 0.04Token:  and, logprob: -12.616807, linearprob: 0.0Token: .
, logprob: -15.190529, linearprob: 0.0Token: .

, logprob: -15.331766, linearprob: 0.0Token:  question, logprob: -0.3101221, linearprob: 73.34Token:  think, logprob: -1.5368372, linearprob: 21.51Token:  and, logprob: -3.2253575, linearprob: 3.97Token:  conceptual, logprob: -4.737578, linearprob: 0.88Token:  focus, logprob: -6.845757, linearprob: 0.11Token: naires, logprob: -2.9352968e-06, linearprob: 100.0Token: aires, logprob: -13.094277, linearprob: 0.0Token: n, logprob: -14.504046, linearprob: 0.0Token: aries, logprob: -15.879991, linearprob: 0.0Token: na, logprob: -15.95658, linearprob: 0.0Token: ,, logprob: -0.025535733, linearprob: 97.48Token: ., logprob: -3.6857493, linearprob: 2.51Token:  and, logprob: -9.144211, linearprob: 0.01Token: .
, logprob: -11.556616, linearprob: 0.0Token: .

, logprob: -12.550509, linearprob: 0.0Token:  think, logprob: -0.6656652, linearprob: 51.39Token:  and, logprob: -0.76493126, linearprob: 46.54Token:  conceptual, logprob: -4.246077, linearprob: 1.43Token:  brainstorm, logprob: -6.103968, linearprob: 0.22Token:  focus, logprob: -6.348002, linearprob: 0.18Token: -al, logprob: -1.4140442e-05, linearprob: 100.0Token: al, logprob: -11.608766, linearprob: 0.0Token:  aloud, logprob: -12.67986, linearprob: 0.0Token: -, logprob: -13.657008, linearprob: 0.0Token: ‐, logprob: -15.323398, linearprob: 0.0Token: oud, logprob: -2.577686e-06, linearprob: 100.0Token: ound, logprob: -13.063451, linearprob: 0.0Token: ould, logprob: -15.727513, linearprob: 0.0Token: ong, logprob: -15.89137, linearprob: 0.0Token: oad, logprob: -16.372164, linearprob: 0.0Token:  sessions, logprob: -3.4121115e-06, linearprob: 100.0Token: sessions, logprob: -13.083067, linearprob: 0.0Token:  session, logprob: -14.229937, linearprob: 0.0Token: s, logprob: -14.393793, linearprob: 0.0Token: ,, logprob: -16.991133, linearprob: 0.0Token: ., logprob: -0.4219539, linearprob: 65.58Token: ,, logprob: -1.068322, linearprob: 34.36Token: <|end|>, logprob: -7.818439, linearprob: 0.04Token: .
, logprob: -9.048079, linearprob: 0.01Token:  and, logprob: -9.789795, linearprob: 0.01,Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250, 2018, 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions, "interviews,think-aloud sessions,,", "logprobs",Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"interviews, online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.0053446363, linearprob: 99.47 Token: Classification, logprob: -6.6514935, linearprob: 0.13 Token: "The, logprob: -6.903395, linearprob: 0.1 Token:  The, logprob: -7.2348256, linearprob: 0.07 Token: Based, logprob: -7.5440907, linearprob: 0.05 Token:  set, logprob: -4.85903e-05, linearprob: 100.0 Token:  data, logprob: -10.637183, linearprob: 0.0 Token:  research, logprob: -10.803372, linearprob: 0.0 Token:  classification, logprob: -13.153525, linearprob: 0.0 Token:  paper, logprob: -14.610829, linearprob: 0.0 Token:  of, logprob: -1.504853e-06, linearprob: 100.0 Token:  for, logprob: -14.163984, linearprob: 0.0 Token:  containing, logprob: -15.497263, linearprob: 0.0 Token:  this, logprob: -15.796061, linearprob: 0.0 Token:  data, logprob: -16.309841, linearprob: 0.0 Token:  this, logprob: -2.677603e-05, linearprob: 100.0 Token:  data, logprob: -10.953609, linearprob: 0.0 Token:  research, logprob: -12.588316, linearprob: 0.0 Token:  the, logprob: -12.617643, linearprob: 0.0 Token:  methodologies, logprob: -13.41696, linearprob: 0.0 Token:  research, logprob: -4.3202e-07, linearprob: 100.0 Token:  re, logprob: -15.855339, linearprob: 0.0 Token:  res, logprob: -16.626537, linearprob: 0.0 Token:  Research, logprob: -17.013416, linearprob: 0.0 Token:  paper, logprob: -17.519608, linearprob: 0.0 Token:  paper, logprob: -1.7120534e-05, linearprob: 100.0 Token:  papers, logprob: -10.997393, linearprob: 0.0 Token:  pap, logprob: -16.098898, linearprob: 0.0 Token: 's, logprob: -16.948444, linearprob: 0.0 Token:  p, logprob: -17.037888, linearprob: 0.0 Token: 's, logprob: -1.2590794e-05, linearprob: 100.0 Token: ’s, logprob: -11.295018, linearprob: 0.0 Token: \', logprob: -16.858479, linearprob: 0.0 Token: ‘s, logprob: -17.837997, linearprob: 0.0 Token: &#, logprob: -18.028784, linearprob: 0.0 Token:  data, logprob: -4.3202e-07, linearprob: 100.0 Token:  Data, logprob: -16.041773, linearprob: 0.0 Token:  gathering, logprob: -16.050755, linearprob: 0.0 Token:  , logprob: -16.262596, linearprob: 0.0 Token: data, logprob: -17.941397, linearprob: 0.0 Token:  gathering, logprob: -0.0003920656, linearprob: 99.96 Token: -g, logprob: -7.8483915, linearprob: 0.04 Token:  g, logprob: -14.377, linearprob: 0.0 Token:  gather, logprob: -14.754331, linearprob: 0.0 Token:  , logprob: -15.654744, linearprob: 0.0 Token:  methodologies, logprob: -0.00010545045, linearprob: 99.99 Token:  methodology, logprob: -9.364614, linearprob: 0.01 Token:  methods, logprob: -11.141622, linearprob: 0.0 Token:  method, logprob: -12.820884, linearprob: 0.0 Token:  contains, logprob: -14.003533, linearprob: 0.0 Token:  contains, logprob: -0.00010747691, linearprob: 99.99 Token:  contain, logprob: -9.385261, linearprob: 0.01 Token:  includes, logprob: -11.171239, linearprob: 0.0 Token:  contained, logprob: -12.85151, linearprob: 0.0 Token:  may, logprob: -13.506022, linearprob: 0.0 Token:  interviews, logprob: -0.63517994, linearprob: 52.98 Token:  brainstorm, logprob: -0.80038685, linearprob: 44.92 Token:  focus, logprob: -4.1933637, linearprob: 1.51 Token:  online, logprob: -5.6658063, linearprob: 0.35 Token: :, logprob: -6.962151, linearprob: 0.09 Token: ,, logprob: -0.014999741, linearprob: 98.51 Token:  and, logprob: -4.2385755, linearprob: 1.44 Token: ., logprob: -7.722546, linearprob: 0.04 Token:  [, logprob: -12.260649, linearprob: 0.0 Token:  as, logprob: -12.933737, linearprob: 0.0 Token:  online, logprob: -0.8091779, linearprob: 44.52 Token:  think, logprob: -1.3708348, linearprob: 25.39 Token:  lab, logprob: -1.5126061, linearprob: 22.03 Token:  in, logprob: -3.7566648, linearprob: 2.34 Token:  question, logprob: -3.9880261, linearprob: 1.85 Token:  surveys, logprob: -6.704273e-07, linearprob: 100.0 Token:  survey, logprob: -14.329085, linearprob: 0.0 Token:  studies, logprob: -16.748487, linearprob: 0.0 Token:  experiments, logprob: -17.261549, linearprob: 0.0 Token:  sur, logprob: -18.21169, linearprob: 0.0 Token:  and, logprob: -0.045203347, linearprob: 95.58 Token: ,, logprob: -3.1192353, linearprob: 4.42 Token: /p, logprob: -12.420497, linearprob: 0.0 Token: ,p, logprob: -14.453399, linearprob: 0.0 Token:  , logprob: -15.245412, linearprob: 0.0 Token:  polls, logprob: -2.2603901e-05, linearprob: 100.0 Token:  question, logprob: -10.807932, linearprob: 0.0 Token:  polling, logprob: -13.903278, linearprob: 0.0 Token:  focus, logprob: -14.709639, linearprob: 0.0 Token:  poll, logprob: -14.796349, linearprob: 0.0 Token: ,, logprob: -0.0034569292, linearprob: 99.65 Token: ., logprob: -5.6722517, linearprob: 0.34 Token:  and, logprob: -11.948344, linearprob: 0.0 Token: .
, logprob: -13.759506, linearprob: 0.0 Token: .

, logprob: -14.177918, linearprob: 0.0 Token:  question, logprob: -0.40814677, linearprob: 66.49 Token:  think, logprob: -1.482684, linearprob: 22.7 Token:  and, logprob: -2.3563452, linearprob: 9.48 Token:  conceptual, logprob: -4.6131983, linearprob: 0.99 Token:  focus, logprob: -6.7612867, linearprob: 0.12 Token: naires, logprob: -7.822647e-06, linearprob: 100.0 Token: aires, logprob: -12.117378, linearprob: 0.0 Token: n, logprob: -13.481203, linearprob: 0.0 Token: na, logprob: -15.14373, linearprob: 0.0 Token: aries, logprob: -15.416756, linearprob: 0.0 Token: ,, logprob: -0.12121274, linearprob: 88.58 Token: ., logprob: -2.1735637, linearprob: 11.38 Token:  and, logprob: -8.254019, linearprob: 0.03 Token: .
, logprob: -10.26977, linearprob: 0.0 Token:  as, logprob: -10.961621, linearprob: 0.0 Token:  and, logprob: -0.49862245, linearprob: 60.74 Token:  think, logprob: -0.9676509, linearprob: 38.0 Token:  conceptual, logprob: -4.87133, linearprob: 0.77 Token:  brainstorm, logprob: -6.150373, linearprob: 0.21 Token:  focus, logprob: -6.804973, linearprob: 0.11 Token:  think, logprob: -0.0014733182, linearprob: 99.85 Token:  focus, logprob: -7.5418253, linearprob: 0.05 Token:  conceptual, logprob: -7.8645315, linearprob: 0.04 Token:  concurrent, logprob: -9.049972, linearprob: 0.01 Token:  shadow, logprob: -9.328862, linearprob: 0.01 Token: -al, logprob: -1.342522e-05, linearprob: 100.0 Token: al, logprob: -11.534998, linearprob: 0.0 Token:  aloud, logprob: -13.072363, linearprob: 0.0 Token: -, logprob: -14.163784, linearprob: 0.0 Token: ‐, logprob: -15.259811, linearprob: 0.0 Token: oud, logprob: -2.220075e-06, linearprob: 100.0 Token: ound, logprob: -13.259214, linearprob: 0.0 Token: ong, logprob: -15.660976, linearprob: 0.0 Token: ould, logprob: -15.720211, linearprob: 0.0 Token: oad, logprob: -16.52056, linearprob: 0.0 Token:  sessions, logprob: -6.749814e-06, linearprob: 100.0 Token: sessions, logprob: -12.235902, linearprob: 0.0 Token:  session, logprob: -13.827771, linearprob: 0.0 Token: s, logprob: -14.426513, linearprob: 0.0 Token:  sess, logprob: -16.765692, linearprob: 0.0 Token: ., logprob: -0.0007829455, linearprob: 99.92 Token:  as, logprob: -8.519734, linearprob: 0.02 Token: <|end|>, logprob: -8.745983, linearprob: 0.02 Token: ,, logprob: -9.35297, linearprob: 0.01 Token:  based, logprob: -9.3628645, linearprob: 0.01 ",Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.013374922, linearprob: 98.67 Token: Classification, logprob: -4.4980206, linearprob: 1.11 Token:  The, logprob: -7.358006, linearprob: 0.06 Token: Class, logprob: -7.7474084, linearprob: 0.04 Token: Based, logprob: -8.33104, linearprob: 0.02 Token:  set, logprob: -5.574252e-05, linearprob: 99.99 Token:  data, logprob: -10.463451, linearprob: 0.0 Token:  research, logprob: -10.670481, linearprob: 0.0 Token:  classification, logprob: -13.315577, linearprob: 0.0 Token:  paper, logprob: -14.606707, linearprob: 0.0 Token:  of, logprob: -1.3856493e-06, linearprob: 100.0 Token:  for, logprob: -14.202308, linearprob: 0.0 Token:  containing, logprob: -15.574258, linearprob: 0.0 Token:  this, logprob: -16.043953, linearprob: 0.0 Token:  data, logprob: -16.331347, linearprob: 0.0 Token:  this, logprob: -3.4762674e-05, linearprob: 100.0 Token:  data, logprob: -10.49827, linearprob: 0.0 Token:  the, logprob: -12.691429, linearprob: 0.0 Token:  research, logprob: -13.079195, linearprob: 0.0 Token:  methodologies, logprob: -13.648218, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.30711, linearprob: 0.0 Token:  res, logprob: -17.086739, linearprob: 0.0 Token:  Research, logprob: -17.325188, linearprob: 0.0 Token:  paper, logprob: -17.511362, linearprob: 0.0 Token:  paper, logprob: -1.3186812e-05, linearprob: 100.0 Token:  papers, logprob: -11.266092, linearprob: 0.0 Token:  pap, logprob: -15.884048, linearprob: 0.0 Token:  p, logprob: -16.668741, linearprob: 0.0 Token: 's, logprob: -16.984554, linearprob: 0.0 Token: 's, logprob: -1.4378848e-05, linearprob: 100.0 Token: ’s, logprob: -11.1604395, linearprob: 0.0 Token: \', logprob: -16.978344, linearprob: 0.0 Token: ‘s, logprob: -17.770048, linearprob: 0.0 Token: ´s, logprob: -17.84572, linearprob: 0.0 Token:  data, logprob: -3.1281633e-07, linearprob: 100.0 Token:  gathering, logprob: -16.380764, linearprob: 0.0 Token:  Data, logprob: -16.525864, linearprob: 0.0 Token:  , logprob: -16.596151, linearprob: 0.0 Token:  dat, logprob: -18.152275, linearprob: 0.0 Token:  gathering, logprob: -0.00028648414, linearprob: 99.97 Token: -g, logprob: -8.162262, linearprob: 0.03 Token:  g, logprob: -14.681746, linearprob: 0.0 Token:  gather, logprob: -14.884043, linearprob: 0.0 Token:  gathered, logprob: -15.782063, linearprob: 0.0 Token:  methodologies, logprob: -8.792751e-05, linearprob: 99.99 Token:  methodology, logprob: -9.589098, linearprob: 0.01 Token:  methods, logprob: -11.128878, linearprob: 0.0 Token:  method, logprob: -12.8746605, linearprob: 0.0 Token:  contains, logprob: -14.21274, linearprob: 0.0 Token:  contains, logprob: -9.138441e-05, linearprob: 99.99 Token:  contain, logprob: -9.589586, linearprob: 0.01 Token:  includes, logprob: -11.207706, linearprob: 0.0 Token:  contained, logprob: -12.900175, linearprob: 0.0 Token:  may, logprob: -13.3983135, linearprob: 0.0 Token:  brainstorm, logprob: -0.51456887, linearprob: 59.78 Token:  interviews, logprob: -0.94360787, linearprob: 38.92 Token:  focus, logprob: -4.7483144, linearprob: 0.87 Token:  online, logprob: -5.7951264, linearprob: 0.3 Token: :, logprob: -7.553393, linearprob: 0.05 Token: ing, logprob: -3.888926e-06, linearprob: 100.0 Token: ings, logprob: -13.408899, linearprob: 0.0 Token: in, logprob: -14.072258, linearprob: 0.0 Token: ,, logprob: -14.543081, linearprob: 0.0 Token:  and, logprob: -15.301197, linearprob: 0.0 Token:  and, logprob: -0.23214333, linearprob: 79.28 Token: ,, logprob: -1.5758399, linearprob: 20.68 Token:  focus, logprob: -8.792181, linearprob: 0.02 Token:  sessions, logprob: -8.948933, linearprob: 0.01 Token: /f, logprob: -10.531441, linearprob: 0.0 Token:  focus, logprob: -9.5079726e-05, linearprob: 99.99 Token:  interviews, logprob: -9.477343, linearprob: 0.01 Token:  online, logprob: -11.687889, linearprob: 0.0 Token:  Focus, logprob: -12.643095, linearprob: 0.0 Token:  focused, logprob: -13.451511, linearprob: 0.0 Token:  groups, logprob: -9.133887e-06, linearprob: 100.0 Token:  group, logprob: -11.617237, linearprob: 0.0 Token: groups, logprob: -17.14958, linearprob: 0.0 Token: -groups, logprob: -17.949015, linearprob: 0.0 Token:  , logprob: -18.087635, linearprob: 0.0 Token: ,, logprob: -0.00015776986, linearprob: 99.98 Token: ., logprob: -9.034358, linearprob: 0.01 Token:  interviews, logprob: -10.852244, linearprob: 0.0 Token:  as, logprob: -12.199594, linearprob: 0.0 Token:  and, logprob: -12.362111, linearprob: 0.0 Token:  interviews, logprob: -0.0010848518, linearprob: 99.89 Token:  online, logprob: -7.603088, linearprob: 0.05 Token:  sem, logprob: -8.195713, linearprob: 0.03 Token:  in, logprob: -8.316093, linearprob: 0.02 Token:  semi, logprob: -10.71092, linearprob: 0.0 Token: ,, logprob: -0.030947214, linearprob: 96.95 Token: ., logprob: -3.4980836, linearprob: 3.03 Token:  and, logprob: -8.954064, linearprob: 0.01 Token: <|end|>, logprob: -10.306311, linearprob: 0.0 Token: .
, logprob: -11.878403, linearprob: 0.0 Token:  online, logprob: -0.6150302, linearprob: 54.06 Token:  and, logprob: -1.5659904, linearprob: 20.89 Token:  think, logprob: -2.2186298, linearprob: 10.88 Token:  question, logprob: -2.6908016, linearprob: 6.78 Token:  lab, logprob: -3.3540745, linearprob: 3.49 Token:  surveys, logprob: -6.704273e-07, linearprob: 100.0 Token:  survey, logprob: -14.762252, linearprob: 0.0 Token:  polls, logprob: -16.904728, linearprob: 0.0 Token:  sur, logprob: -17.479134, linearprob: 0.0 Token:  studies, logprob: -18.00473, linearprob: 0.0 Token:  and, logprob: -0.0038026923, linearprob: 99.62 Token: ,, logprob: -5.5746226, linearprob: 0.38 Token: /p, logprob: -13.971675, linearprob: 0.0 Token:  , logprob: -14.6364565, linearprob: 0.0 Token: ., logprob: -14.737329, linearprob: 0.0 Token:  polls, logprob: -1.6240566e-06, linearprob: 100.0 Token:  question, logprob: -13.794451, linearprob: 0.0 Token:  polling, logprob: -15.064377, linearprob: 0.0 Token:  poll, logprob: -15.779762, linearprob: 0.0 Token:  , logprob: -17.819298, linearprob: 0.0 Token: ,, logprob: -0.010118452, linearprob: 98.99 Token: ., logprob: -4.6024284, linearprob: 1.0 Token:  and, logprob: -10.791541, linearprob: 0.0 Token: .
, logprob: -12.45235, linearprob: 0.0 Token:  as, logprob: -12.835939, linearprob: 0.0 Token:  question, logprob: -0.07722733, linearprob: 92.57 Token:  and, logprob: -3.27048, linearprob: 3.8 Token:  think, logprob: -3.400685, linearprob: 3.34 Token:  conceptual, logprob: -6.3592496, linearprob: 0.17 Token:  interviews, logprob: -8.060458, linearprob: 0.03 Token: naires, logprob: -1.1398757e-05, linearprob: 100.0 Token: aires, logprob: -11.457422, linearprob: 0.0 Token: n, logprob: -14.8408375, linearprob: 0.0 Token: aries, logprob: -15.343006, linearprob: 0.0 Token: na, logprob: -16.511038, linearprob: 0.0 Token: ,, logprob: -0.03551574, linearprob: 96.51 Token: ., logprob: -3.3635416, linearprob: 3.46 Token:  and, logprob: -8.533505, linearprob: 0.02 Token:  as, logprob: -10.552981, linearprob: 0.0 Token: .
, logprob: -11.27241, linearprob: 0.0 Token:  think, logprob: -0.6212048, linearprob: 53.73 Token:  and, logprob: -0.83373684, linearprob: 43.44 Token:  conceptual, logprob: -3.7574143, linearprob: 2.33 Token:  work, logprob: -5.834872, linearprob: 0.29 Token:  concept, logprob: -7.68595, linearprob: 0.05 Token: -al, logprob: -1.3902034e-05, linearprob: 100.0 Token: al, logprob: -11.522838, linearprob: 0.0 Token:  aloud, logprob: -13.06206, linearprob: 0.0 Token: -, logprob: -13.706897, linearprob: 0.0 Token: ‐, logprob: -15.295869, linearprob: 0.0 Token: oud, logprob: -1.2664457e-06, linearprob: 100.0 Token: ound, logprob: -13.856511, linearprob: 0.0 Token: ould, logprob: -16.208767, linearprob: 0.0 Token: oad, logprob: -16.670725, linearprob: 0.0 Token: ong, logprob: -16.83814, linearprob: 0.0 Token:  sessions, logprob: -7.703444e-06, linearprob: 100.0 Token: sessions, logprob: -12.449089, linearprob: 0.0 Token:  session, logprob: -13.296888, linearprob: 0.0 Token: s, logprob: -13.332241, linearprob: 0.0 Token: ,, logprob: -15.774263, linearprob: 0.0 Token: ., logprob: -0.3318697, linearprob: 71.76 Token: ,, logprob: -1.2665926, linearprob: 28.18 Token: .
, logprob: -8.55356, linearprob: 0.02 Token: <|end|>, logprob: -8.801749, linearprob: 0.02 Token:  and, logprob: -8.976252, linearprob: 0.01 ",Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"interviews,online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.0053446363, linearprob: 99.47 Token: Classification, logprob: -6.6514935, linearprob: 0.13 Token: "The, logprob: -6.903395, linearprob: 0.1 Token:  The, logprob: -7.2348256, linearprob: 0.07 Token: Based, logprob: -7.5440907, linearprob: 0.05 Token:  set, logprob: -4.85903e-05, linearprob: 100.0 Token:  data, logprob: -10.637183, linearprob: 0.0 Token:  research, logprob: -10.803372, linearprob: 0.0 Token:  classification, logprob: -13.153525, linearprob: 0.0 Token:  paper, logprob: -14.610829, linearprob: 0.0 Token:  of, logprob: -1.504853e-06, linearprob: 100.0 Token:  for, logprob: -14.163984, linearprob: 0.0 Token:  containing, logprob: -15.497263, linearprob: 0.0 Token:  this, logprob: -15.796061, linearprob: 0.0 Token:  data, logprob: -16.309841, linearprob: 0.0 Token:  this, logprob: -2.677603e-05, linearprob: 100.0 Token:  data, logprob: -10.953609, linearprob: 0.0 Token:  research, logprob: -12.588316, linearprob: 0.0 Token:  the, logprob: -12.617643, linearprob: 0.0 Token:  methodologies, logprob: -13.41696, linearprob: 0.0 Token:  research, logprob: -4.3202e-07, linearprob: 100.0 Token:  re, logprob: -15.855339, linearprob: 0.0 Token:  res, logprob: -16.626537, linearprob: 0.0 Token:  Research, logprob: -17.013416, linearprob: 0.0 Token:  paper, logprob: -17.519608, linearprob: 0.0 Token:  paper, logprob: -1.7120534e-05, linearprob: 100.0 Token:  papers, logprob: -10.997393, linearprob: 0.0 Token:  pap, logprob: -16.098898, linearprob: 0.0 Token: 's, logprob: -16.948444, linearprob: 0.0 Token:  p, logprob: -17.037888, linearprob: 0.0 Token: 's, logprob: -1.2590794e-05, linearprob: 100.0 Token: ’s, logprob: -11.295018, linearprob: 0.0 Token: \', logprob: -16.858479, linearprob: 0.0 Token: ‘s, logprob: -17.837997, linearprob: 0.0 Token: &#, logprob: -18.028784, linearprob: 0.0 Token:  data, logprob: -4.3202e-07, linearprob: 100.0 Token:  Data, logprob: -16.041773, linearprob: 0.0 Token:  gathering, logprob: -16.050755, linearprob: 0.0 Token:  , logprob: -16.262596, linearprob: 0.0 Token: data, logprob: -17.941397, linearprob: 0.0 Token:  gathering, logprob: -0.0003920656, linearprob: 99.96 Token: -g, logprob: -7.8483915, linearprob: 0.04 Token:  g, logprob: -14.377, linearprob: 0.0 Token:  gather, logprob: -14.754331, linearprob: 0.0 Token:  , logprob: -15.654744, linearprob: 0.0 Token:  methodologies, logprob: -0.0001037816, linearprob: 99.99 Token:  methodology, logprob: -9.426021, linearprob: 0.01 Token:  methods, logprob: -10.940257, linearprob: 0.0 Token:  method, logprob: -12.846536, linearprob: 0.0 Token:  contains, logprob: -13.7497, linearprob: 0.0 Token:  contains, logprob: -0.0001245157, linearprob: 99.99 Token:  contain, logprob: -9.221183, linearprob: 0.01 Token:  includes, logprob: -11.0803995, linearprob: 0.0 Token:  contained, logprob: -12.776078, linearprob: 0.0 Token:  may, logprob: -13.366014, linearprob: 0.0 Token:  interviews, logprob: -0.645359, linearprob: 52.45 Token:  brainstorm, logprob: -0.7860908, linearprob: 45.56 Token:  focus, logprob: -4.2741566, linearprob: 1.39 Token:  online, logprob: -5.783657, linearprob: 0.31 Token: :, logprob: -6.899143, linearprob: 0.1 Token: ,, logprob: -0.016237998, linearprob: 98.39 Token:  and, logprob: -4.173568, linearprob: 1.54 Token: ., logprob: -7.2754717, linearprob: 0.07 Token:  [, logprob: -12.120574, linearprob: 0.0 Token:  for, logprob: -12.951898, linearprob: 0.0 Token:  online, logprob: -0.8191595, linearprob: 44.08 Token:  think, logprob: -1.3312349, linearprob: 26.42 Token:  lab, logprob: -1.520545, linearprob: 21.86 Token:  in, logprob: -3.6255636, linearprob: 2.66 Token:  question, logprob: -4.081872, linearprob: 1.69 Token:  surveys, logprob: -1.0280384e-06, linearprob: 100.0 Token:  survey, logprob: -14.086738, linearprob: 0.0 Token:  studies, logprob: -16.274796, linearprob: 0.0 Token:  experiments, logprob: -16.937456, linearprob: 0.0 Token:  sur, logprob: -17.839592, linearprob: 0.0 Token:  and, logprob: -0.071621254, linearprob: 93.09 Token: ,, logprob: -2.672043, linearprob: 6.91 Token: /p, logprob: -12.601118, linearprob: 0.0 Token: ,p, logprob: -14.221319, linearprob: 0.0 Token:  &, logprob: -15.163345, linearprob: 0.0 Token:  polls, logprob: -2.486877e-05, linearprob: 100.0 Token:  question, logprob: -10.698063, linearprob: 0.0 Token:  polling, logprob: -13.936136, linearprob: 0.0 Token:  focus, logprob: -14.59795, linearprob: 0.0 Token:  poll, logprob: -14.682156, linearprob: 0.0 Token: ,, logprob: -0.0020385603, linearprob: 99.8 Token: ., logprob: -6.200578, linearprob: 0.2 Token:  and, logprob: -12.219139, linearprob: 0.0 Token: .\n, logprob: -14.111231, linearprob: 0.0 Token:  as, logprob: -14.549136, linearprob: 0.0 Token:  question, logprob: -0.3639637, linearprob: 69.49 Token:  think, logprob: -1.5761906, linearprob: 20.68 Token:  and, logprob: -2.455608, linearprob: 8.58 Token:  conceptual, logprob: -4.657253, linearprob: 0.95 Token:  focus, logprob: -6.9663215, linearprob: 0.09 Token: naires, logprob: -1.0325924e-05, linearprob: 100.0 Token: aires, logprob: -11.8725195, linearprob: 0.0 Token: n, logprob: -13.121638, linearprob: 0.0 Token: na, logprob: -15.006595, linearprob: 0.0 Token: nn, logprob: -15.18411, linearprob: 0.0 Token: ,, logprob: -0.13559376, linearprob: 87.32 Token: ., logprob: -2.0674822, linearprob: 12.65 Token:  and, logprob: -8.771625, linearprob: 0.02 Token: .\n, logprob: -10.016453, linearprob: 0.0 Token: .\n\n, logprob: -11.049882, linearprob: 0.0 Token:  and, logprob: -0.6346168, linearprob: 53.01 Token:  think, logprob: -0.79043573, linearprob: 45.36 Token:  conceptual, logprob: -4.5125504, linearprob: 1.1 Token:  brainstorm, logprob: -5.957882, linearprob: 0.26 Token:  focus, logprob: -6.9573574, linearprob: 0.1 Token:  think, logprob: -0.00160639, linearprob: 99.84 Token:  focus, logprob: -7.441582, linearprob: 0.06 Token:  conceptual, logprob: -7.7615933, linearprob: 0.04 Token:  shadow, logprob: -8.915918, linearprob: 0.01 Token:  document, logprob: -9.671415, linearprob: 0.01 Token: -al, logprob: -1.1398757e-05, linearprob: 100.0 Token: al, logprob: -11.760128, linearprob: 0.0 Token:  aloud, logprob: -13.07473, linearprob: 0.0 Token: -, logprob: -14.162939, linearprob: 0.0 Token: ‐, logprob: -15.326767, linearprob: 0.0 Token: oud, logprob: -1.8624639e-06, linearprob: 100.0 Token: ound, logprob: -13.477003, linearprob: 0.0 Token: ong, logprob: -15.467625, linearprob: 0.0 Token: ould, logprob: -15.816771, linearprob: 0.0 Token: oad, logprob: -16.584597, linearprob: 0.0 Token:  sessions, logprob: -6.6306106e-06, linearprob: 100.0 Token: sessions, logprob: -12.268853, linearprob: 0.0 Token:  session, logprob: -13.807473, linearprob: 0.0 Token: s, logprob: -14.56527, linearprob: 0.0 Token:  sess, logprob: -16.619083, linearprob: 0.0 Token: ., logprob: -0.00091086753, linearprob: 99.91 Token:  as, logprob: -8.238769, linearprob: 0.03 Token: <|end|>, logprob: -8.747215, linearprob: 0.02 Token: ,, logprob: -9.033858, linearprob: 0.01 Token:  based, logprob: -9.161082, linearprob: 0.01 ",Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.013374922, linearprob: 98.67 Token: Classification, logprob: -4.4980206, linearprob: 1.11 Token:  The, logprob: -7.358006, linearprob: 0.06 Token: Class, logprob: -7.7474084, linearprob: 0.04 Token: Based, logprob: -8.33104, linearprob: 0.02 Token:  set, logprob: -6.098748e-05, linearprob: 99.99 Token:  data, logprob: -10.34297, linearprob: 0.0 Token:  research, logprob: -10.666056, linearprob: 0.0 Token:  classification, logprob: -12.922815, linearprob: 0.0 Token:  paper, logprob: -14.289242, linearprob: 0.0 Token:  of, logprob: -1.2664457e-06, linearprob: 100.0 Token:  for, logprob: -14.419368, linearprob: 0.0 Token:  containing, logprob: -15.791127, linearprob: 0.0 Token:  this, logprob: -16.002617, linearprob: 0.0 Token:  data, logprob: -16.325209, linearprob: 0.0 Token:  this, logprob: -3.6669935e-05, linearprob: 100.0 Token:  data, logprob: -10.446957, linearprob: 0.0 Token:  the, logprob: -12.653353, linearprob: 0.0 Token:  research, logprob: -13.025467, linearprob: 0.0 Token:  methodologies, logprob: -13.598948, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.384926, linearprob: 0.0 Token:  res, logprob: -17.06941, linearprob: 0.0 Token:  Research, logprob: -17.355215, linearprob: 0.0 Token:  paper, logprob: -17.766275, linearprob: 0.0 Token:  paper, logprob: -1.7597347e-05, linearprob: 100.0 Token:  papers, logprob: -10.976782, linearprob: 0.0 Token:  pap, logprob: -16.054682, linearprob: 0.0 Token: 's, logprob: -16.646803, linearprob: 0.0 Token:  p, logprob: -16.923298, linearprob: 0.0 Token: 's, logprob: -1.4378848e-05, linearprob: 100.0 Token: ’s, logprob: -11.163154, linearprob: 0.0 Token: \', logprob: -16.927313, linearprob: 0.0 Token: ‘s, logprob: -17.808525, linearprob: 0.0 Token: ´s, logprob: -17.849669, linearprob: 0.0 Token:  data, logprob: -3.1281633e-07, linearprob: 100.0 Token:  gathering, logprob: -16.330463, linearprob: 0.0 Token:  Data, logprob: -16.474606, linearprob: 0.0 Token:  , logprob: -16.54154, linearprob: 0.0 Token:  dat, logprob: -18.156538, linearprob: 0.0 Token:  gathering, logprob: -0.00042876197, linearprob: 99.96 Token: -g, logprob: -7.7581763, linearprob: 0.04 Token:  g, logprob: -14.511078, linearprob: 0.0 Token:  gather, logprob: -14.83338, linearprob: 0.0 Token:  gathered, logprob: -15.624084, linearprob: 0.0 Token:  methodologies, logprob: -9.674858e-05, linearprob: 99.99 Token:  methodology, logprob: -9.480237, linearprob: 0.01 Token:  methods, logprob: -11.084437, linearprob: 0.0 Token:  method, logprob: -12.822461, linearprob: 0.0 Token:  contains, logprob: -14.099145, linearprob: 0.0 Token:  contains, logprob: -9.054999e-05, linearprob: 99.99 Token:  contain, logprob: -9.591739, linearprob: 0.01 Token:  includes, logprob: -11.224399, linearprob: 0.0 Token:  contained, logprob: -12.94852, linearprob: 0.0 Token:  may, logprob: -13.402959, linearprob: 0.0 Token:  brainstorm, logprob: -0.40057993, linearprob: 66.99 Token:  interviews, logprob: -1.1509423, linearprob: 31.63 Token:  focus, logprob: -4.7020593, linearprob: 0.91 Token:  online, logprob: -5.8320413, linearprob: 0.29 Token: :, logprob: -7.4453053, linearprob: 0.06 Token: ing, logprob: -4.365741e-06, linearprob: 100.0 Token: ings, logprob: -13.274417, linearprob: 0.0 Token: in, logprob: -14.001578, linearprob: 0.0 Token: ,, logprob: -14.444917, linearprob: 0.0 Token:  and, logprob: -15.218471, linearprob: 0.0 Token:  and, logprob: -0.2402842, linearprob: 78.64 Token: ,, logprob: -1.5453227, linearprob: 21.32 Token:  sessions, logprob: -8.629332, linearprob: 0.02 Token:  focus, logprob: -8.980642, linearprob: 0.01 Token: /f, logprob: -10.579367, linearprob: 0.0 Token:  focus, logprob: -0.00032604876, linearprob: 99.97 Token:  interviews, logprob: -8.248198, linearprob: 0.03 Token:  online, logprob: -10.139484, linearprob: 0.0 Token:  Focus, logprob: -12.138792, linearprob: 0.0 Token:  shadow, logprob: -12.193459, linearprob: 0.0 Token:  groups, logprob: -5.5577775e-06, linearprob: 100.0 Token:  group, logprob: -12.129759, linearprob: 0.0 Token: groups, logprob: -17.358051, linearprob: 0.0 Token:  , logprob: -18.02925, linearprob: 0.0 Token: -groups, logprob: -18.044456, linearprob: 0.0 Token: ,, logprob: -0.00016384924, linearprob: 99.98 Token: ., logprob: -9.038267, linearprob: 0.01 Token:  interviews, logprob: -10.806858, linearprob: 0.0 Token:  as, logprob: -11.7358265, linearprob: 0.0 Token:  and, logprob: -12.08219, linearprob: 0.0 Token:  interviews, logprob: -0.0015903194, linearprob: 99.84 Token:  online, logprob: -7.106857, linearprob: 0.08 Token:  sem, logprob: -7.758119, linearprob: 0.04 Token:  in, logprob: -8.292862, linearprob: 0.03 Token:  semi, logprob: -10.408628, linearprob: 0.0 Token: ,, logprob: -0.0292653, linearprob: 97.12 Token: ., logprob: -3.5540361, linearprob: 2.86 Token:  and, logprob: -8.649059, linearprob: 0.02 Token: <|end|>, logprob: -11.437847, linearprob: 0.0 Token:  (, logprob: -11.615245, linearprob: 0.0 Token:  online, logprob: -0.63643676, linearprob: 52.92 Token:  and, logprob: -1.5529082, linearprob: 21.16 Token:  think, logprob: -2.0195982, linearprob: 13.27 Token:  question, logprob: -2.7817938, linearprob: 6.19 Token:  conceptual, logprob: -3.5388987, linearprob: 2.9 Token:  surveys, logprob: -7.89631e-07, linearprob: 100.0 Token:  survey, logprob: -14.636199, linearprob: 0.0 Token:  polls, logprob: -16.90682, linearprob: 0.0 Token:  sur, logprob: -17.124105, linearprob: 0.0 Token:  , logprob: -17.82988, linearprob: 0.0 Token:  and, logprob: -0.006992066, linearprob: 99.3 Token: ,, logprob: -4.9669814, linearprob: 0.7 Token: /p, logprob: -13.722012, linearprob: 0.0 Token: ., logprob: -14.202269, linearprob: 0.0 Token:  , logprob: -14.55061, linearprob: 0.0 Token:  polls, logprob: -3.4121115e-06, linearprob: 100.0 Token:  question, logprob: -12.805506, linearprob: 0.0 Token:  polling, logprob: -15.1435375, linearprob: 0.0 Token:  poll, logprob: -15.738583, linearprob: 0.0 Token:  , logprob: -17.67291, linearprob: 0.0 Token: ,, logprob: -0.009173453, linearprob: 99.09 Token: ., logprob: -4.699748, linearprob: 0.91 Token:  and, logprob: -11.2150135, linearprob: 0.0 Token:  as, logprob: -12.448206, linearprob: 0.0 Token: .\n, logprob: -12.522379, linearprob: 0.0 Token:  question, logprob: -0.058782633, linearprob: 94.29 Token:  and, logprob: -3.5626965, linearprob: 2.84 Token:  think, logprob: -3.6516247, linearprob: 2.59 Token:  conceptual, logprob: -6.2916775, linearprob: 0.19 Token:  questionnaire, logprob: -8.369112, linearprob: 0.02 Token: naires, logprob: -1.3186812e-05, linearprob: 100.0 Token: aires, logprob: -11.312759, linearprob: 0.0 Token: n, logprob: -14.500416, linearprob: 0.0 Token: aries, logprob: -15.486851, linearprob: 0.0 Token: na, logprob: -16.256975, linearprob: 0.0 Token: ,, logprob: -0.07033541, linearprob: 93.21 Token: ., logprob: -2.6946373, linearprob: 6.76 Token:  and, logprob: -8.432537, linearprob: 0.02 Token:  as, logprob: -10.095347, linearprob: 0.0 Token: .\n, logprob: -10.584616, linearprob: 0.0 Token:  think, logprob: -0.65735364, linearprob: 51.82 Token:  and, logprob: -0.79946256, linearprob: 44.96 Token:  conceptual, logprob: -3.6082566, linearprob: 2.71 Token:  work, logprob: -5.770276, linearprob: 0.31 Token:  shadow, logprob: -7.741763, linearprob: 0.04 Token: -al, logprob: -1.5451682e-05, linearprob: 100.0 Token: al, logprob: -11.423237, linearprob: 0.0 Token:  aloud, logprob: -12.9635105, linearprob: 0.0 Token: -, logprob: -13.55901, linearprob: 0.0 Token: ‐, logprob: -15.255007, linearprob: 0.0 Token: oud, logprob: -1.6240566e-06, linearprob: 100.0 Token: ound, logprob: -13.580944, linearprob: 0.0 Token: ould, logprob: -15.873268, linearprob: 0.0 Token: oad, logprob: -16.449224, linearprob: 0.0 Token: ong, logprob: -16.706142, linearprob: 0.0 Token:  sessions, logprob: -8.657073e-06, linearprob: 100.0 Token: sessions, logprob: -12.398932, linearprob: 0.0 Token: s, logprob: -13.074365, linearprob: 0.0 Token:  session, logprob: -13.178025, linearprob: 0.0 Token: ,, logprob: -15.680139, linearprob: 0.0 Token: ., logprob: -0.3437507, linearprob: 70.91 Token: ,, logprob: -1.2370098, linearprob: 29.03 Token: .\n, logprob: -8.439943, linearprob: 0.02 Token:  and, logprob: -8.940265, linearprob: 0.01 Token: <|end|>, logprob: -9.02417, linearprob: 0.01 ",Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set.
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"interviews,online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.050640512, linearprob: 95.06 Token: Classification, logprob: -3.1040254, linearprob: 4.49 Token: Class, logprob: -6.935963, linearprob: 0.1 Token:  The, logprob: -7.235747, linearprob: 0.07 Token: The, logprob: -7.3330026, linearprob: 0.07 Token:  set, logprob: -5.0497558e-05, linearprob: 99.99 Token:  data, logprob: -10.405009, linearprob: 0.0 Token:  research, logprob: -11.082389, linearprob: 0.0 Token:  classification, logprob: -13.074008, linearprob: 0.0 Token:  paper, logprob: -14.425435, linearprob: 0.0 Token:  of, logprob: -1.8624639e-06, linearprob: 100.0 Token:  for, logprob: -13.81325, linearprob: 0.0 Token:  this, logprob: -15.449001, linearprob: 0.0 Token:  containing, logprob: -16.016619, linearprob: 0.0 Token:  data, logprob: -16.123062, linearprob: 0.0 Token:  this, logprob: -2.7610455e-05, linearprob: 100.0 Token:  data, logprob: -10.991061, linearprob: 0.0 Token:  the, logprob: -12.395423, linearprob: 0.0 Token:  research, logprob: -12.442285, linearprob: 0.0 Token:  methodologies, logprob: -13.403354, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.306488, linearprob: 0.0 Token:  res, logprob: -17.146162, linearprob: 0.0 Token:  Research, logprob: -17.392778, linearprob: 0.0 Token:  , logprob: -17.816996, linearprob: 0.0 Token:  paper, logprob: -9.372295e-06, linearprob: 100.0 Token:  papers, logprob: -11.593475, linearprob: 0.0 Token:  pap, logprob: -16.318491, linearprob: 0.0 Token: 's, logprob: -17.400831, linearprob: 0.0 Token:  p, logprob: -17.461391, linearprob: 0.0 Token: 's, logprob: -1.3067608e-05, linearprob: 100.0 Token: ’s, logprob: -11.262432, linearprob: 0.0 Token: \', logprob: -16.74763, linearprob: 0.0 Token: ´s, logprob: -17.860771, linearprob: 0.0 Token: &#, logprob: -17.875055, linearprob: 0.0 Token:  data, logprob: -4.3202e-07, linearprob: 100.0 Token:  gathering, logprob: -16.171646, linearprob: 0.0 Token:  , logprob: -16.229439, linearprob: 0.0 Token:  Data, logprob: -16.361116, linearprob: 0.0 Token:  dat, logprob: -17.80143, linearprob: 0.0 Token:  gathering, logprob: -0.00032199582, linearprob: 99.97 Token: -g, logprob: -8.045286, linearprob: 0.03 Token:  g, logprob: -14.544613, linearprob: 0.0 Token:  gather, logprob: -14.952681, linearprob: 0.0 Token:  gathered, logprob: -15.804153, linearprob: 0.0 Token:  methodologies, logprob: -0.00011427152, linearprob: 99.99 Token:  methodology, logprob: -9.292154, linearprob: 0.01 Token:  methods, logprob: -11.080402, linearprob: 0.0 Token:  method, logprob: -12.590849, linearprob: 0.0 Token:  contains, logprob: -13.701868, linearprob: 0.0 Token:  contains, logprob: -0.00010616567, linearprob: 99.99 Token:  contain, logprob: -9.384683, linearprob: 0.01 Token:  includes, logprob: -11.279504, linearprob: 0.0 Token:  contained, logprob: -12.84719, linearprob: 0.0 Token:  may, logprob: -13.531983, linearprob: 0.0 Token:  interviews, logprob: -0.4949615, linearprob: 60.96 Token:  brainstorm, logprob: -0.98669505, linearprob: 37.28 Token:  focus, logprob: -4.5094824, linearprob: 1.1 Token:  online, logprob: -5.438838, linearprob: 0.43 Token:  question, logprob: -7.246028, linearprob: 0.07 Token: ,, logprob: -0.015006082, linearprob: 98.51 Token:  and, logprob: -4.2389708, linearprob: 1.44 Token: ., logprob: -7.6949205, linearprob: 0.05 Token:  [, logprob: -12.177218, linearprob: 0.0 Token:  for, logprob: -12.696837, linearprob: 0.0 Token:  online, logprob: -0.98533034, linearprob: 37.33 Token:  lab, logprob: -1.1412771, linearprob: 31.94 Token:  think, logprob: -1.556484, linearprob: 21.09 Token:  in, logprob: -3.0508478, linearprob: 4.73 Token:  question, logprob: -4.6074333, linearprob: 1.0 Token:  surveys, logprob: -1.0280384e-06, linearprob: 100.0 Token:  survey, logprob: -13.971471, linearprob: 0.0 Token:  studies, logprob: -16.931301, linearprob: 0.0 Token:  experiments, logprob: -17.153168, linearprob: 0.0 Token:  sur, logprob: -18.1758, linearprob: 0.0 Token:  and, logprob: -0.06064704, linearprob: 94.12 Token: ,, logprob: -2.8329554, linearprob: 5.88 Token: /p, logprob: -12.523711, linearprob: 0.0 Token: ,p, logprob: -14.459506, linearprob: 0.0 Token:  &, logprob: -14.950936, linearprob: 0.0 Token:  polls, logprob: -2.5941605e-05, linearprob: 100.0 Token:  question, logprob: -10.639865, linearprob: 0.0 Token:  polling, logprob: -13.906133, linearprob: 0.0 Token:  poll, logprob: -14.811931, linearprob: 0.0 Token:  focus, logprob: -15.117885, linearprob: 0.0 Token: ,, logprob: -0.0027240368, linearprob: 99.73 Token: ., logprob: -5.911507, linearprob: 0.27 Token:  and, logprob: -11.781572, linearprob: 0.0 Token: .\n, logprob: -13.919224, linearprob: 0.0 Token:  for, logprob: -14.041501, linearprob: 0.0 Token:  question, logprob: -0.84085816, linearprob: 43.13 Token:  think, logprob: -0.9091012, linearprob: 40.29 Token:  and, logprob: -1.9050329, linearprob: 14.88 Token:  conceptual, logprob: -4.556852, linearprob: 1.05 Token:  focus, logprob: -6.200311, linearprob: 0.2 Token: naires, logprob: -4.1273333e-06, linearprob: 100.0 Token: aires, logprob: -12.808464, linearprob: 0.0 Token: n, logprob: -13.943886, linearprob: 0.0 Token: aries, logprob: -15.367771, linearprob: 0.0 Token: na, logprob: -15.699368, linearprob: 0.0 Token: ,, logprob: -0.10663927, linearprob: 89.88 Token: ., logprob: -2.2938628, linearprob: 10.09 Token:  and, logprob: -8.779104, linearprob: 0.02 Token: .\n, logprob: -10.157017, linearprob: 0.0 Token: .\n\n, logprob: -11.285421, linearprob: 0.0 Token:  and, logprob: -0.6684469, linearprob: 51.25 Token:  think, logprob: -0.7505468, linearprob: 47.21 Token:  conceptual, logprob: -4.8967338, linearprob: 0.75 Token:  brainstorm, logprob: -6.0849757, linearprob: 0.23 Token:  focus, logprob: -6.2358737, linearprob: 0.2 Token:  think, logprob: -0.0018376068, linearprob: 99.82 Token:  focus, logprob: -7.52732, linearprob: 0.05 Token:  conceptual, logprob: -7.677949, linearprob: 0.05 Token:  shadow, logprob: -8.489665, linearprob: 0.02 Token:  document, logprob: -8.884732, linearprob: 0.01 Token: -al, logprob: -9.4914985e-06, linearprob: 100.0 Token: al, logprob: -12.016216, linearprob: 0.0 Token:  aloud, logprob: -13.181866, linearprob: 0.0 Token: -, logprob: -13.71101, linearprob: 0.0 Token: ‐, logprob: -15.820066, linearprob: 0.0 Token: oud, logprob: -1.8624639e-06, linearprob: 100.0 Token: ound, logprob: -13.381628, linearprob: 0.0 Token: ould, logprob: -15.686863, linearprob: 0.0 Token: ong, logprob: -16.079613, linearprob: 0.0 Token: oad, logprob: -16.578106, linearprob: 0.0 Token:  sessions, logprob: -7.465036e-06, linearprob: 100.0 Token: sessions, logprob: -12.135925, linearprob: 0.0 Token:  session, logprob: -13.837933, linearprob: 0.0 Token: s, logprob: -14.123365, linearprob: 0.0 Token:  sess, logprob: -16.541311, linearprob: 0.0 Token: ., logprob: -0.0008240377, linearprob: 99.92 Token:  as, logprob: -8.237139, linearprob: 0.03 Token: <|end|>, logprob: -8.798487, linearprob: 0.02 Token:  based, logprob: -9.244248, linearprob: 0.01 Token: ,, logprob: -9.456372, linearprob: 0.01 ","Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"brainstorming and focus groups,interviews,online surveys and polls,questionnaires,think-aloud sessions,,","Token: The, logprob: -0.013374922, linearprob: 98.67 Token: Classification, logprob: -4.4980206, linearprob: 1.11 Token:  The, logprob: -7.358006, linearprob: 0.06 Token: Class, logprob: -7.7474084, linearprob: 0.04 Token: Based, logprob: -8.33104, linearprob: 0.02 Token:  set, logprob: -6.098748e-05, linearprob: 99.99 Token:  data, logprob: -10.34297, linearprob: 0.0 Token:  research, logprob: -10.666056, linearprob: 0.0 Token:  classification, logprob: -12.922815, linearprob: 0.0 Token:  paper, logprob: -14.289242, linearprob: 0.0 Token:  of, logprob: -1.2664457e-06, linearprob: 100.0 Token:  for, logprob: -14.419368, linearprob: 0.0 Token:  containing, logprob: -15.791127, linearprob: 0.0 Token:  this, logprob: -16.002617, linearprob: 0.0 Token:  data, logprob: -16.325209, linearprob: 0.0 Token:  this, logprob: -3.392825e-05, linearprob: 100.0 Token:  data, logprob: -10.61297, linearprob: 0.0 Token:  the, logprob: -12.50115, linearprob: 0.0 Token:  research, logprob: -12.788424, linearprob: 0.0 Token:  methodologies, logprob: -13.209879, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.271643, linearprob: 0.0 Token:  res, logprob: -16.775684, linearprob: 0.0 Token:  Research, logprob: -17.191198, linearprob: 0.0 Token:  paper, logprob: -17.765217, linearprob: 0.0 Token:  paper, logprob: -1.867018e-05, linearprob: 100.0 Token:  papers, logprob: -10.911085, linearprob: 0.0 Token:  pap, logprob: -16.09885, linearprob: 0.0 Token: 's, logprob: -16.679008, linearprob: 0.0 Token:  p, logprob: -16.840654, linearprob: 0.0 Token: 's, logprob: -1.2590794e-05, linearprob: 100.0 Token: ’s, logprob: -11.299306, linearprob: 0.0 Token: \', logprob: -17.048256, linearprob: 0.0 Token: ´s, logprob: -17.89221, linearprob: 0.0 Token: ‘s, logprob: -17.952225, linearprob: 0.0 Token:  data, logprob: -3.1281633e-07, linearprob: 100.0 Token:  gathering, logprob: -16.279625, linearprob: 0.0 Token:  Data, logprob: -16.357887, linearprob: 0.0 Token:  , logprob: -16.564425, linearprob: 0.0 Token:  dat, logprob: -18.110432, linearprob: 0.0 Token:  gathering, logprob: -0.0003097215, linearprob: 99.97 Token: -g, logprob: -8.0841, linearprob: 0.03 Token:  g, logprob: -14.538892, linearprob: 0.0 Token:  gather, logprob: -14.866786, linearprob: 0.0 Token:  gathered, logprob: -15.768071, linearprob: 0.0 Token:  methodologies, logprob: -0.00011534435, linearprob: 99.99 Token:  methodology, logprob: -9.278274, linearprob: 0.01 Token:  methods, logprob: -11.018424, linearprob: 0.0 Token:  method, logprob: -12.726285, linearprob: 0.0 Token:  contains, logprob: -14.021491, linearprob: 0.0 Token:  contains, logprob: -9.0073176e-05, linearprob: 99.99 Token:  contain, logprob: -9.56975, linearprob: 0.01 Token:  includes, logprob: -11.412687, linearprob: 0.0 Token:  contained, logprob: -12.985474, linearprob: 0.0 Token:  may, logprob: -13.267221, linearprob: 0.0 Token:  brainstorm, logprob: -0.46662813, linearprob: 62.71 Token:  interviews, logprob: -1.0192709, linearprob: 36.09 Token:  focus, logprob: -4.8748083, linearprob: 0.76 Token:  online, logprob: -5.868827, linearprob: 0.28 Token: :, logprob: -7.518401, linearprob: 0.05 Token: ing, logprob: -4.1273333e-06, linearprob: 100.0 Token: ings, logprob: -13.301376, linearprob: 0.0 Token: in, logprob: -14.0353985, linearprob: 0.0 Token: ,, logprob: -14.451412, linearprob: 0.0 Token:  and, logprob: -15.179209, linearprob: 0.0 Token:  and, logprob: -0.25365573, linearprob: 77.6 Token: ,, logprob: -1.4974911, linearprob: 22.37 Token:  sessions, logprob: -8.815339, linearprob: 0.01 Token:  focus, logprob: -8.8727255, linearprob: 0.01 Token: /f, logprob: -10.221807, linearprob: 0.0 Token:  focus, logprob: -0.00013083349, linearprob: 99.99 Token:  interviews, logprob: -9.175612, linearprob: 0.01 Token:  online, logprob: -11.158809, linearprob: 0.0 Token:  Focus, logprob: -12.559436, linearprob: 0.0 Token:  focused, logprob: -13.170966, linearprob: 0.0 Token:  groups, logprob: -6.392203e-06, linearprob: 100.0 Token:  group, logprob: -11.989127, linearprob: 0.0 Token: groups, logprob: -17.16287, linearprob: 0.0 Token: -groups, logprob: -18.002958, linearprob: 0.0 Token:  , logprob: -18.111618, linearprob: 0.0 Token: ,, logprob: -0.00018792471, linearprob: 99.98 Token: ., logprob: -8.849257, linearprob: 0.01 Token:  interviews, logprob: -10.691699, linearprob: 0.0 Token:  and, logprob: -12.20687, linearprob: 0.0 Token:  as, logprob: -12.257186, linearprob: 0.0 Token:  interviews, logprob: -0.0007959277, linearprob: 99.92 Token:  online, logprob: -7.7036824, linearprob: 0.05 Token:  sem, logprob: -8.58335, linearprob: 0.02 Token:  in, logprob: -9.298261, linearprob: 0.01 Token:  think, logprob: -10.958476, linearprob: 0.0 Token: ,, logprob: -0.020739945, linearprob: 97.95 Token: ., logprob: -3.89356, linearprob: 2.04 Token:  and, logprob: -9.0765705, linearprob: 0.01 Token:  (, logprob: -11.883971, linearprob: 0.0 Token: <|end|>, logprob: -12.158218, linearprob: 0.0 Token:  online, logprob: -0.6041696, linearprob: 54.65 Token:  think, logprob: -1.6846006, linearprob: 18.55 Token:  and, logprob: -1.9172819, linearprob: 14.7 Token:  question, logprob: -2.698074, linearprob: 6.73 Token:  conceptual, logprob: -3.5557172, linearprob: 2.86 Token:  surveys, logprob: -6.704273e-07, linearprob: 100.0 Token:  survey, logprob: -14.766646, linearprob: 0.0 Token:  polls, logprob: -16.841572, linearprob: 0.0 Token:  sur, logprob: -17.380995, linearprob: 0.0 Token:  , logprob: -18.167187, linearprob: 0.0 Token:  and, logprob: -0.0052369563, linearprob: 99.48 Token: ,, logprob: -5.25522, linearprob: 0.52 Token: /p, logprob: -13.756797, linearprob: 0.0 Token:  , logprob: -14.694196, linearprob: 0.0 Token: ., logprob: -14.733632, linearprob: 0.0 Token:  polls, logprob: -1.7432603e-06, linearprob: 100.0 Token:  question, logprob: -13.546238, linearprob: 0.0 Token:  polling, logprob: -15.15509, linearprob: 0.0 Token:  poll, logprob: -15.809124, linearprob: 0.0 Token:  , logprob: -17.798868, linearprob: 0.0 Token: ,, logprob: -0.007830669, linearprob: 99.22 Token: ., logprob: -4.8573236, linearprob: 0.78 Token:  and, logprob: -11.201783, linearprob: 0.0 Token: .\n, logprob: -12.597551, linearprob: 0.0 Token: .\n\n, logprob: -12.879168, linearprob: 0.0 Token:  question, logprob: -0.055125423, linearprob: 94.64 Token:  and, logprob: -3.5099595, linearprob: 2.99 Token:  think, logprob: -3.82109, linearprob: 2.19 Token:  conceptual, logprob: -7.0496492, linearprob: 0.09 Token:  questionnaire, logprob: -8.5671015, linearprob: 0.02 Token: naires, logprob: -8.180258e-06, linearprob: 100.0 Token: aires, logprob: -11.814467, linearprob: 0.0 Token: n, logprob: -14.747247, linearprob: 0.0 Token: aries, logprob: -16.022831, linearprob: 0.0 Token: na, logprob: -16.558006, linearprob: 0.0 Token: ,, logprob: -0.04484176, linearprob: 95.61 Token: ., logprob: -3.1329079, linearprob: 4.36 Token:  and, logprob: -8.627592, linearprob: 0.02 Token:  as, logprob: -10.660354, linearprob: 0.0 Token: .\n, logprob: -11.109641, linearprob: 0.0 Token:  think, logprob: -0.59001905, linearprob: 55.43 Token:  and, logprob: -0.8647879, linearprob: 42.11 Token:  conceptual, logprob: -4.0364704, linearprob: 1.77 Token:  work, logprob: -5.3166637, linearprob: 0.49 Token:  shadow, logprob: -7.527487, linearprob: 0.05 Token: -al, logprob: -1.4855664e-05, linearprob: 100.0 Token: al, logprob: -11.463095, linearprob: 0.0 Token:  aloud, logprob: -12.98073, linearprob: 0.0 Token: -, logprob: -13.613253, linearprob: 0.0 Token: ‐, logprob: -15.210495, linearprob: 0.0 Token: oud, logprob: -2.9352968e-06, linearprob: 100.0 Token: ound, logprob: -12.910928, linearprob: 0.0 Token: ould, logprob: -15.50788, linearprob: 0.0 Token: oad, logprob: -16.404383, linearprob: 0.0 Token: ong, logprob: -16.460173, linearprob: 0.0 Token:  sessions, logprob: -6.9882217e-06, linearprob: 100.0 Token: sessions, logprob: -12.575992, linearprob: 0.0 Token: s, logprob: -13.332799, linearprob: 0.0 Token:  session, logprob: -13.412593, linearprob: 0.0 Token: ,, logprob: -15.805043, linearprob: 0.0 Token: ., logprob: -0.25228873, linearprob: 77.7 Token: ,, logprob: -1.5023669, linearprob: 22.26 Token: .\n, logprob: -8.788616, linearprob: 0.02 Token:  and, logprob: -9.713394, linearprob: 0.01 Token: <|end|>, logprob: -9.863453, linearprob: 0.01 ","Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best.  The set of this research paper's data gathering methodologies contains [MASK]. Be confident in your classification before adding a methodology to the set."
 882,1996,Pavlov: programming by stimulus-response demonstration,"brainstorming and focus groups,interviews,online surveys and polls,questionnaires,conceptual modeling,work diaries,think-aloud sessions,shadowing,participant-observer,instrumenting systems,fly on the wall,document corpus analysis,social media analysis,image,audio,and video library analysis,lab experiment,no user study,,","Token: The, logprob: -0.051216487, linearprob: 95.01 Token: Classification, logprob: -3.5569615, linearprob: 2.85 Token: Class, logprob: -3.9128137, linearprob: 2.0 Token: Based, logprob: -8.617626, linearprob: 0.02 Token: Answer, logprob: -8.662479, linearprob: 0.02 Token:  set, logprob: -2.4272753e-05, linearprob: 100.0 Token:  data, logprob: -11.245059, linearprob: 0.0 Token:  research, logprob: -11.81906, linearprob: 0.0 Token:  classification, logprob: -12.963109, linearprob: 0.0 Token:  provided, logprob: -14.590798, linearprob: 0.0 Token:  of, logprob: -1.2664457e-06, linearprob: 100.0 Token:  for, logprob: -14.601251, linearprob: 0.0 Token:  containing, logprob: -14.776156, linearprob: 0.0 Token:  data, logprob: -16.032293, linearprob: 0.0 Token:  this, logprob: -16.50112, linearprob: 0.0 Token:  this, logprob: -8.780831e-05, linearprob: 99.99 Token:  data, logprob: -9.458504, linearprob: 0.01 Token:  methodologies, logprob: -12.594366, linearprob: 0.0 Token:  research, logprob: -12.720661, linearprob: 0.0 Token:  the, logprob: -12.801834, linearprob: 0.0 Token:  research, logprob: -9.0883464e-07, linearprob: 100.0 Token:  re, logprob: -14.850537, linearprob: 0.0 Token:  res, logprob: -15.713841, linearprob: 0.0 Token:  paper, logprob: -16.626791, linearprob: 0.0 Token:  Research, logprob: -16.7049, linearprob: 0.0 Token:  paper, logprob: -1.8074163e-05, linearprob: 100.0 Token:  papers, logprob: -10.9923315, linearprob: 0.0 Token:  pap, logprob: -14.453231, linearprob: 0.0 Token:  p, logprob: -14.9676695, linearprob: 0.0 Token: 's, logprob: -16.547878, linearprob: 0.0 Token: 's, logprob: -5.9153886e-06, linearprob: 100.0 Token: ’s, logprob: -12.074612, linearprob: 0.0 Token: \', logprob: -16.788265, linearprob: 0.0 Token: &#, logprob: -17.686087, linearprob: 0.0 Token:  data, logprob: -18.201082, linearprob: 0.0 Token:  data, logprob: -7.89631e-07, linearprob: 100.0 Token:  gathering, logprob: -14.797288, linearprob: 0.0 Token:  , logprob: -15.995946, linearprob: 0.0 Token:  Data, logprob: -16.740147, linearprob: 0.0 Token:  dat, logprob: -17.124243, linearprob: 0.0 Token:  gathering, logprob: -0.00021736434, linearprob: 99.98 Token: -g, logprob: -8.443855, linearprob: 0.02 Token:  g, logprob: -13.750025, linearprob: 0.0 Token:  gather, logprob: -14.758297, linearprob: 0.0 Token:  gathered, logprob: -15.278482, linearprob: 0.0 Token:  methodologies, logprob: -7.827201e-05, linearprob: 99.99 Token:  methodology, logprob: -9.901758, linearprob: 0.01 Token:  methods, logprob: -10.754015, linearprob: 0.0 Token:  method, logprob: -12.549963, linearprob: 0.0 Token:  meth, logprob: -13.353823, linearprob: 0.0 Token:  contains, logprob: -0.00010950337, linearprob: 99.99 Token:  contain, logprob: -9.487599, linearprob: 0.01 Token:  includes, logprob: -10.728375, linearprob: 0.0 Token:  contained, logprob: -12.651738, linearprob: 0.0 Token:  consists, logprob: -13.104671, linearprob: 0.0 Token:  brainstorm, logprob: -0.016504433, linearprob: 98.36 Token:  [, logprob: -4.4907646, linearprob: 1.12 Token:  interviews, logprob: -5.6459846, linearprob: 0.35 Token:  online, logprob: -7.5805817, linearprob: 0.05 Token:  a, logprob: -8.257841, linearprob: 0.03 Token: ing, logprob: -4.9617593e-06, linearprob: 100.0 Token: ings, logprob: -13.604465, linearprob: 0.0 Token: in, logprob: -14.042193, linearprob: 0.0 Token: ,, logprob: -14.1169, linearprob: 0.0 Token:  and, logprob: -14.481153, linearprob: 0.0 Token:  and, logprob: -0.12225943, linearprob: 88.49 Token: ,, logprob: -2.1625464, linearprob: 11.5 Token:  sessions, logprob: -10.848673, linearprob: 0.0 Token:  focus, logprob: -10.849691, linearprob: 0.0 Token: /f, logprob: -12.564404, linearprob: 0.0 Token:  focus, logprob: -1.735894e-05, linearprob: 100.0 Token:  interviews, logprob: -11.572739, linearprob: 0.0 Token:  focused, logprob: -13.064381, linearprob: 0.0 Token:  Focus, logprob: -13.15233, linearprob: 0.0 Token:  concept, logprob: -13.805058, linearprob: 0.0 Token:  groups, logprob: -1.1041146e-05, linearprob: 100.0 Token:  group, logprob: -11.431425, linearprob: 0.0 Token: groups, logprob: -17.421066, linearprob: 0.0 Token:  , logprob: -17.51353, linearprob: 0.0 Token:  grou, logprob: -17.521166, linearprob: 0.0 Token: ,, logprob: -0.0001004439, linearprob: 99.99 Token: ., logprob: -9.500031, linearprob: 0.01 Token:  interviews, logprob: -10.927638, linearprob: 0.0 Token:  as, logprob: -12.516629, linearprob: 0.0 Token: ,int, logprob: -13.794772, linearprob: 0.0 Token:  interviews, logprob: -0.00057150197, linearprob: 99.94 Token:  conceptual, logprob: -8.2653475, linearprob: 0.03 Token:  online, logprob: -8.318849, linearprob: 0.02 Token:  participant, logprob: -10.770455, linearprob: 0.0 Token:  question, logprob: -11.028161, linearprob: 0.0 Token: ,, logprob: -0.00010819213, linearprob: 99.99 Token: ., logprob: -9.187377, linearprob: 0.01 Token:  and, logprob: -12.209041, linearprob: 0.0 Token:  online, logprob: -14.898438, linearprob: 0.0 Token:  [, logprob: -15.617172, linearprob: 0.0 Token:  online, logprob: -0.46248233, linearprob: 62.97 Token:  question, logprob: -1.2810134, linearprob: 27.78 Token:  conceptual, logprob: -2.7514172, linearprob: 6.38 Token:  and, logprob: -3.9758053, linearprob: 1.88 Token:  think, logprob: -5.570059, linearprob: 0.38 Token:  surveys, logprob: -7.58424e-06, linearprob: 100.0 Token:  survey, logprob: -13.034182, linearprob: 0.0 Token:  sur, logprob: -13.105444, linearprob: 0.0 Token:  polls, logprob: -13.655439, linearprob: 0.0 Token:  su, logprob: -14.327831, linearprob: 0.0 Token:  and, logprob: -0.0014013101, linearprob: 99.86 Token: ,, logprob: -6.5717597, linearprob: 0.14 Token:  , logprob: -15.403363, linearprob: 0.0 Token: /p, logprob: -15.868028, linearprob: 0.0 Token:  &, logprob: -16.276764, linearprob: 0.0 Token:  polls, logprob: -7.89631e-07, linearprob: 100.0 Token:  polling, logprob: -14.798669, linearprob: 0.0 Token: \n, logprob: -16.245045, linearprob: 0.0 Token:  , logprob: -16.502142, linearprob: 0.0 Token:  poll, logprob: -16.676655, linearprob: 0.0 Token: ,, logprob: -0.0005554168, linearprob: 99.94 Token: ., logprob: -7.498976, linearprob: 0.06 Token:  and, logprob: -14.846756, linearprob: 0.0 Token: .\n, logprob: -15.269297, linearprob: 0.0 Token: ,\n, logprob: -15.4781065, linearprob: 0.0 Token:  question, logprob: -0.009858319, linearprob: 99.02 Token:  conceptual, logprob: -4.8133345, linearprob: 0.81 Token:  think, logprob: -7.3604403, linearprob: 0.06 Token:  and, logprob: -7.390622, linearprob: 0.06 Token:  questionnaire, logprob: -8.949426, linearprob: 0.01 Token: naires, logprob: -9.014684e-06, linearprob: 100.0 Token: aires, logprob: -12.037503, linearprob: 0.0 Token: n, logprob: -13.03292, linearprob: 0.0 Token: aries, logprob: -15.072946, linearprob: 0.0 Token: ..., logprob: -15.666812, linearprob: 0.0 Token: ,, logprob: -0.00052503456, linearprob: 99.95 Token: ., logprob: -7.5626454, linearprob: 0.05 Token:  and, logprob: -12.433906, linearprob: 0.0 Token: ,\n, logprob: -15.281565, linearprob: 0.0 Token: .\n, logprob: -15.4478245, linearprob: 0.0 Token:  conceptual, logprob: -0.34463096, linearprob: 70.85 Token:  think, logprob: -1.3879468, linearprob: 24.96 Token:  and, logprob: -3.6394708, linearprob: 2.63 Token:  work, logprob: -4.5543156, linearprob: 1.05 Token:  shadow, logprob: -6.53298, linearprob: 0.15 Token:  modeling, logprob: -3.1305768e-05, linearprob: 100.0 Token:  modelling, logprob: -10.457124, linearprob: 0.0 Token:  model, logprob: -13.8288145, linearprob: 0.0 Token:  , logprob: -14.482179, linearprob: 0.0 Token:  [, logprob: -15.04296, linearprob: 0.0 Token: ,, logprob: -0.00054481137, linearprob: 99.95 Token: ., logprob: -7.527204, linearprob: 0.05 Token:  and, logprob: -12.176137, linearprob: 0.0 Token: .\n, logprob: -15.018177, linearprob: 0.0 Token: ,\n, logprob: -15.279061, linearprob: 0.0 Token:  work, logprob: -0.5555239, linearprob: 57.38 Token:  think, logprob: -0.90092564, linearprob: 40.62 Token:  and, logprob: -4.550807, linearprob: 1.06 Token:  document, logprob: -5.6069937, linearprob: 0.37 Token:  shadow, logprob: -5.896039, linearprob: 0.28 Token:  di, logprob: -1.3856493e-06, linearprob: 100.0 Token:  d, logprob: -14.552152, linearprob: 0.0 Token:  diary, logprob: -15.007806, linearprob: 0.0 Token: di, logprob: -15.071052, linearprob: 0.0 Token:  Di, logprob: -16.773476, linearprob: 0.0 Token: aries, logprob: -3.0545007e-06, linearprob: 100.0 Token: ..., logprob: -14.216033, linearprob: 0.0 Token: arie, logprob: -14.25969, linearprob: 0.0 Token: ar, logprob: -14.841203, linearprob: 0.0 Token: aris, logprob: -15.138093, linearprob: 0.0 Token: ,, logprob: -2.188868e-05, linearprob: 100.0 Token: ., logprob: -10.832152, linearprob: 0.0 Token:  and, logprob: -13.264459, linearprob: 0.0 Token: ,\n, logprob: -16.14502, linearprob: 0.0 Token: .\n, logprob: -17.402708, linearprob: 0.0 Token:  think, logprob: -0.0015922267, linearprob: 99.84 Token:  and, logprob: -7.1855235, linearprob: 0.08 Token:  shadow, logprob: -7.3277087, linearprob: 0.07 Token:  lab, logprob: -9.675722, linearprob: 0.01 Token:  fly, logprob: -10.399147, linearprob: 0.0 Token: -al, logprob: -1.504853e-06, linearprob: 100.0 Token:  aloud, logprob: -14.259758, linearprob: 0.0 Token: -, logprob: -14.919838, linearprob: 0.0 Token: al, logprob: -15.708488, linearprob: 0.0 Token: ‐, logprob: -16.593166, linearprob: 0.0 Token: oud, logprob: -1.2664457e-06, linearprob: 100.0 Token: ound, logprob: -13.865478, linearprob: 0.0 Token: ould, logprob: -16.06596, linearprob: 0.0 Token: oad, logprob: -16.830688, linearprob: 0.0 Token: ​, logprob: -16.998287, linearprob: 0.0 Token:  sessions, logprob: -2.577686e-06, linearprob: 100.0 Token: sessions, logprob: -13.622727, linearprob: 0.0 Token:  ses, logprob: -14.87567, linearprob: 0.0 Token:  sess, logprob: -15.046065, linearprob: 0.0 Token:  session, logprob: -15.369098, linearprob: 0.0 Token: ,, logprob: -0.008827403, linearprob: 99.12 Token: ., logprob: -4.738903, linearprob: 0.87 Token:  and, logprob: -10.358038, linearprob: 0.0 Token: .\n, logprob: -13.169101, linearprob: 0.0 Token: <|end|>, logprob: -13.178248, linearprob: 0.0 Token:  shadow, logprob: -0.048532177, linearprob: 95.26 Token:  participant, logprob: -3.7966325, linearprob: 2.24 Token:  and, logprob: -4.1444106, linearprob: 1.59 Token:  document, logprob: -5.2330203, linearprob: 0.53 Token:  instrument, logprob: -6.296512, linearprob: 0.18 Token: ing, logprob: -5.7961847e-06, linearprob: 100.0 Token: ings, logprob: -12.752186, linearprob: 0.0 Token: in, logprob: -13.477026, linearprob: 0.0 Token: ,, logprob: -14.918596, linearprob: 0.0 Token: i, logprob: -15.421984, linearprob: 0.0 Token: ,, logprob: -0.00011582117, linearprob: 99.99 Token: ., logprob: -9.205056, linearprob: 0.01 Token:  participant, logprob: -11.864165, linearprob: 0.0 Token:  and, logprob: -11.955482, linearprob: 0.0 Token:  participants, logprob: -14.303364, linearprob: 0.0 Token:  participant, logprob: -0.01675908, linearprob: 98.34 Token:  and, logprob: -4.572816, linearprob: 1.03 Token:  document, logprob: -5.43674, linearprob: 0.44 Token:  instrument, logprob: -7.016923, linearprob: 0.09 Token:  lab, logprob: -7.1578283, linearprob: 0.08 Token: -, logprob: -0.00014382669, linearprob: 99.99 Token:  observer, logprob: -9.023034, linearprob: 0.01 Token:  observation, logprob: -10.867969, linearprob: 0.0 Token: -ob, logprob: -13.159516, linearprob: 0.0 Token: observer, logprob: -14.528304, linearprob: 0.0 Token: observer, logprob: -0.0005485067, linearprob: 99.95 Token: observation, logprob: -7.602771, linearprob: 0.05 Token: obs, logprob: -11.063615, linearprob: 0.0 Token: observ, logprob: -11.266561, linearprob: 0.0 Token:  observer, logprob: -11.617969, linearprob: 0.0 Token: ,, logprob: -0.0012978541, linearprob: 99.87 Token: ., logprob: -6.8171263, linearprob: 0.11 Token:  observations, logprob: -9.393563, linearprob: 0.01 Token:  methods, logprob: -10.80829, linearprob: 0.0 Token:  and, logprob: -10.980433, linearprob: 0.0 Token:  instrument, logprob: -0.052623075, linearprob: 94.87 Token:  and, logprob: -3.16982, linearprob: 4.2 Token:  document, logprob: -5.076362, linearprob: 0.62 Token:  lab, logprob: -6.0090404, linearprob: 0.25 Token:  fly, logprob: -8.166741, linearprob: 0.03 Token: ing, logprob: -1.2590794e-05, linearprob: 100.0 Token:  systems, logprob: -11.6441965, linearprob: 0.0 Token: ed, logprob: -13.212222, linearprob: 0.0 Token:  system, logprob: -14.441691, linearprob: 0.0 Token: ating, logprob: -14.689382, linearprob: 0.0 Token:  systems, logprob: -1.6286107e-05, linearprob: 100.0 Token:  system, logprob: -11.209189, linearprob: 0.0 Token:  the, logprob: -14.245931, linearprob: 0.0 Token:  of, logprob: -14.673186, linearprob: 0.0 Token:  [, logprob: -14.849797, linearprob: 0.0 Token: ,, logprob: -0.001327976, linearprob: 99.87 Token: ., logprob: -6.637463, linearprob: 0.13 Token:  and, logprob: -11.2796955, linearprob: 0.0 Token: .\n, logprob: -14.005241, linearprob: 0.0 Token:  as, logprob: -14.678368, linearprob: 0.0 Token:  fly, logprob: -0.4475257, linearprob: 63.92 Token:  lab, logprob: -1.862889, linearprob: 15.52 Token:  and, logprob: -2.168946, linearprob: 11.43 Token:  document, logprob: -2.4356372, linearprob: 8.75 Token:  social, logprob: -6.574599, linearprob: 0.14 Token:  on, logprob: -0.0009925569, linearprob: 99.9 Token: -on, logprob: -6.919873, linearprob: 0.1 Token:  the, logprob: -13.6393175, linearprob: 0.0 Token:  in, logprob: -13.999545, linearprob: 0.0 Token: ,, logprob: -15.038777, linearprob: 0.0 Token:  the, logprob: -1.7478143e-05, linearprob: 100.0 Token: -the, logprob: -11.229195, linearprob: 0.0 Token:  wall, logprob: -14.127087, linearprob: 0.0 Token:  a, logprob: -14.258293, linearprob: 0.0 Token: \n, logprob: -14.352648, linearprob: 0.0 Token:  wall, logprob: -6.392203e-06, linearprob: 100.0 Token:  all, logprob: -13.420457, linearprob: 0.0 Token:  wal, logprob: -13.776509, linearprob: 0.0 Token:  walls, logprob: -13.791724, linearprob: 0.0 Token:  ball, logprob: -14.328298, linearprob: 0.0 Token: ,, logprob: -7.910644e-05, linearprob: 99.99 Token: ., logprob: -9.753211, linearprob: 0.01 Token:  observations, logprob: -11.894063, linearprob: 0.0 Token:  observation, logprob: -12.610612, linearprob: 0.0 Token:  and, logprob: -12.742007, linearprob: 0.0 Token:  document, logprob: -0.013434555, linearprob: 98.67 Token:  and, logprob: -4.397483, linearprob: 1.23 Token:  lab, logprob: -7.260419, linearprob: 0.07 Token:  social, logprob: -8.10031, linearprob: 0.03 Token:  documentary, logprob: -11.396274, linearprob: 0.0 Token:  corpus, logprob: -3.650519e-06, linearprob: 100.0 Token:  corps, logprob: -13.519344, linearprob: 0.0 Token:  corp, logprob: -13.805029, linearprob: 0.0 Token:  corpor, logprob: -14.786707, linearprob: 0.0 Token:  Corpus, logprob: -15.305094, linearprob: 0.0 Token:  analysis, logprob: -1.147242e-06, linearprob: 100.0 Token: analysis, logprob: -15.488748, linearprob: 0.0 Token:  an, logprob: -15.754281, linearprob: 0.0 Token: ,, logprob: -15.863263, linearprob: 0.0 Token:  analys, logprob: -16.19596, linearprob: 0.0 Token: ,, logprob: -0.00090693746, linearprob: 99.91 Token: ., logprob: -7.0220327, linearprob: 0.09 Token:  and, logprob: -11.327443, linearprob: 0.0 Token: .\n, logprob: -14.087648, linearprob: 0.0 Token: .\n\n, logprob: -15.108224, linearprob: 0.0 Token:  social, logprob: -0.066315696, linearprob: 93.58 Token:  lab, logprob: -3.3214865, linearprob: 3.61 Token:  and, logprob: -3.7153673, linearprob: 2.43 Token:  image, logprob: -5.641095, linearprob: 0.35 Token:  as, logprob: -9.827151, linearprob: 0.01 Token:  media, logprob: -1.504853e-06, linearprob: 100.0 Token: -media, logprob: -14.882437, linearprob: 0.0 Token:  , logprob: -15.955627, linearprob: 0.0 Token:  Media, logprob: -16.082634, linearprob: 0.0 Token: media, logprob: -16.1584, linearprob: 0.0 Token:  analysis, logprob: -5.5122365e-07, linearprob: 100.0 Token: ,, logprob: -15.789411, linearprob: 0.0 Token:  an, logprob: -16.319656, linearprob: 0.0 Token:  and, logprob: -16.693842, linearprob: 0.0 Token:  analy, logprob: -16.99598, linearprob: 0.0 Token: ,, logprob: -6.0033854e-05, linearprob: 99.99 Token: ., logprob: -9.73688, linearprob: 0.01 Token:  and, logprob: -14.599938, linearprob: 0.0 Token: .\n, logprob: -16.465172, linearprob: 0.0 Token: ,\n, logprob: -16.499329, linearprob: 0.0 Token:  image, logprob: -0.0025818434, linearprob: 99.74 Token:  lab, logprob: -6.5613875, linearprob: 0.14 Token:  and, logprob: -6.7760196, linearprob: 0.11 Token:  as, logprob: -11.033299, linearprob: 0.0 Token:  video, logprob: -12.938508, linearprob: 0.0 Token: ,, logprob: -0.006262459, linearprob: 99.38 Token:  analysis, logprob: -5.3023305, linearprob: 0.5 Token:  and, logprob: -7.080525, linearprob: 0.08 Token: /audio, logprob: -8.462707, linearprob: 0.02 Token:  audio, logprob: -8.599099, linearprob: 0.02 Token:  audio, logprob: -3.4285862e-05, linearprob: 100.0 Token:  and, logprob: -10.670992, linearprob: 0.0 Token:  lab, logprob: -12.074215, linearprob: 0.0 Token:  video, logprob: -12.715811, linearprob: 0.0 Token:  Audio, logprob: -14.625095, linearprob: 0.0 Token: ,, logprob: -0.00033975352, linearprob: 99.97 Token:  and, logprob: -7.9935956, linearprob: 0.03 Token: ,and, logprob: -13.649223, linearprob: 0.0 Token: ,\n, logprob: -14.747219, linearprob: 0.0 Token:  ,, logprob: -15.891365, linearprob: 0.0 Token:  and, logprob: -5.5146502e-05, linearprob: 99.99 Token:  video, logprob: -9.836849, linearprob: 0.01 Token:  lab, logprob: -14.706081, linearprob: 0.0 Token:  , logprob: -15.105581, linearprob: 0.0 Token:  \n, logprob: -15.909019, linearprob: 0.0 Token:  video, logprob: -1.7432603e-06, linearprob: 100.0 Token:  vide, logprob: -14.355801, linearprob: 0.0 Token:  Video, logprob: -15.096872, linearprob: 0.0 Token:  a, logprob: -15.51178, linearprob: 0.0 Token:  videos, logprob: -15.965935, linearprob: 0.0 Token:  library, logprob: -1.0087517e-05, linearprob: 100.0 Token:  analysis, logprob: -12.439666, linearprob: 0.0 Token:  Library, logprob: -12.700795, linearprob: 0.0 Token: ,, logprob: -14.602826, linearprob: 0.0 Token:  , logprob: -14.865985, linearprob: 0.0 Token:  analysis, logprob: -1.2664457e-06, linearprob: 100.0 Token: analysis, logprob: -15.090526, linearprob: 0.0 Token: ,, logprob: -15.711692, linearprob: 0.0 Token:  analyses, logprob: -16.273249, linearprob: 0.0 Token:  an, logprob: -16.301447, linearprob: 0.0 Token: ,, logprob: -0.005727913, linearprob: 99.43 Token: ., logprob: -5.1744556, linearprob: 0.57 Token:  and, logprob: -10.200701, linearprob: 0.0 Token:  lab, logprob: -12.17313, linearprob: 0.0 Token: .\n, logprob: -12.60276, linearprob: 0.0 Token:  lab, logprob: -0.21129543, linearprob: 80.95 Token:  and, logprob: -1.6586335, linearprob: 19.04 Token:  as, logprob: -9.998366, linearprob: 0.0 Token:  laboratory, logprob: -11.431165, linearprob: 0.0 Token:  Lab, logprob: -12.685943, linearprob: 0.0 Token:  experiment, logprob: -0.0013936884, linearprob: 99.86 Token:  experiments, logprob: -6.587066, linearprob: 0.14 Token:  experimentation, logprob: -11.646178, linearprob: 0.0 Token:  exper, logprob: -13.220836, linearprob: 0.0 Token:  experi, logprob: -13.743754, linearprob: 0.0 Token: ,, logprob: -0.5232979, linearprob: 59.26 Token: ., logprob: -0.8992363, linearprob: 40.69 Token:  and, logprob: -8.500693, linearprob: 0.02 Token: .\n, logprob: -8.958695, linearprob: 0.01 Token: <|end|>, logprob: -10.084382, linearprob: 0.0 Token:  and, logprob: -0.25030333, linearprob: 77.86 Token:  no, logprob: -1.5103683, linearprob: 22.08 Token:  think, logprob: -8.50745, linearprob: 0.02 Token:  participant, logprob: -8.688038, linearprob: 0.02 Token:  user, logprob: -8.92551, linearprob: 0.01 Token:  no, logprob: -0.05243587, linearprob: 94.89 Token:  user, logprob: -3.3779917, linearprob: 3.41 Token:  participant, logprob: -4.3693495, linearprob: 1.27 Token:  observational, logprob: -7.304446, linearprob: 0.07 Token:  document, logprob: -7.5334616, linearprob: 0.05 Token:  user, logprob: -1.8431772e-05, linearprob: 100.0 Token: -user, logprob: -11.812102, linearprob: 0.0 Token:  [, logprob: -13.011905, linearprob: 0.0 Token:  users, logprob: -13.104667, linearprob: 0.0 Token:  use, logprob: -13.4137335, linearprob: 0.0 Token:  study, logprob: -4.894791e-05, linearprob: 100.0 Token:  studies, logprob: -10.039359, linearprob: 0.0 Token:  stud, logprob: -13.358899, linearprob: 0.0 Token: study, logprob: -14.251067, linearprob: 0.0 Token:  Study, logprob: -14.738809, linearprob: 0.0 Token: ., logprob: -0.00018411019, linearprob: 99.98 Token: .\n, logprob: -9.678374, linearprob: 0.01 Token: <|end|>, logprob: -9.896651, linearprob: 0.01 Token:  methods, logprob: -10.822241, linearprob: 0.0 Token:  methodologies, logprob: -11.14364, linearprob: 0.0 ","Classification: Data source and data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method into any applicable classes in the data gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that of the research paper. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,"brainstorming and focus groups,interviews,questionnaires,conceptual modeling,think-aloud sessions,document corpus analysis,lab experiment,,","Token: The, logprob: -0.017736036, linearprob: 98.24 Token: Class, logprob: -4.770614, linearprob: 0.85 Token: Classification, logprob: -4.911161, linearprob: 0.74 Token: Set, logprob: -7.703468, linearprob: 0.05 Token: Based, logprob: -7.9246783, linearprob: 0.04 Token:  set, logprob: -5.4312077e-05, linearprob: 99.99 Token:  research, logprob: -10.393832, linearprob: 0.0 Token:  data, logprob: -11.269252, linearprob: 0.0 Token:  classification, logprob: -12.1074705, linearprob: 0.0 Token:  provided, logprob: -12.800967, linearprob: 0.0 Token:  of, logprob: -1.2664457e-06, linearprob: 100.0 Token:  for, logprob: -14.1414385, linearprob: 0.0 Token:  containing, logprob: -14.889314, linearprob: 0.0 Token:  this, logprob: -16.484713, linearprob: 0.0 Token:  , logprob: -16.578651, linearprob: 0.0 Token:  this, logprob: -5.3000836e-05, linearprob: 99.99 Token:  data, logprob: -10.10405, linearprob: 0.0 Token:  research, logprob: -12.236069, linearprob: 0.0 Token:  methodologies, logprob: -12.612516, linearprob: 0.0 Token:  the, logprob: -12.624596, linearprob: 0.0 Token:  research, logprob: -9.0883464e-07, linearprob: 100.0 Token:  re, logprob: -14.948503, linearprob: 0.0 Token:  res, logprob: -15.617234, linearprob: 0.0 Token:  Research, logprob: -16.386488, linearprob: 0.0 Token:  paper, logprob: -16.500086, linearprob: 0.0 Token:  paper, logprob: -1.247159e-05, linearprob: 100.0 Token:  papers, logprob: -11.366248, linearprob: 0.0 Token:  pap, logprob: -14.9637575, linearprob: 0.0 Token:  p, logprob: -15.369515, linearprob: 0.0 Token: 's, logprob: -16.201084, linearprob: 0.0 Token: 's, logprob: -8.061054e-06, linearprob: 100.0 Token: ’s, logprob: -11.75457, linearprob: 0.0 Token: \', logprob: -16.316713, linearprob: 0.0 Token: &#, logprob: -17.378155, linearprob: 0.0 Token: ´s, logprob: -17.539787, linearprob: 0.0 Token:  data, logprob: -5.5122365e-07, linearprob: 100.0 Token:  gathering, logprob: -15.282868, linearprob: 0.0 Token:  Data, logprob: -15.748036, linearprob: 0.0 Token:  , logprob: -16.15856, linearprob: 0.0 Token:  dat, logprob: -17.543928, linearprob: 0.0 Token:  gathering, logprob: -0.000118324446, linearprob: 99.99 Token: -g, logprob: -9.059834, linearprob: 0.01 Token:  g, logprob: -13.856287, linearprob: 0.0 Token:  gather, logprob: -14.618799, linearprob: 0.0 Token:  gathered, logprob: -15.936537, linearprob: 0.0 Token:  methodologies, logprob: -0.00015765065, linearprob: 99.98 Token:  methodology, logprob: -9.037635, linearprob: 0.01 Token:  methods, logprob: -10.551978, linearprob: 0.0 Token:  method, logprob: -11.53085, linearprob: 0.0 Token:  meth, logprob: -13.519704, linearprob: 0.0 Token:  contains, logprob: -0.00013238314, linearprob: 99.99 Token:  contain, logprob: -9.084706, linearprob: 0.01 Token:  includes, logprob: -11.5157385, linearprob: 0.0 Token:  contained, logprob: -13.5225115, linearprob: 0.0 Token:  may, logprob: -13.830129, linearprob: 0.0 Token:  brainstorm, logprob: -0.13064316, linearprob: 87.75 Token:  interviews, logprob: -2.2480786, linearprob: 10.56 Token:  [, logprob: -4.699569, linearprob: 0.91 Token:  online, logprob: -5.5845065, linearprob: 0.38 Token:  focus, logprob: -6.3314624, linearprob: 0.18 Token: ing, logprob: -6.1537958e-06, linearprob: 100.0 Token: ings, logprob: -13.306385, linearprob: 0.0 Token: in, logprob: -13.711597, linearprob: 0.0 Token: ,, logprob: -14.157507, linearprob: 0.0 Token:  and, logprob: -14.588186, linearprob: 0.0 Token:  and, logprob: -0.10548008, linearprob: 89.99 Token: ,, logprob: -2.3021736, linearprob: 10.0 Token:  focus, logprob: -10.32427, linearprob: 0.0 Token:  sessions, logprob: -10.831366, linearprob: 0.0 Token: /f, logprob: -11.807373, linearprob: 0.0 Token:  focus, logprob: -0.0005071613, linearprob: 99.95 Token:  interviews, logprob: -8.265097, linearprob: 0.03 Token:  conceptual, logprob: -8.560659, linearprob: 0.02 Token:  concept, logprob: -10.438749, linearprob: 0.0 Token:  online, logprob: -11.751665, linearprob: 0.0 Token:  groups, logprob: -2.9636918e-05, linearprob: 100.0 Token:  group, logprob: -10.43253, linearprob: 0.0 Token:  grou, logprob: -17.159195, linearprob: 0.0 Token: groups, logprob: -17.293276, linearprob: 0.0 Token:  , logprob: -17.625547, linearprob: 0.0 Token: ,, logprob: -0.00023083435, linearprob: 99.98 Token: ., logprob: -8.504501, linearprob: 0.02 Token:  interviews, logprob: -11.038296, linearprob: 0.0 Token:  as, logprob: -12.106993, linearprob: 0.0 Token:  and, logprob: -12.857981, linearprob: 0.0 Token:  interviews, logprob: -0.0017447966, linearprob: 99.83 Token:  conceptual, logprob: -6.781553, linearprob: 0.11 Token:  online, logprob: -7.7073264, linearprob: 0.04 Token:  question, logprob: -9.340361, linearprob: 0.01 Token:  participant, logprob: -10.932617, linearprob: 0.0 Token: ,, logprob: -0.0010731808, linearprob: 99.89 Token: ., logprob: -6.855951, linearprob: 0.11 Token:  and, logprob: -10.978759, linearprob: 0.0 Token: .\n, logprob: -14.540836, linearprob: 0.0 Token: .\n\n, logprob: -15.154377, linearprob: 0.0 Token:  question, logprob: -0.63592213, linearprob: 52.94 Token:  online, logprob: -1.0289581, linearprob: 35.74 Token:  conceptual, logprob: -2.594813, linearprob: 7.47 Token:  and, logprob: -3.5320995, linearprob: 2.92 Token:  think, logprob: -5.8548055, linearprob: 0.29 Token: naires, logprob: -1.4259645e-05, linearprob: 100.0 Token: aires, logprob: -11.639043, linearprob: 0.0 Token: n, logprob: -12.602506, linearprob: 0.0 Token: nn, logprob: -14.303647, linearprob: 0.0 Token: aries, logprob: -14.457229, linearprob: 0.0 Token: ,, logprob: -0.0011161657, linearprob: 99.89 Token: ., logprob: -6.823376, linearprob: 0.11 Token:  and, logprob: -10.58769, linearprob: 0.0 Token: .\n, logprob: -14.32919, linearprob: 0.0 Token: .\n\n, logprob: -15.2329445, linearprob: 0.0 Token:  conceptual, logprob: -0.7053529, linearprob: 49.39 Token:  think, logprob: -0.8811761, linearprob: 41.43 Token:  work, logprob: -3.5159893, linearprob: 2.97 Token:  and, logprob: -3.7626057, linearprob: 2.32 Token:  document, logprob: -4.0986347, linearprob: 1.66 Token:  modeling, logprob: -4.9067115e-05, linearprob: 100.0 Token:  modelling, logprob: -10.004855, linearprob: 0.0 Token:  model, logprob: -13.058616, linearprob: 0.0 Token:  , logprob: -14.254863, linearprob: 0.0 Token:  mapping, logprob: -15.6447, linearprob: 0.0 Token: ,, logprob: -0.020981498, linearprob: 97.92 Token: ., logprob: -3.8806827, linearprob: 2.06 Token:  and, logprob: -9.11093, linearprob: 0.01 Token: .\n, logprob: -11.874591, linearprob: 0.0 Token: .\n\n, logprob: -12.975914, linearprob: 0.0 Token:  think, logprob: -0.23878202, linearprob: 78.76 Token:  and, logprob: -2.307557, linearprob: 9.95 Token:  document, logprob: -3.1998148, linearprob: 4.08 Token:  work, logprob: -3.4681044, linearprob: 3.12 Token:  instrument, logprob: -4.062398, linearprob: 1.72 Token: -al, logprob: -3.1737043e-06, linearprob: 100.0 Token:  aloud, logprob: -13.349996, linearprob: 0.0 Token: -, logprob: -13.947995, linearprob: 0.0 Token: al, logprob: -15.325564, linearprob: 0.0 Token: -out, logprob: -15.739604, linearprob: 0.0 Token: oud, logprob: -4.1273333e-06, linearprob: 100.0 Token: ound, logprob: -12.562746, linearprob: 0.0 Token: ould, logprob: -15.308809, linearprob: 0.0 Token: ong, logprob: -15.734972, linearprob: 0.0 Token: oad, logprob: -16.734785, linearprob: 0.0 Token:  sessions, logprob: -3.2929079e-06, linearprob: 100.0 Token:  session, logprob: -13.70073, linearprob: 0.0 Token: sessions, logprob: -13.772271, linearprob: 0.0 Token:  ses, logprob: -14.756461, linearprob: 0.0 Token: s, logprob: -15.272634, linearprob: 0.0 Token: ,, logprob: -0.1301383, linearprob: 87.8 Token: ., logprob: -2.1049309, linearprob: 12.19 Token:  and, logprob: -8.954199, linearprob: 0.01 Token: .\n, logprob: -10.778874, linearprob: 0.0 Token: .\n\n, logprob: -11.744767, linearprob: 0.0 Token:  document, logprob: -1.2305082, linearprob: 29.21 Token:  lab, logprob: -1.3425382, linearprob: 26.12 Token:  and, logprob: -1.941255, linearprob: 14.35 Token:  participant, logprob: -2.069099, linearprob: 12.63 Token:  shadow, logprob: -2.3927894, linearprob: 9.14 Token:  corpus, logprob: -4.1273333e-06, linearprob: 100.0 Token:  analysis, logprob: -13.36562, linearprob: 0.0 Token:  corps, logprob: -13.965145, linearprob: 0.0 Token:  corp, logprob: -14.1199, linearprob: 0.0 Token:  corpor, logprob: -14.824181, linearprob: 0.0 Token:  analysis, logprob: -1.147242e-06, linearprob: 100.0 Token:  and, logprob: -15.05931, linearprob: 0.0 Token:  an, logprob: -15.470235, linearprob: 0.0 Token: ,, logprob: -15.511939, linearprob: 0.0 Token:  analy, logprob: -16.343262, linearprob: 0.0 Token: ,, logprob: -0.6872081, linearprob: 50.3 Token: ., logprob: -0.69969743, linearprob: 49.67 Token:  and, logprob: -8.761946, linearprob: 0.02 Token: .\n, logprob: -9.399371, linearprob: 0.01 Token: .\n\n, logprob: -10.964312, linearprob: 0.0 Token:  lab, logprob: -0.594098, linearprob: 55.21 Token:  and, logprob: -0.83555305, linearprob: 43.36 Token:  image, logprob: -4.905347, linearprob: 0.74 Token:  social, logprob: -5.6790476, linearprob: 0.34 Token:  instrument, logprob: -6.5047827, linearprob: 0.15 Token:  experiment, logprob: -0.0075279553, linearprob: 99.25 Token:  experiments, logprob: -4.8939567, linearprob: 0.75 Token:  experimentation, logprob: -11.825666, linearprob: 0.0 Token:  exper, logprob: -15.446665, linearprob: 0.0 Token:  experimental, logprob: -15.7146435, linearprob: 0.0 Token: ., logprob: -0.019646436, linearprob: 98.05 Token: ,, logprob: -3.9548385, linearprob: 1.92 Token: .\n, logprob: -8.790264, linearprob: 0.02 Token: <|end|>, logprob: -10.175947, linearprob: 0.0 Token: .\n\n, logprob: -10.472974, linearprob: 0.0 ","Classification: Data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method(s) into any applicable classes in the data methodology gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,"interviews,questionnaires,conceptual modeling,think-aloud sessions,,","Token: Classification, logprob: -0.112480514, linearprob: 89.36 Token: The, logprob: -2.2733233, linearprob: 10.3 Token: Class, logprob: -5.920117, linearprob: 0.27 Token:  Classification, logprob: -8.517706, linearprob: 0.02 Token: Based, logprob: -8.919647, linearprob: 0.01 Token: :, logprob: -0.02653935, linearprob: 97.38 Token: :\n, logprob: -3.6788316, linearprob: 2.53 Token: :\n\n, logprob: -7.7609677, linearprob: 0.04 Token:  result, logprob: -8.131145, linearprob: 0.03 Token:  Result, logprob: -9.569339, linearprob: 0.01 Token:  Data, logprob: -0.0113648055, linearprob: 98.87 Token:  \n, logprob: -5.207309, linearprob: 0.55 Token:  \n\n, logprob: -5.7344503, linearprob: 0.32 Token:   \n, logprob: -6.3364077, linearprob: 0.18 Token:  The, logprob: -7.4925623, linearprob: 0.06 Token:  gathering, logprob: -7.421909e-05, linearprob: 99.99 Token:  Gathering, logprob: -9.666159, linearprob: 0.01 Token:  methodology, logprob: -12.199108, linearprob: 0.0 Token: -g, logprob: -12.926037, linearprob: 0.0 Token:  gather, logprob: -13.921766, linearprob: 0.0 Token:  methodology, logprob: -0.0009680376, linearprob: 99.9 Token:  methodologies, logprob: -7.1349154, linearprob: 0.08 Token:  method, logprob: -8.891086, linearprob: 0.01 Token:  Method, logprob: -11.132531, linearprob: 0.0 Token:  methods, logprob: -11.86296, linearprob: 0.0 Token:  -, logprob: -0.008415406, linearprob: 99.16 Token: \n\n, logprob: -5.6617756, linearprob: 0.35 Token: \n, logprob: -5.8990307, linearprob: 0.27 Token:  \n\n, logprob: -7.1265316, linearprob: 0.08 Token:  \n, logprob: -7.9459324, linearprob: 0.04 Token:  You, logprob: -0.11909746, linearprob: 88.77 Token:  The, logprob: -2.3420322, linearprob: 9.61 Token:  \n, logprob: -4.6491704, linearprob: 0.96 Token:  , logprob: -5.8504663, linearprob: 0.29 Token:  \n\n, logprob: -5.9772305, linearprob: 0.25 Token:  are, logprob: -1.378283e-05, linearprob: 100.0 Token: 're, logprob: -12.667917, linearprob: 0.0 Token:  can, logprob: -13.83009, linearprob: 0.0 Token:  , logprob: -13.87422, linearprob: 0.0 Token:  have, logprob: -13.928961, linearprob: 0.0 Token:  to, logprob: -1.8431772e-05, linearprob: 100.0 Token:  performing, logprob: -12.7174425, linearprob: 0.0 Token:  asked, logprob: -12.830533, linearprob: 0.0 Token:  perform, logprob: -12.862411, linearprob: 0.0 Token:  tasked, logprob: -13.353948, linearprob: 0.0 Token:  perform, logprob: -8.11329e-05, linearprob: 99.99 Token:  classify, logprob: -9.961497, linearprob: 0.0 Token:  pe, logprob: -11.85794, linearprob: 0.0 Token:  per, logprob: -12.252719, linearprob: 0.0 Token:  perf, logprob: -12.602614, linearprob: 0.0 Token:  a, logprob: -0.00014132341, linearprob: 99.99 Token:  mult, logprob: -8.943878, linearprob: 0.01 Token:  am, logprob: -12.460686, linearprob: 0.0 Token:  multi, logprob: -12.660767, linearprob: 0.0 Token:  the, logprob: -14.234024, linearprob: 0.0 Token:  mult, logprob: -0.001805011, linearprob: 99.82 Token:  multi, logprob: -6.367619, linearprob: 0.17 Token:  classification, logprob: -9.954288, linearprob: 0.0 Token:  multic, logprob: -10.794032, linearprob: 0.0 Token:  mul, logprob: -11.368482, linearprob: 0.0 Token: iclass, logprob: -4.3202e-07, linearprob: 100.0 Token: il, logprob: -15.320091, linearprob: 0.0 Token: ical, logprob: -16.723951, linearprob: 0.0 Token: aclass, logprob: -16.977558, linearprob: 0.0 Token: class, logprob: -17.176498, linearprob: 0.0 Token:  classification, logprob: -1.1637165e-05, linearprob: 100.0 Token:  class, logprob: -12.038921, linearprob: 0.0 Token: ification, logprob: -13.217508, linearprob: 0.0 Token:  classic, logprob: -13.317846, linearprob: 0.0 Token:  Classification, logprob: -14.668177, linearprob: 0.0 Token:  of, logprob: -3.8934802e-05, linearprob: 100.0 Token: ., logprob: -11.319631, linearprob: 0.0 Token:  on, logprob: -11.746715, linearprob: 0.0 Token:  for, logprob: -12.654651, linearprob: 0.0 Token:  the, logprob: -13.160741, linearprob: 0.0 Token:  the, logprob: -6.9882217e-06, linearprob: 100.0 Token:  data, logprob: -12.707976, linearprob: 0.0 Token:  this, logprob: -13.711415, linearprob: 0.0 Token:  provided, logprob: -14.677594, linearprob: 0.0 Token:  research, logprob: -14.850314, linearprob: 0.0 Token:  provided, logprob: -9.9371064e-05, linearprob: 99.99 Token:  research, logprob: -9.520903, linearprob: 0.01 Token:  data, logprob: -10.757805, linearprob: 0.0 Token:  paper, logprob: -13.263374, linearprob: 0.0 Token:  provide, logprob: -13.802776, linearprob: 0.0 Token:  research, logprob: -2.9352968e-06, linearprob: 100.0 Token:  re, logprob: -13.967486, linearprob: 0.0 Token:  res, logprob: -14.287768, linearprob: 0.0 Token:  , logprob: -15.142121, linearprob: 0.0 Token:  paper, logprob: -15.364785, linearprob: 0.0 Token:  paper, logprob: -3.1305768e-05, linearprob: 100.0 Token:  papers, logprob: -10.422929, linearprob: 0.0 Token:  pap, logprob: -14.647033, linearprob: 0.0 Token:  p, logprob: -15.388553, linearprob: 0.0 Token:  , logprob: -15.823083, linearprob: 0.0 Token: 's, logprob: -2.8921695e-05, linearprob: 100.0 Token: ’s, logprob: -10.95956, linearprob: 0.0 Token:  data, logprob: -11.702805, linearprob: 0.0 Token: \', logprob: -14.103943, linearprob: 0.0 Token: ., logprob: -14.712839, linearprob: 0.0 Token:  data, logprob: -3.7697225e-06, linearprob: 100.0 Token:  methodology, logprob: -13.4813595, linearprob: 0.0 Token:  , logprob: -14.501385, linearprob: 0.0 Token:  Data, logprob: -14.654507, linearprob: 0.0 Token:  method, logprob: -15.710577, linearprob: 0.0 Token:  gathering, logprob: -2.5226382e-05, linearprob: 100.0 Token: -g, logprob: -11.1699295, linearprob: 0.0 Token:  methodology, logprob: -12.078535, linearprob: 0.0 Token:  gather, logprob: -13.388016, linearprob: 0.0 Token:  g, logprob: -13.775322, linearprob: 0.0 Token:  methodology, logprob: -0.00014239625, linearprob: 99.99 Token:  methodologies, logprob: -9.1024275, linearprob: 0.01 Token:  method, logprob: -10.780423, linearprob: 0.0 Token:  methods, logprob: -12.828898, linearprob: 0.0 Token:  meth, logprob: -13.305516, linearprob: 0.0 Token: ., logprob: -0.0014117816, linearprob: 99.86 Token: .\n\n, logprob: -6.8987184, linearprob: 0.1 Token: .\n, logprob: -7.925925, linearprob: 0.04 Token: .Class, logprob: -11.41625, linearprob: 0.0 Token:  Class, logprob: -12.7420025, linearprob: 0.0 Token:  Class, logprob: -0.003956698, linearprob: 99.61 Token:  \n\n, logprob: -5.8128023, linearprob: 0.3 Token:  \n, logprob: -7.104253, linearprob: 0.08 Token:  , logprob: -10.072709, linearprob: 0.0 Token:  The, logprob: -10.552943, linearprob: 0.0 Token: ify, logprob: -1.4974867e-05, linearprob: 100.0 Token: ifying, logprob: -11.5157795, linearprob: 0.0 Token: ifi, logprob: -14.06345, linearprob: 0.0 Token: if, logprob: -14.272831, linearprob: 0.0 Token:  the, logprob: -14.461855, linearprob: 0.0 Token:  the, logprob: -1.7120534e-05, linearprob: 100.0 Token:  data, logprob: -11.503883, linearprob: 0.0 Token:  , logprob: -13.875692, linearprob: 0.0 Token: the, logprob: -14.146009, linearprob: 0.0 Token:  [, logprob: -14.23452, linearprob: 0.0 Token:  data, logprob: -2.6968896e-06, linearprob: 100.0 Token:  gathering, logprob: -14.662108, linearprob: 0.0 Token:  research, logprob: -14.930009, linearprob: 0.0 Token:  method, logprob: -15.031285, linearprob: 0.0 Token:  methodology, logprob: -15.297307, linearprob: 0.0 Token:  gathering, logprob: -3.1424974e-05, linearprob: 100.0 Token: -g, logprob: -10.549407, linearprob: 0.0 Token:  gather, logprob: -12.847808, linearprob: 0.0 Token:  gathered, logprob: -14.323459, linearprob: 0.0 Token:  g, logprob: -14.484843, linearprob: 0.0 Token:  method, logprob: -0.00116165, linearprob: 99.88 Token:  methods, logprob: -6.8235264, linearprob: 0.11 Token:  methodology, logprob: -9.8034725, linearprob: 0.01 Token:  meth, logprob: -11.94474, linearprob: 0.0 Token:  methodologies, logprob: -12.170383, linearprob: 0.0 Token: (s, logprob: -2.749125e-05, linearprob: 100.0 Token:  into, logprob: -10.621625, linearprob: 0.0 Token:  (, logprob: -14.183606, linearprob: 0.0 Token:  in, logprob: -14.558166, linearprob: 0.0 Token:  of, logprob: -15.294602, linearprob: 0.0 Token: ), logprob: -2.8160932e-06, linearprob: 100.0 Token: )\n\n, logprob: -14.467023, linearprob: 0.0 Token: )., logprob: -14.4843855, linearprob: 0.0 Token: )\n, logprob: -14.630916, linearprob: 0.0 Token: ).\n\n, logprob: -15.167815, linearprob: 0.0 Token:  into, logprob: -6.50404e-05, linearprob: 99.99 Token: into, logprob: -10.43651, linearprob: 0.0 Token:  [, logprob: -11.336369, linearprob: 0.0 Token:  in, logprob: -12.062704, linearprob: 0.0 Token:  , logprob: -12.296825, linearprob: 0.0 Token:  any, logprob: -0.00092873344, linearprob: 99.91 Token:  applicable, logprob: -7.2443485, linearprob: 0.07 Token:  the, logprob: -8.513903, linearprob: 0.02 Token:  classes, logprob: -12.550198, linearprob: 0.0 Token:  relevant, logprob: -12.899954, linearprob: 0.0 Token:  applicable, logprob: -3.7697225e-06, linearprob: 100.0 Token:  , logprob: -13.557812, linearprob: 0.0 Token:  classes, logprob: -14.078695, linearprob: 0.0 Token:  of, logprob: -14.636164, linearprob: 0.0 Token:  applic, logprob: -15.14226, linearprob: 0.0 Token:  classes, logprob: -6.511407e-06, linearprob: 100.0 Token:  class, logprob: -12.278354, linearprob: 0.0 Token:  classe, logprob: -14.599128, linearprob: 0.0 Token:  [, logprob: -15.589761, linearprob: 0.0 Token:  , logprob: -15.760983, linearprob: 0.0 Token:  in, logprob: -4.5252596e-05, linearprob: 100.0 Token: ., logprob: -10.656554, linearprob: 0.0 Token: .\n\n, logprob: -12.298432, linearprob: 0.0 Token:  from, logprob: -12.549544, linearprob: 0.0 Token:  of, logprob: -13.028714, linearprob: 0.0 Token:  the, logprob: -6.9882217e-06, linearprob: 100.0 Token:  data, logprob: -12.249948, linearprob: 0.0 Token: the, logprob: -14.775294, linearprob: 0.0 Token:  a, logprob: -14.912837, linearprob: 0.0 Token: \n, logprob: -15.134011, linearprob: 0.0 Token:  data, logprob: -3.4047454e-05, linearprob: 100.0 Token:  taxonomy, logprob: -11.215204, linearprob: 0.0 Token:  research, logprob: -11.351025, linearprob: 0.0 Token:  Data, logprob: -12.398745, linearprob: 0.0 Token:  methodology, logprob: -12.896931, linearprob: 0.0 Token:  methodology, logprob: -0.010585193, linearprob: 98.95 Token:  gathering, logprob: -4.560186, linearprob: 1.05 Token:  method, logprob: -10.041329, linearprob: 0.0 Token: -g, logprob: -11.869466, linearprob: 0.0 Token:  collection, logprob: -12.077493, linearprob: 0.0 Token:  gathering, logprob: -0.00023357604, linearprob: 99.98 Token:  taxonomy, logprob: -8.405602, linearprob: 0.02 Token: -g, logprob: -11.960952, linearprob: 0.0 Token:  gather, logprob: -13.524337, linearprob: 0.0 Token:  , logprob: -14.834346, linearprob: 0.0 Token:  taxonomy, logprob: -3.650519e-06, linearprob: 100.0 Token: taxonomy, logprob: -13.5459385, linearprob: 0.0 Token:  [, logprob: -14.33906, linearprob: 0.0 Token:  tax, logprob: -15.010305, linearprob: 0.0 Token:  Tax, logprob: -15.186043, linearprob: 0.0 Token: ., logprob: -0.11377335, linearprob: 89.25 Token: .\n\n, logprob: -2.4450932, linearprob: 8.67 Token: .\n, logprob: -4.1030407, linearprob: 1.65 Token: .An, logprob: -5.4626255, linearprob: 0.42 Token: .\n\n\n, logprob: -10.509983, linearprob: 0.0 Token:  Answer, logprob: -0.61152804, linearprob: 54.25 Token:  \n\n, logprob: -1.3171593, linearprob: 26.79 Token:   \n, logprob: -2.4343772, linearprob: 8.77 Token:  \n, logprob: -2.8847632, linearprob: 5.59 Token:   \n\n, logprob: -3.646007, linearprob: 2.61 Token:  in, logprob: -0.018105693, linearprob: 98.21 Token: :, logprob: -4.1142626, linearprob: 1.63 Token: :\n\n, logprob: -7.9936037, linearprob: 0.03 Token: :\n, logprob: -8.240872, linearprob: 0.03 Token: ing, logprob: -8.753318, linearprob: 0.02 Token:  the, logprob: -9.41261e-05, linearprob: 99.99 Token:  this, logprob: -9.937392, linearprob: 0.0 Token:  a, logprob: -10.424794, linearprob: 0.0 Token: :, logprob: -13.47404, linearprob: 0.0 Token:  format, logprob: -13.491971, linearprob: 0.0 Token:  format, logprob: -0.0001690942, linearprob: 99.98 Token:  following, logprob: -8.750764, linearprob: 0.02 Token:  below, logprob: -12.185443, linearprob: 0.0 Token:  form, logprob: -13.178943, linearprob: 0.0 Token:  [, logprob: -14.477672, linearprob: 0.0 Token:  of, logprob: -0.0011472448, linearprob: 99.89 Token: :, logprob: -7.295598, linearprob: 0.07 Token: :\n\n, logprob: -8.570897, linearprob: 0.02 Token:  below, logprob: -9.528512, linearprob: 0.01 Token:  as, logprob: -9.576154, linearprob: 0.01 Token:  the, logprob: -0.0003802681, linearprob: 99.96 Token: :, logprob: -8.790753, linearprob: 0.02 Token:  a, logprob: -9.6979685, linearprob: 0.01 Token: :\n\n, logprob: -10.480151, linearprob: 0.0 Token:  , logprob: -10.775029, linearprob: 0.0 Token:  following, logprob: -2.486877e-05, linearprob: 100.0 Token:  sentence, logprob: -11.447493, linearprob: 0.0 Token:  the, logprob: -13.347553, linearprob: 0.0 Token:  , logprob: -13.543348, linearprob: 0.0 Token:  [, logprob: -13.596598, linearprob: 0.0 Token:  sentence, logprob: -2.2007884e-05, linearprob: 100.0 Token:  sentences, logprob: -11.820775, linearprob: 0.0 Token:  set, logprob: -13.1319, linearprob: 0.0 Token:  statement, logprob: -13.440314, linearprob: 0.0 Token:  , logprob: -13.46083, linearprob: 0.0 Token: ,, logprob: -0.000181849, linearprob: 99.98 Token:  filling, logprob: -10.012809, linearprob: 0.0 Token: :, logprob: -10.052453, linearprob: 0.0 Token: :\n\n, logprob: -10.16924, linearprob: 0.0 Token: ., logprob: -11.058589, linearprob: 0.0 Token:  filling, logprob: -9.555654e-05, linearprob: 99.99 Token:  , logprob: -11.109976, linearprob: 0.0 Token:  \n\n, logprob: -11.308496, linearprob: 0.0 Token:  replacing, logprob: -11.499916, linearprob: 0.0 Token:  filing, logprob: -11.990429, linearprob: 0.0 Token:  in, logprob: -0.00014609157, linearprob: 99.99 Token:  the, logprob: -8.990817, linearprob: 0.01 Token:  [, logprob: -11.645486, linearprob: 0.0 Token:  out, logprob: -13.035108, linearprob: 0.0 Token: -in, logprob: -13.17114, linearprob: 0.0 Token:  the, logprob: -4.1080468e-05, linearprob: 100.0 Token:  [, logprob: -11.022878, linearprob: 0.0 Token:  with, logprob: -12.12127, linearprob: 0.0 Token:  , logprob: -12.721417, linearprob: 0.0 Token:  a, logprob: -12.851197, linearprob: 0.0 Token:  [, logprob: -0.0010935463, linearprob: 99.89 Token:  `[, logprob: -7.1559796, linearprob: 0.08 Token:  \, logprob: -8.339223, linearprob: 0.02 Token:  **, logprob: -10.9226465, linearprob: 0.0 Token:  , logprob: -11.382062, linearprob: 0.0 Token: MASK, logprob: -7.3861476e-05, linearprob: 99.99 Token: mask, logprob: -10.548617, linearprob: 0.0 Token:  MASK, logprob: -10.635247, linearprob: 0.0 Token: Mask, logprob: -11.302184, linearprob: 0.0 Token:  ], logprob: -12.653056, linearprob: 0.0 Token: ], logprob: -4.1273333e-06, linearprob: 100.0 Token: ]\n, logprob: -13.797859, linearprob: 0.0 Token: ]\n\n, logprob: -14.035818, linearprob: 0.0 Token: 1, logprob: -14.183126, linearprob: 0.0 Token: }, logprob: -15.247286, linearprob: 0.0 Token:  token, logprob: -1.4974867e-05, linearprob: 100.0 Token: token, logprob: -11.86388, linearprob: 0.0 Token:  tokens, logprob: -12.468895, linearprob: 0.0 Token: 	token, logprob: -13.495386, linearprob: 0.0 Token:  , logprob: -13.901199, linearprob: 0.0 Token:  with, logprob: -3.0113732e-05, linearprob: 100.0 Token: :, logprob: -12.411846, linearprob: 0.0 Token: .\n\n, logprob: -12.489748, linearprob: 0.0 Token: :\n\n, logprob: -12.645889, linearprob: 0.0 Token: ., logprob: -13.018305, linearprob: 0.0 Token:  a, logprob: -7.588794e-05, linearprob: 99.99 Token:  methodologies, logprob: -10.371052, linearprob: 0.0 Token:  the, logprob: -10.977669, linearprob: 0.0 Token:  any, logprob: -11.843689, linearprob: 0.0 Token:  sets, logprob: -12.384043, linearprob: 0.0 Token:  set, logprob: -3.9530823e-05, linearprob: 100.0 Token:  list, logprob: -10.354217, linearprob: 0.0 Token:  [, logprob: -13.549695, linearprob: 0.0 Token:  a, logprob: -14.290122, linearprob: 0.0 Token:  , logprob: -14.660411, linearprob: 0.0 Token:  of, logprob: -2.319992e-05, linearprob: 100.0 Token:  containing, logprob: -10.9060955, linearprob: 0.0 Token:  that, logprob: -14.280766, linearprob: 0.0 Token:  any, logprob: -14.4054165, linearprob: 0.0 Token:  , logprob: -14.461355, linearprob: 0.0 Token:  any, logprob: -2.9160103e-05, linearprob: 100.0 Token:  methodologies, logprob: -10.908658, linearprob: 0.0 Token:  the, logprob: -12.944142, linearprob: 0.0 Token:  length, logprob: -13.163211, linearprob: 0.0 Token:  a, logprob: -13.280583, linearprob: 0.0 Token:  length, logprob: -5.50273e-05, linearprob: 99.99 Token:  lengths, logprob: -10.58947, linearprob: 0.0 Token: -length, logprob: -12.271072, linearprob: 0.0 Token:  size, logprob: -12.429794, linearprob: 0.0 Token:  methodology, logprob: -12.6767235, linearprob: 0.0 Token:  containing, logprob: -4.3583746e-05, linearprob: 100.0 Token: ,, logprob: -11.5774555, linearprob: 0.0 Token:  that, logprob: -11.843786, linearprob: 0.0 Token:  of, logprob: -12.7357235, linearprob: 0.0 Token:  contain, logprob: -12.786238, linearprob: 0.0 Token:  methodologies, logprob: -0.00024906153, linearprob: 99.98 Token:  the, logprob: -8.86991, linearprob: 0.01 Token:  methodology, logprob: -10.19344, linearprob: 0.0 Token:  methods, logprob: -10.846238, linearprob: 0.0 Token:  method, logprob: -11.124352, linearprob: 0.0 Token:  that, logprob: -6.396757e-05, linearprob: 99.99 Token: .\n\n, logprob: -11.500081, linearprob: 0.0 Token: :\n\n, logprob: -11.534022, linearprob: 0.0 Token: ., logprob: -11.735703, linearprob: 0.0 Token: :, logprob: -12.291743, linearprob: 0.0 Token:  match, logprob: -0.00012535013, linearprob: 99.99 Token:  best, logprob: -9.059087, linearprob: 0.01 Token:  matches, logprob: -12.354304, linearprob: 0.0 Token:  fit, logprob: -13.464982, linearprob: 0.0 Token:  are, logprob: -13.713769, linearprob: 0.0 Token:  best, logprob: -0.0012491621, linearprob: 99.88 Token:  the, logprob: -6.69903, linearprob: 0.12 Token: ., logprob: -12.423496, linearprob: 0.0 Token:  well, logprob: -12.814465, linearprob: 0.0 Token:  it, logprob: -13.795078, linearprob: 0.0 Token: ., logprob: -0.0013635702, linearprob: 99.86 Token: .\n\n, logprob: -6.7055836, linearprob: 0.12 Token: .\n, logprob: -9.186029, linearprob: 0.01 Token: :\n\n, logprob: -11.5760975, linearprob: 0.0 Token: :, logprob: -12.121424, linearprob: 0.0 Token:  The, logprob: -0.008545544, linearprob: 99.15 Token:  \n\n, logprob: -4.8392777, linearprob: 0.79 Token:  \n, logprob: -7.904677, linearprob: 0.04 Token:  , logprob: -9.027516, linearprob: 0.01 Token:   \n\n, logprob: -9.828933, linearprob: 0.01 Token:  set, logprob: -4.310693e-05, linearprob: 100.0 Token:  data, logprob: -10.411614, linearprob: 0.0 Token:  research, logprob: -11.496167, linearprob: 0.0 Token:  classification, logprob: -14.903529, linearprob: 0.0 Token: set, logprob: -15.451724, linearprob: 0.0 Token:  of, logprob: -1.2113979e-05, linearprob: 100.0 Token:  for, logprob: -12.618157, linearprob: 0.0 Token:  containing, logprob: -12.907363, linearprob: 0.0 Token:  this, logprob: -13.03445, linearprob: 0.0 Token:  the, logprob: -14.421189, linearprob: 0.0 Token:  this, logprob: -6.46828e-05, linearprob: 99.99 Token:  data, logprob: -10.183369, linearprob: 0.0 Token:  the, logprob: -11.276123, linearprob: 0.0 Token:  research, logprob: -12.15016, linearprob: 0.0 Token:  these, logprob: -12.4251995, linearprob: 0.0 Token:  research, logprob: -9.0883464e-07, linearprob: 100.0 Token:  re, logprob: -15.0166445, linearprob: 0.0 Token:  paper, logprob: -15.786567, linearprob: 0.0 Token:  , logprob: -16.032093, linearprob: 0.0 Token:  res, logprob: -16.050915, linearprob: 0.0 Token:  paper, logprob: -9.968313e-06, linearprob: 100.0 Token:  papers, logprob: -11.619719, linearprob: 0.0 Token:  p, logprob: -15.155148, linearprob: 0.0 Token:  pap, logprob: -15.157141, linearprob: 0.0 Token: 's, logprob: -16.304058, linearprob: 0.0 Token: 's, logprob: -1.3663626e-05, linearprob: 100.0 Token: ’s, logprob: -11.46933, linearprob: 0.0 Token:  data, logprob: -13.253723, linearprob: 0.0 Token: \', logprob: -15.371487, linearprob: 0.0 Token: ', logprob: -15.729118, linearprob: 0.0 Token:  data, logprob: -7.89631e-07, linearprob: 100.0 Token:  , logprob: -15.294833, linearprob: 0.0 Token:  Data, logprob: -15.87441, linearprob: 0.0 Token:  gathering, logprob: -15.891013, linearprob: 0.0 Token:  dat, logprob: -17.499159, linearprob: 0.0 Token:  gathering, logprob: -9.150362e-05, linearprob: 99.99 Token: -g, logprob: -9.324276, linearprob: 0.01 Token:  g, logprob: -14.466675, linearprob: 0.0 Token:  gather, logprob: -14.583523, linearprob: 0.0 Token:  , logprob: -14.915503, linearprob: 0.0 Token:  methodologies, logprob: -4.954393e-05, linearprob: 100.0 Token:  methodology, logprob: -10.028927, linearprob: 0.0 Token:  methods, logprob: -13.120272, linearprob: 0.0 Token:  method, logprob: -13.26856, linearprob: 0.0 Token:  , logprob: -14.250753, linearprob: 0.0 Token:  contains, logprob: -0.0003646634, linearprob: 99.96 Token:  contain, logprob: -8.039049, linearprob: 0.03 Token:  includes, logprob: -10.788044, linearprob: 0.0 Token:  , logprob: -12.785517, linearprob: 0.0 Token:  consists, logprob: -12.983938, linearprob: 0.0 Token:  [, logprob: -0.024277126, linearprob: 97.6 Token:  brainstorm, logprob: -3.9643788, linearprob: 1.9 Token:  interviews, logprob: -6.3788056, linearprob: 0.17 Token:  [, logprob: -6.971951, linearprob: 0.09 Token:  online, logprob: -7.2811856, linearprob: 0.07 Token: brain, logprob: -0.558544, linearprob: 57.2 Token: int, logprob: -1.661844, linearprob: 18.98 Token: focus, logprob: -2.4604242, linearprob: 8.54 Token: MASK, logprob: -2.800266, linearprob: 6.08 Token: online, logprob: -3.9558408, linearprob: 1.91 Token: storm, logprob: -6.869018e-06, linearprob: 100.0 Token: st, logprob: -13.102031, linearprob: 0.0 Token: strom, logprob: -13.545054, linearprob: 0.0 Token:  storm, logprob: -14.062167, linearprob: 0.0 Token: storms, logprob: -14.107741, linearprob: 0.0 Token: ing, logprob: -2.3438328e-05, linearprob: 100.0 Token: ,, logprob: -12.021865, linearprob: 0.0 Token: in, logprob: -12.129485, linearprob: 0.0 Token: ings, logprob: -12.691003, linearprob: 0.0 Token:  and, logprob: -12.964618, linearprob: 0.0 Token: ,, logprob: -0.01329046, linearprob: 98.68 Token:  and, logprob: -4.4593143, linearprob: 1.16 Token: ],, logprob: -6.768738, linearprob: 0.11 Token: ], logprob: -8.0400505, linearprob: 0.03 Token: ]., logprob: -9.741531, linearprob: 0.01 Token:  interviews, logprob: -0.19507545, linearprob: 82.28 Token:  focus, logprob: -2.0877147, linearprob: 12.4 Token:  conceptual, logprob: -3.2238193, linearprob: 3.98 Token:  online, logprob: -5.5597568, linearprob: 0.38 Token:  question, logprob: -5.600822, linearprob: 0.37 Token: ,, logprob: -0.009502366, linearprob: 99.05 Token: ]., logprob: -4.727112, linearprob: 0.89 Token: ], logprob: -7.604309, linearprob: 0.05 Token: ].\n\n, logprob: -9.6128845, linearprob: 0.01 Token: ].\n, logprob: -10.491045, linearprob: 0.0 Token:  question, logprob: -1.184144, linearprob: 30.6 Token:  conceptual, logprob: -1.2094259, linearprob: 29.84 Token:  online, logprob: -1.5834188, linearprob: 20.53 Token:  document, logprob: -2.4020739, linearprob: 9.05 Token:  think, logprob: -2.823104, linearprob: 5.94 Token: naires, logprob: -4.6683042e-05, linearprob: 100.0 Token: n, logprob: -10.957494, linearprob: 0.0 Token: aires, logprob: -11.167026, linearprob: 0.0 Token: nn, logprob: -12.4677515, linearprob: 0.0 Token: aries, logprob: -12.812673, linearprob: 0.0 Token: ,, logprob: -0.104137085, linearprob: 90.11 Token: ]., logprob: -2.3299158, linearprob: 9.73 Token: ], logprob: -7.121305, linearprob: 0.08 Token: ].\n\n, logprob: -7.712629, linearprob: 0.04 Token: ].\n, logprob: -8.119841, linearprob: 0.03 Token:  conceptual, logprob: -0.7631643, linearprob: 46.62 Token:  think, logprob: -1.3384778, linearprob: 26.22 Token:  lab, logprob: -2.1773317, linearprob: 11.33 Token:  document, logprob: -2.8704221, linearprob: 5.67 Token:  shadow, logprob: -3.359657, linearprob: 3.47 Token:  modeling, logprob: -0.0001202317, linearprob: 99.99 Token:  modelling, logprob: -9.347113, linearprob: 0.01 Token:  model, logprob: -10.416931, linearprob: 0.0 Token:  , logprob: -14.455906, linearprob: 0.0 Token:  mapping, logprob: -14.567415, linearprob: 0.0 Token: ,, logprob: -0.5824995, linearprob: 55.85 Token: ]., logprob: -0.8242445, linearprob: 43.86 Token: ].\n\n, logprob: -6.56911, linearprob: 0.14 Token: ], logprob: -7.153821, linearprob: 0.08 Token: ].\n, logprob: -7.308667, linearprob: 0.07 Token:  think, logprob: -0.48801032, linearprob: 61.38 Token:  document, logprob: -1.764633, linearprob: 17.12 Token:  instrument, logprob: -2.379158, linearprob: 9.26 Token:  lab, logprob: -3.0886288, linearprob: 4.56 Token:  shadow, logprob: -3.2556229, linearprob: 3.86 Token: -al, logprob: -6.392203e-06, linearprob: 100.0 Token:  aloud, logprob: -12.46074, linearprob: 0.0 Token: -, logprob: -13.552213, linearprob: 0.0 Token: al, logprob: -14.489385, linearprob: 0.0 Token: -out, logprob: -15.508409, linearprob: 0.0 Token: oud, logprob: -1.18755715e-05, linearprob: 100.0 Token: ound, logprob: -11.614613, linearprob: 0.0 Token: ould, logprob: -13.842379, linearprob: 0.0 Token: ong, logprob: -14.043026, linearprob: 0.0 Token: oad, logprob: -15.280636, linearprob: 0.0 Token:  sessions, logprob: -2.0219828e-05, linearprob: 100.0 Token:  session, logprob: -11.394139, linearprob: 0.0 Token: sessions, logprob: -12.25228, linearprob: 0.0 Token: ,, logprob: -13.2224455, linearprob: 0.0 Token: s, logprob: -13.944802, linearprob: 0.0 Token: ]., logprob: -0.42238355, linearprob: 65.55 Token: ,, logprob: -1.0820973, linearprob: 33.89 Token: ], logprob: -6.1679087, linearprob: 0.21 Token: ].\n\n, logprob: -6.1752462, linearprob: 0.21 Token: ].\n, logprob: -6.6486387, linearprob: 0.13 ","Classification: Data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method(s) into any applicable classes in the data methodology gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: the set of this research paper's data gathering methodologies contains brainstorming, focus groups, interviews, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, document corpus analysis, and lab experiment.,"Token: The, logprob: -0.0009755438, linearprob: 99.9 Token: Classification, logprob: -7.0780745, linearprob: 0.08 Token: In, logprob: -10.638225, linearprob: 0.0 Token: Class, logprob: -10.70515, linearprob: 0.0 Token:  The, logprob: -10.76237, linearprob: 0.0 Token:  set, logprob: -7.0523776e-05, linearprob: 99.99 Token:  research, logprob: -10.187494, linearprob: 0.0 Token:  data, logprob: -10.632824, linearprob: 0.0 Token:  classification, logprob: -12.071688, linearprob: 0.0 Token:  provided, logprob: -13.119837, linearprob: 0.0 Token:  of, logprob: -2.220075e-06, linearprob: 100.0 Token:  containing, logprob: -14.112082, linearprob: 0.0 Token:  for, logprob: -14.3041, linearprob: 0.0 Token:  this, logprob: -15.064798, linearprob: 0.0 Token:  the, logprob: -15.646343, linearprob: 0.0 Token:  this, logprob: -1.962381e-05, linearprob: 100.0 Token:  data, logprob: -11.280477, linearprob: 0.0 Token:  the, logprob: -12.803149, linearprob: 0.0 Token:  research, logprob: -13.100526, linearprob: 0.0 Token:  methodologies, logprob: -13.2779875, linearprob: 0.0 Token:  research, logprob: -6.704273e-07, linearprob: 100.0 Token:  re, logprob: -15.245311, linearprob: 0.0 Token:  res, logprob: -16.160433, linearprob: 0.0 Token:  Research, logprob: -16.306973, linearprob: 0.0 Token: research, logprob: -17.109636, linearprob: 0.0 Token:  paper, logprob: -1.9027791e-05, linearprob: 100.0 Token:  papers, logprob: -10.910236, linearprob: 0.0 Token:  pap, logprob: -15.188889, linearprob: 0.0 Token:  p, logprob: -15.716658, linearprob: 0.0 Token: 's, logprob: -16.54115, linearprob: 0.0 Token: 's, logprob: -1.1517961e-05, linearprob: 100.0 Token: ’s, logprob: -11.390558, linearprob: 0.0 Token: \', logprob: -16.396082, linearprob: 0.0 Token: ‘s, logprob: -17.218147, linearprob: 0.0 Token: ´s, logprob: -17.478846, linearprob: 0.0 Token:  data, logprob: -3.1281633e-07, linearprob: 100.0 Token:  Data, logprob: -16.107714, linearprob: 0.0 Token:  gathering, logprob: -16.155493, linearprob: 0.0 Token:  , logprob: -17.507563, linearprob: 0.0 Token:  dat, logprob: -18.324125, linearprob: 0.0 Token:  gathering, logprob: -6.897413e-05, linearprob: 99.99 Token: -g, logprob: -9.631838, linearprob: 0.01 Token:  methodology, logprob: -14.081856, linearprob: 0.0 Token:  g, logprob: -14.130768, linearprob: 0.0 Token:  gather, logprob: -14.210527, linearprob: 0.0 Token:  methodologies, logprob: -0.00024703506, linearprob: 99.98 Token:  methodology, logprob: -8.4571905, linearprob: 0.02 Token:  methods, logprob: -10.536395, linearprob: 0.0 Token:  method, logprob: -12.009639, linearprob: 0.0 Token:  meth, logprob: -13.815078, linearprob: 0.0 Token:  contains, logprob: -0.00021879478, linearprob: 99.98 Token:  contain, logprob: -8.565223, linearprob: 0.02 Token:  includes, logprob: -10.798324, linearprob: 0.0 Token:  contained, logprob: -13.129388, linearprob: 0.0 Token:  consists, logprob: -14.118716, linearprob: 0.0 Token:  brainstorm, logprob: -0.17452994, linearprob: 83.99 Token:  interviews, logprob: -1.8859671, linearprob: 15.17 Token:  focus, logprob: -5.5389957, linearprob: 0.39 Token:  think, logprob: -6.667364, linearprob: 0.13 Token: :, logprob: -7.166193, linearprob: 0.08 Token: ing, logprob: -8.299462e-06, linearprob: 100.0 Token: ,, logprob: -12.872834, linearprob: 0.0 Token: ings, logprob: -13.280774, linearprob: 0.0 Token: in, logprob: -13.545966, linearprob: 0.0 Token:  and, logprob: -14.609351, linearprob: 0.0 Token: ,, logprob: -0.04588727, linearprob: 95.51 Token:  and, logprob: -3.105076, linearprob: 4.48 Token:  sessions, logprob: -11.39172, linearprob: 0.0 Token: ,f, logprob: -12.150221, linearprob: 0.0 Token: /f, logprob: -12.429411, linearprob: 0.0 Token:  focus, logprob: -0.34139323, linearprob: 71.08 Token:  interviews, logprob: -1.2695587, linearprob: 28.1 Token:  conceptual, logprob: -5.43328, linearprob: 0.44 Token:  concept, logprob: -6.099784, linearprob: 0.22 Token:  think, logprob: -7.800269, linearprob: 0.04 Token:  groups, logprob: -9.138441e-05, linearprob: 99.99 Token:  group, logprob: -9.301739, linearprob: 0.01 Token:  Groups, logprob: -17.128328, linearprob: 0.0 Token: -groups, logprob: -17.882374, linearprob: 0.0 Token: groups, logprob: -18.254929, linearprob: 0.0 Token: ,, logprob: -4.6206227e-05, linearprob: 100.0 Token:  and, logprob: -10.649403, linearprob: 0.0 Token: ., logprob: -10.911148, linearprob: 0.0 Token:  interviews, logprob: -12.704815, linearprob: 0.0 Token:  (, logprob: -14.804912, linearprob: 0.0 Token:  interviews, logprob: -0.04896237, linearprob: 95.22 Token:  conceptual, logprob: -3.3913982, linearprob: 3.37 Token:  participant, logprob: -5.742255, linearprob: 0.32 Token:  and, logprob: -5.993758, linearprob: 0.25 Token:  concept, logprob: -6.1367464, linearprob: 0.22 Token: ,, logprob: -0.0029477512, linearprob: 99.71 Token: ., logprob: -5.900649, linearprob: 0.27 Token:  and, logprob: -8.758397, linearprob: 0.02 Token:  (, logprob: -10.104209, linearprob: 0.0 Token:  with, logprob: -13.155245, linearprob: 0.0 Token:  question, logprob: -0.3419838, linearprob: 71.04 Token:  and, logprob: -1.7237797, linearprob: 17.84 Token:  conceptual, logprob: -2.9592533, linearprob: 5.19 Token:  concept, logprob: -4.072445, linearprob: 1.7 Token:  think, logprob: -4.1005096, linearprob: 1.66 Token: naires, logprob: -2.6895234e-05, linearprob: 100.0 Token: aires, logprob: -10.741217, linearprob: 0.0 Token: n, logprob: -12.850414, linearprob: 0.0 Token: aries, logprob: -13.972462, linearprob: 0.0 Token: na, logprob: -14.140282, linearprob: 0.0 Token: ,, logprob: -0.0046680495, linearprob: 99.53 Token: ., logprob: -5.3908415, linearprob: 0.46 Token:  and, logprob: -9.4560175, linearprob: 0.01 Token:  as, logprob: -12.078421, linearprob: 0.0 Token: .\n, logprob: -12.920135, linearprob: 0.0 Token:  conceptual, logprob: -0.58991784, linearprob: 55.44 Token:  and, logprob: -1.0123765, linearprob: 36.34 Token:  think, logprob: -3.1544168, linearprob: 4.27 Token:  shadow, logprob: -4.188352, linearprob: 1.52 Token:  participant, logprob: -5.009977, linearprob: 0.67 Token:  modeling, logprob: -5.8841815e-05, linearprob: 99.99 Token:  modelling, logprob: -9.765371, linearprob: 0.01 Token:  model, logprob: -14.165096, linearprob: 0.0 Token:  mapping, logprob: -15.783764, linearprob: 0.0 Token:  , logprob: -15.955112, linearprob: 0.0 Token: ,, logprob: -0.009076463, linearprob: 99.1 Token: ., logprob: -4.7136683, linearprob: 0.9 Token:  and, logprob: -9.872452, linearprob: 0.01 Token: .\n, logprob: -12.763718, linearprob: 0.0 Token: .\n\n, logprob: -14.022005, linearprob: 0.0 Token:  work, logprob: -0.9922491, linearprob: 37.07 Token:  think, logprob: -1.5282941, linearprob: 21.69 Token:  and, logprob: -1.605042, linearprob: 20.09 Token:  document, logprob: -2.0975347, linearprob: 12.28 Token:  shadow, logprob: -3.4414449, linearprob: 3.2 Token:  di, logprob: -3.0545007e-06, linearprob: 100.0 Token:  diary, logprob: -12.998824, linearprob: 0.0 Token:  d, logprob: -14.688956, linearprob: 0.0 Token: di, logprob: -15.808391, linearprob: 0.0 Token:  Di, logprob: -17.07532, linearprob: 0.0 Token: aries, logprob: -4.246537e-06, linearprob: 100.0 Token: ar, logprob: -13.602524, linearprob: 0.0 Token: aris, logprob: -14.000137, linearprob: 0.0 Token: arie, logprob: -14.335695, linearprob: 0.0 Token: ari, logprob: -14.480827, linearprob: 0.0 Token: ,, logprob: -0.0034976734, linearprob: 99.65 Token: ., logprob: -5.661659, linearprob: 0.35 Token:  and, logprob: -11.512142, linearprob: 0.0 Token: .\n, logprob: -13.358175, linearprob: 0.0 Token: .\n\n, logprob: -14.48034, linearprob: 0.0 Token:  think, logprob: -0.2148346, linearprob: 80.67 Token:  and, logprob: -1.9839672, linearprob: 13.75 Token:  shadow, logprob: -3.6206288, linearprob: 2.68 Token:  participant, logprob: -4.1199803, linearprob: 1.62 Token:  fly, logprob: -5.1206365, linearprob: 0.6 Token: -al, logprob: -6.1537958e-06, linearprob: 100.0 Token:  aloud, logprob: -12.218464, linearprob: 0.0 Token: -, logprob: -14.197607, linearprob: 0.0 Token: ‐, logprob: -15.205317, linearprob: 0.0 Token: -out, logprob: -16.294727, linearprob: 0.0 Token: oud, logprob: -8.299462e-06, linearprob: 100.0 Token: ound, logprob: -11.808124, linearprob: 0.0 Token: ong, logprob: -14.834058, linearprob: 0.0 Token: ould, logprob: -16.226488, linearprob: 0.0 Token: oad, logprob: -16.527159, linearprob: 0.0 Token:  sessions, logprob: -5.6769813e-06, linearprob: 100.0 Token: sessions, logprob: -13.337044, linearprob: 0.0 Token:  session, logprob: -13.489698, linearprob: 0.0 Token: s, logprob: -13.66428, linearprob: 0.0 Token: ,, logprob: -14.233017, linearprob: 0.0 Token: ,, logprob: -0.034002498, linearprob: 96.66 Token: ., logprob: -3.3997977, linearprob: 3.34 Token:  and, logprob: -10.266325, linearprob: 0.0 Token: .\n, logprob: -12.023294, linearprob: 0.0 Token: .\n\n, logprob: -13.09798, linearprob: 0.0 Token:  shadow, logprob: -0.88865805, linearprob: 41.12 Token:  and, logprob: -1.5291, linearprob: 21.67 Token:  participant, logprob: -1.609899, linearprob: 19.99 Token:  document, logprob: -2.431005, linearprob: 8.79 Token:  fly, logprob: -3.341783, linearprob: 3.54 Token: ing, logprob: -3.650519e-06, linearprob: 100.0 Token: in, logprob: -13.619804, linearprob: 0.0 Token: ,, logprob: -14.023178, linearprob: 0.0 Token: ings, logprob: -15.148687, linearprob: 0.0 Token: ling, logprob: -15.1585865, linearprob: 0.0 Token: ,, logprob: -0.008336906, linearprob: 99.17 Token: ., logprob: -4.8030777, linearprob: 0.82 Token:  and, logprob: -10.086824, linearprob: 0.0 Token:  participant, logprob: -10.148169, linearprob: 0.0 Token: .\n, logprob: -12.257991, linearprob: 0.0 Token:  participant, logprob: -0.26616326, linearprob: 76.63 Token:  and, logprob: -1.9413818, linearprob: 14.35 Token:  document, logprob: -3.1706777, linearprob: 4.2 Token:  instrument, logprob: -3.5063806, linearprob: 3.0 Token:  fly, logprob: -4.633801, linearprob: 0.97 Token: -, logprob: -4.4179764e-05, linearprob: 100.0 Token:  observer, logprob: -10.790976, linearprob: 0.0 Token:  observation, logprob: -10.801178, linearprob: 0.0 Token: -ob, logprob: -13.0218315, linearprob: 0.0 Token:  observations, logprob: -14.932177, linearprob: 0.0 Token: observer, logprob: -0.0010092307, linearprob: 99.9 Token: observation, logprob: -6.9348435, linearprob: 0.1 Token: obs, logprob: -11.26684, linearprob: 0.0 Token:  observer, logprob: -11.844412, linearprob: 0.0 Token: observ, logprob: -12.3027725, linearprob: 0.0 Token: ,, logprob: -0.009898456, linearprob: 99.02 Token: ., logprob: -4.6571383, linearprob: 0.95 Token:  technique, logprob: -8.887555, linearprob: 0.01 Token:  techniques, logprob: -9.195424, linearprob: 0.01 Token:  and, logprob: -9.99532, linearprob: 0.0 Token:  instrument, logprob: -0.58170325, linearprob: 55.89 Token:  and, logprob: -1.0627537, linearprob: 34.55 Token:  document, logprob: -2.9802628, linearprob: 5.08 Token:  fly, logprob: -3.5974522, linearprob: 2.74 Token:  lab, logprob: -4.5816765, linearprob: 1.02 Token: ing, logprob: -1.18755715e-05, linearprob: 100.0 Token:  systems, logprob: -12.395407, linearprob: 0.0 Token: ed, logprob: -12.647154, linearprob: 0.0 Token: ating, logprob: -13.196545, linearprob: 0.0 Token:  system, logprob: -13.347889, linearprob: 0.0 Token:  systems, logprob: -6.15835e-05, linearprob: 99.99 Token:  system, logprob: -9.729789, linearprob: 0.01 Token: ,, logprob: -14.967385, linearprob: 0.0 Token:  Systems, logprob: -15.023055, linearprob: 0.0 Token:  the, logprob: -15.051888, linearprob: 0.0 Token: ,, logprob: -0.0011008031, linearprob: 99.89 Token: ., logprob: -6.819407, linearprob: 0.11 Token:  and, logprob: -12.390115, linearprob: 0.0 Token: .\n, logprob: -14.48261, linearprob: 0.0 Token:  document, logprob: -14.81528, linearprob: 0.0 Token:  document, logprob: -0.7796621, linearprob: 45.86 Token:  fly, logprob: -1.1445398, linearprob: 31.84 Token:  and, logprob: -1.731143, linearprob: 17.71 Token:  lab, logprob: -3.1470861, linearprob: 4.3 Token:  image, logprob: -6.8589087, linearprob: 0.11 Token:  corpus, logprob: -1.8074163e-05, linearprob: 100.0 Token:  corps, logprob: -11.545299, linearprob: 0.0 Token:  corp, logprob: -12.511489, linearprob: 0.0 Token:  corpor, logprob: -12.798421, linearprob: 0.0 Token:  cor, logprob: -14.206828, linearprob: 0.0 Token:  analysis, logprob: -1.504853e-06, linearprob: 100.0 Token: ,, logprob: -14.314842, linearprob: 0.0 Token:  an, logprob: -15.432896, linearprob: 0.0 Token:  analy, logprob: -15.665983, linearprob: 0.0 Token:  Analysis, logprob: -15.826191, linearprob: 0.0 Token: ,, logprob: -0.10575177, linearprob: 89.96 Token: ., logprob: -2.300742, linearprob: 10.02 Token:  and, logprob: -9.332242, linearprob: 0.01 Token: .\n, logprob: -9.947863, linearprob: 0.0 Token: .\n\n, logprob: -11.425951, linearprob: 0.0 Token:  and, logprob: -0.90174204, linearprob: 40.59 Token:  lab, logprob: -1.337903, linearprob: 26.24 Token:  image, logprob: -1.755702, linearprob: 17.28 Token:  social, logprob: -1.8526964, linearprob: 15.68 Token:  fly, logprob: -7.468055, linearprob: 0.06 Token:  lab, logprob: -0.03747613, linearprob: 96.32 Token:  social, logprob: -3.800305, linearprob: 2.24 Token:  image, logprob: -4.472531, linearprob: 1.14 Token:  no, logprob: -7.0646005, linearprob: 0.09 Token:  laboratory, logprob: -7.2527604, linearprob: 0.07 Token:  experiment, logprob: -0.0026496174, linearprob: 99.74 Token:  experiments, logprob: -5.936217, linearprob: 0.26 Token:  experimentation, logprob: -12.617479, linearprob: 0.0 Token:  Experiment, logprob: -15.174101, linearprob: 0.0 Token:  exper, logprob: -15.791817, linearprob: 0.0 Token: ., logprob: -0.0009305142, linearprob: 99.91 Token: <|end|>, logprob: -8.498409, linearprob: 0.02 Token:  techniques, logprob: -8.603758, linearprob: 0.02 Token:  methodologies, logprob: -8.7463045, linearprob: 0.02 Token:  methods, logprob: -8.822799, linearprob: 0.01 ","Classification: Data gathering methodology - You are to perform a multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method(s) into any applicable classes in the data methodology gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: the set of this research paper's data gathering methodologies contains brainstorming, focus groups, interviews, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, and lab experiment.,"Token: The, logprob: -0.00019233524, linearprob: 99.98 Token:  The, logprob: -9.909912, linearprob: 0.0 Token: Classification, logprob: -10.389009, linearprob: 0.0 Token: In, logprob: -11.032698, linearprob: 0.0 Token: Class, logprob: -11.072542, linearprob: 0.0 Token:  set, logprob: -3.6789137e-05, linearprob: 100.0 Token:  research, logprob: -10.741378, linearprob: 0.0 Token:  data, logprob: -11.657573, linearprob: 0.0 Token:  classification, logprob: -12.47602, linearprob: 0.0 Token:  provided, logprob: -13.209107, linearprob: 0.0 Token:  of, logprob: -2.1008714e-06, linearprob: 100.0 Token:  containing, logprob: -13.942362, linearprob: 0.0 Token:  for, logprob: -14.9183235, linearprob: 0.0 Token:  this, logprob: -15.205688, linearprob: 0.0 Token:  the, logprob: -15.615898, linearprob: 0.0 Token:  this, logprob: -1.7120534e-05, linearprob: 100.0 Token:  data, logprob: -11.448185, linearprob: 0.0 Token:  the, logprob: -12.917202, linearprob: 0.0 Token:  methodologies, logprob: -13.10297, linearprob: 0.0 Token:  research, logprob: -13.498447, linearprob: 0.0 Token:  research, logprob: -5.5122365e-07, linearprob: 100.0 Token:  re, logprob: -15.348754, linearprob: 0.0 Token:  res, logprob: -16.3502, linearprob: 0.0 Token:  Research, logprob: -16.394062, linearprob: 0.0 Token:  paper, logprob: -17.266455, linearprob: 0.0 Token:  paper, logprob: -3.3213026e-05, linearprob: 100.0 Token:  papers, logprob: -10.338597, linearprob: 0.0 Token:  pap, logprob: -14.925315, linearprob: 0.0 Token:  p, logprob: -15.797067, linearprob: 0.0 Token: 's, logprob: -15.990962, linearprob: 0.0 Token: 's, logprob: -1.0683535e-05, linearprob: 100.0 Token: ’s, logprob: -11.459912, linearprob: 0.0 Token: \', logprob: -16.82848, linearprob: 0.0 Token: ‘s, logprob: -17.479698, linearprob: 0.0 Token: ´s, logprob: -17.709461, linearprob: 0.0 Token:  data, logprob: -4.3202e-07, linearprob: 100.0 Token:  Data, logprob: -15.79064, linearprob: 0.0 Token:  gathering, logprob: -16.106375, linearprob: 0.0 Token:  , logprob: -17.192507, linearprob: 0.0 Token:  dat, logprob: -18.020086, linearprob: 0.0 Token:  gathering, logprob: -4.334534e-05, linearprob: 100.0 Token: -g, logprob: -10.0921545, linearprob: 0.0 Token:  g, logprob: -14.459168, linearprob: 0.0 Token:  gather, logprob: -14.749743, linearprob: 0.0 Token:  methodologies, logprob: -15.034579, linearprob: 0.0 Token:  methodologies, logprob: -0.00011331789, linearprob: 99.99 Token:  methodology, logprob: -9.295438, linearprob: 0.01 Token:  methods, logprob: -11.159011, linearprob: 0.0 Token:  method, logprob: -12.223573, linearprob: 0.0 Token:  meth, logprob: -13.629386, linearprob: 0.0 Token:  contains, logprob: -0.0002005603, linearprob: 99.98 Token:  contain, logprob: -8.660744, linearprob: 0.02 Token:  includes, logprob: -10.848606, linearprob: 0.0 Token:  contained, logprob: -13.073811, linearprob: 0.0 Token:  consists, logprob: -13.755037, linearprob: 0.0 Token:  brainstorm, logprob: -0.031570766, linearprob: 96.89 Token:  interviews, logprob: -3.725361, linearprob: 2.41 Token:  focus, logprob: -5.2932916, linearprob: 0.5 Token: :, logprob: -7.780371, linearprob: 0.04 Token:  think, logprob: -7.783694, linearprob: 0.04 Token: ing, logprob: -6.6306106e-06, linearprob: 100.0 Token: ,, logprob: -13.195327, linearprob: 0.0 Token: ings, logprob: -13.607234, linearprob: 0.0 Token: in, logprob: -13.799485, linearprob: 0.0 Token:  and, logprob: -14.509248, linearprob: 0.0 Token: ,, logprob: -0.029228026, linearprob: 97.12 Token:  and, logprob: -3.5476148, linearprob: 2.88 Token:  sessions, logprob: -12.2797165, linearprob: 0.0 Token: ,f, logprob: -12.954784, linearprob: 0.0 Token: /f, logprob: -13.742851, linearprob: 0.0 Token:  focus, logprob: -0.25256732, linearprob: 77.68 Token:  interviews, logprob: -1.530365, linearprob: 21.65 Token:  conceptual, logprob: -5.623577, linearprob: 0.36 Token:  concept, logprob: -6.244047, linearprob: 0.19 Token:  focused, logprob: -8.490677, linearprob: 0.02 Token:  groups, logprob: -7.493431e-05, linearprob: 99.99 Token:  group, logprob: -9.501075, linearprob: 0.01 Token:  Groups, logprob: -17.097795, linearprob: 0.0 Token: -groups, logprob: -17.89897, linearprob: 0.0 Token: groups, logprob: -18.468727, linearprob: 0.0 Token: ,, logprob: -9.729906e-06, linearprob: 100.0 Token:  and, logprob: -12.036627, linearprob: 0.0 Token: ., logprob: -13.543865, linearprob: 0.0 Token:  interviews, logprob: -13.61545, linearprob: 0.0 Token:  (, logprob: -14.26775, linearprob: 0.0 Token:  interviews, logprob: -0.03577929, linearprob: 96.49 Token:  conceptual, logprob: -3.6511548, linearprob: 2.6 Token:  and, logprob: -5.9645247, linearprob: 0.26 Token:  participant, logprob: -6.4682517, linearprob: 0.16 Token:  question, logprob: -6.470899, linearprob: 0.15 Token: ,, logprob: -0.0003576304, linearprob: 99.96 Token: ., logprob: -8.380153, linearprob: 0.02 Token:  (, logprob: -9.232373, linearprob: 0.01 Token:  and, logprob: -10.492484, linearprob: 0.0 Token:  with, logprob: -14.064261, linearprob: 0.0 Token:  question, logprob: -0.25890884, linearprob: 77.19 Token:  and, logprob: -2.051744, linearprob: 12.85 Token:  conceptual, logprob: -3.1429687, linearprob: 4.32 Token:  think, logprob: -3.9961047, linearprob: 1.84 Token:  concept, logprob: -4.170196, linearprob: 1.54 Token: naires, logprob: -1.1756368e-05, linearprob: 100.0 Token: aires, logprob: -11.588385, linearprob: 0.0 Token: n, logprob: -13.730364, linearprob: 0.0 Token: aries, logprob: -14.391722, linearprob: 0.0 Token: na, logprob: -14.682974, linearprob: 0.0 Token: ,, logprob: -0.00077508535, linearprob: 99.92 Token: ., logprob: -7.2043576, linearprob: 0.07 Token:  and, logprob: -10.546245, linearprob: 0.0 Token:  as, logprob: -13.222545, linearprob: 0.0 Token: .\n, logprob: -14.574552, linearprob: 0.0 Token:  conceptual, logprob: -0.4862594, linearprob: 61.49 Token:  and, logprob: -1.0890121, linearprob: 33.65 Token:  think, logprob: -3.8144665, linearprob: 2.2 Token:  shadow, logprob: -4.52227, linearprob: 1.09 Token:  concept, logprob: -5.6782455, linearprob: 0.34 Token:  modeling, logprob: -5.3596854e-05, linearprob: 99.99 Token:  modelling, logprob: -9.852671, linearprob: 0.01 Token:  model, logprob: -14.732714, linearprob: 0.0 Token:  mapping, logprob: -16.117231, linearprob: 0.0 Token:  , logprob: -16.151976, linearprob: 0.0 Token: ,, logprob: -0.0026115673, linearprob: 99.74 Token: ., logprob: -5.963286, linearprob: 0.26 Token:  and, logprob: -10.312889, linearprob: 0.0 Token: .\n, logprob: -14.025575, linearprob: 0.0 Token:  as, logprob: -14.780244, linearprob: 0.0 Token:  work, logprob: -0.5846255, linearprob: 55.73 Token:  and, logprob: -1.7003653, linearprob: 18.26 Token:  think, logprob: -2.322611, linearprob: 9.8 Token:  document, logprob: -2.3404067, linearprob: 9.63 Token:  participant, logprob: -3.5036414, linearprob: 3.01 Token:  di, logprob: -1.9816675e-06, linearprob: 100.0 Token:  diary, logprob: -13.536743, linearprob: 0.0 Token:  d, logprob: -14.751003, linearprob: 0.0 Token: di, logprob: -15.98506, linearprob: 0.0 Token:  Di, logprob: -17.319347, linearprob: 0.0 Token: aries, logprob: -3.1737043e-06, linearprob: 100.0 Token: ar, logprob: -13.761342, linearprob: 0.0 Token: aris, logprob: -14.159806, linearprob: 0.0 Token: arie, logprob: -14.891789, linearprob: 0.0 Token: ari, logprob: -15.03626, linearprob: 0.0 Token: ,, logprob: -0.0009375471, linearprob: 99.91 Token: ., logprob: -6.985341, linearprob: 0.09 Token:  and, logprob: -11.551814, linearprob: 0.0 Token: .\n, logprob: -14.653918, linearprob: 0.0 Token:  think, logprob: -15.129402, linearprob: 0.0 Token:  think, logprob: -0.1453532, linearprob: 86.47 Token:  and, logprob: -2.4547462, linearprob: 8.59 Token:  shadow, logprob: -3.7453861, linearprob: 2.36 Token:  participant, logprob: -4.177845, linearprob: 1.53 Token:  fly, logprob: -5.299284, linearprob: 0.5 Token: -al, logprob: -4.604148e-06, linearprob: 100.0 Token:  aloud, logprob: -12.709456, linearprob: 0.0 Token: -, logprob: -14.000867, linearprob: 0.0 Token: ‐, logprob: -14.715608, linearprob: 0.0 Token: -out, logprob: -16.25582, linearprob: 0.0 Token: oud, logprob: -1.147242e-06, linearprob: 100.0 Token: ound, logprob: -13.961028, linearprob: 0.0 Token: ong, logprob: -16.82441, linearprob: 0.0 Token: oad, logprob: -16.874434, linearprob: 0.0 Token: ough, logprob: -16.95118, linearprob: 0.0 Token:  sessions, logprob: -4.8425554e-06, linearprob: 100.0 Token: sessions, logprob: -13.460107, linearprob: 0.0 Token:  session, logprob: -13.622117, linearprob: 0.0 Token: s, logprob: -14.030829, linearprob: 0.0 Token: ,, logprob: -14.4276495, linearprob: 0.0 Token: ,, logprob: -0.012616654, linearprob: 98.75 Token: ., logprob: -4.3818774, linearprob: 1.25 Token:  and, logprob: -10.6381235, linearprob: 0.0 Token: .\n, logprob: -12.465611, linearprob: 0.0 Token: .\n\n, logprob: -13.121401, linearprob: 0.0 Token:  shadow, logprob: -0.6301888, linearprob: 53.25 Token:  participant, logprob: -1.7509545, linearprob: 17.36 Token:  and, logprob: -1.792777, linearprob: 16.65 Token:  document, logprob: -2.588666, linearprob: 7.51 Token:  fly, logprob: -3.8701525, linearprob: 2.09 Token: ing, logprob: -3.0545007e-06, linearprob: 100.0 Token: in, logprob: -13.858207, linearprob: 0.0 Token: ,, logprob: -13.906783, linearprob: 0.0 Token: ings, logprob: -15.200267, linearprob: 0.0 Token: ling, logprob: -15.53357, linearprob: 0.0 Token: ,, logprob: -0.00077127083, linearprob: 99.92 Token: ., logprob: -7.202111, linearprob: 0.07 Token:  participant, logprob: -11.11682, linearprob: 0.0 Token:  and, logprob: -11.712621, linearprob: 0.0 Token: .\n, logprob: -14.159057, linearprob: 0.0 Token:  participant, logprob: -0.23444267, linearprob: 79.1 Token:  and, logprob: -2.1532507, linearprob: 11.61 Token:  document, logprob: -3.0181398, linearprob: 4.89 Token:  instrument, logprob: -3.5230875, linearprob: 2.95 Token:  fly, logprob: -4.724845, linearprob: 0.89 Token: -, logprob: -2.6299214e-05, linearprob: 100.0 Token:  observer, logprob: -11.294071, linearprob: 0.0 Token:  observation, logprob: -11.376936, linearprob: 0.0 Token: -ob, logprob: -13.399588, linearprob: 0.0 Token:  observations, logprob: -15.458797, linearprob: 0.0 Token: observer, logprob: -0.0012946429, linearprob: 99.87 Token: observation, logprob: -6.680757, linearprob: 0.13 Token: obs, logprob: -11.112766, linearprob: 0.0 Token:  observer, logprob: -11.798031, linearprob: 0.0 Token: observ, logprob: -12.11908, linearprob: 0.0 Token: ,, logprob: -0.000993153, linearprob: 99.9 Token: ., logprob: -7.038961, linearprob: 0.09 Token:  technique, logprob: -9.506598, linearprob: 0.01 Token:  techniques, logprob: -10.61053, linearprob: 0.0 Token:  and, logprob: -12.436027, linearprob: 0.0 Token:  instrument, logprob: -0.49102488, linearprob: 61.2 Token:  and, logprob: -1.2117757, linearprob: 29.77 Token:  document, logprob: -2.965621, linearprob: 5.15 Token:  fly, logprob: -3.7699556, linearprob: 2.31 Token:  lab, logprob: -4.785445, linearprob: 0.84 Token: ing, logprob: -1.0206721e-05, linearprob: 100.0 Token: ed, logprob: -12.497582, linearprob: 0.0 Token:  systems, logprob: -12.623893, linearprob: 0.0 Token: ating, logprob: -13.511897, linearprob: 0.0 Token:  system, logprob: -13.934943, linearprob: 0.0 Token:  systems, logprob: -4.9186317e-05, linearprob: 100.0 Token:  system, logprob: -9.953249, linearprob: 0.0 Token:  of, logprob: -15.008001, linearprob: 0.0 Token: ,, logprob: -15.240318, linearprob: 0.0 Token:  Systems, logprob: -15.331856, linearprob: 0.0 Token: ,, logprob: -0.0006732359, linearprob: 99.93 Token: ., logprob: -7.3192215, linearprob: 0.07 Token:  and, logprob: -11.863319, linearprob: 0.0 Token:  document, logprob: -14.323822, linearprob: 0.0 Token: .\n, logprob: -14.716118, linearprob: 0.0 Token:  fly, logprob: -0.5960532, linearprob: 55.1 Token:  document, logprob: -1.1313372, linearprob: 32.26 Token:  and, logprob: -2.3340178, linearprob: 9.69 Token:  lab, logprob: -3.5859804, linearprob: 2.77 Token:  , logprob: -7.7047873, linearprob: 0.05 Token:  on, logprob: -0.0006514363, linearprob: 99.93 Token: -on, logprob: -7.339864, linearprob: 0.06 Token:  in, logprob: -14.270358, linearprob: 0.0 Token:  the, logprob: -14.9028635, linearprob: 0.0 Token: ,, logprob: -15.3278475, linearprob: 0.0 Token:  the, logprob: -3.5313153e-06, linearprob: 100.0 Token: -the, logprob: -13.789181, linearprob: 0.0 Token:  a, logprob: -14.260834, linearprob: 0.0 Token:  wall, logprob: -14.638229, linearprob: 0.0 Token:  , logprob: -15.217691, linearprob: 0.0 Token:  wall, logprob: -7.226629e-06, linearprob: 100.0 Token:  Wall, logprob: -12.634632, linearprob: 0.0 Token:  walls, logprob: -13.464941, linearprob: 0.0 Token:  wal, logprob: -14.26483, linearprob: 0.0 Token:  all, logprob: -14.641293, linearprob: 0.0 Token: ,, logprob: -0.001367381, linearprob: 99.86 Token: ., logprob: -7.104597, linearprob: 0.08 Token:  observation, logprob: -8.250637, linearprob: 0.03 Token:  technique, logprob: -8.6009865, linearprob: 0.02 Token:  observations, logprob: -10.427795, linearprob: 0.0 Token:  document, logprob: -0.0361934, linearprob: 96.45 Token:  and, logprob: -3.4702659, linearprob: 3.11 Token:  social, logprob: -6.151632, linearprob: 0.21 Token:  lab, logprob: -6.271669, linearprob: 0.19 Token:  image, logprob: -8.070551, linearprob: 0.03 Token:  corpus, logprob: -1.5451682e-05, linearprob: 100.0 Token:  corps, logprob: -11.6455765, linearprob: 0.0 Token:  corp, logprob: -12.759321, linearprob: 0.0 Token:  corpor, logprob: -12.868418, linearprob: 0.0 Token:  cor, logprob: -14.6975, linearprob: 0.0 Token:  analysis, logprob: -1.7432603e-06, linearprob: 100.0 Token: ,, logprob: -13.99614, linearprob: 0.0 Token:  an, logprob: -15.5585575, linearprob: 0.0 Token:  Analysis, logprob: -15.629654, linearprob: 0.0 Token:  analy, logprob: -15.900478, linearprob: 0.0 Token: ,, logprob: -0.10435372, linearprob: 90.09 Token: ., logprob: -2.3134124, linearprob: 9.89 Token:  and, logprob: -9.094099, linearprob: 0.01 Token: .\n, logprob: -10.369803, linearprob: 0.0 Token: .\n\n, logprob: -11.8763895, linearprob: 0.0 Token:  and, logprob: -0.9814638, linearprob: 37.48 Token:  social, logprob: -0.9871248, linearprob: 37.26 Token:  lab, logprob: -1.6961969, linearprob: 18.34 Token:  image, logprob: -2.6987834, linearprob: 6.73 Token:  audio, logprob: -7.6931796, linearprob: 0.05 Token:  lab, logprob: -0.036181897, linearprob: 96.45 Token:  social, logprob: -3.5782447, linearprob: 2.79 Token:  image, logprob: -5.2812095, linearprob: 0.51 Token:  laboratory, logprob: -6.9398265, linearprob: 0.1 Token:  no, logprob: -7.3247275, linearprob: 0.07 Token:  experiment, logprob: -0.0030448586, linearprob: 99.7 Token:  experiments, logprob: -5.797517, linearprob: 0.3 Token:  experimentation, logprob: -12.4680605, linearprob: 0.0 Token:  Experiment, logprob: -14.747432, linearprob: 0.0 Token:  exper, logprob: -15.4792795, linearprob: 0.0 Token: ., logprob: -0.0017367053, linearprob: 99.83 Token:  methodologies, logprob: -6.8949213, linearprob: 0.1 Token:  methods, logprob: -8.372689, linearprob: 0.02 Token:  techniques, logprob: -8.727947, linearprob: 0.02 Token: <|end|>, logprob: -8.90251, linearprob: 0.01 ","Classification: Data gathering methodology - You are to perform a single or multiclass classification of the provided research paper's data gathering methodology. Classify the data gathering method(s) into any applicable classes in the data methodology gathering taxonomy.Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,"audio,audio,","Token: The, logprob: -0.08223316, linearprob: 92.11 Token: Classification, logprob: -2.5431554, linearprob: 7.86 Token: Based, logprob: -9.348261, linearprob: 0.01 Token: In, logprob: -10.300838, linearprob: 0.0 Token:  The, logprob: -10.856688, linearprob: 0.0 Token:  set, logprob: -0.00024631983, linearprob: 99.98 Token:  data, logprob: -8.473743, linearprob: 0.02 Token:  research, logprob: -10.635954, linearprob: 0.0 Token:  classification, logprob: -11.902727, linearprob: 0.0 Token:  provided, logprob: -12.102182, linearprob: 0.0 Token:  of, logprob: -1.0280384e-06, linearprob: 100.0 Token:  containing, logprob: -14.641307, linearprob: 0.0 Token:  data, logprob: -15.783759, linearprob: 0.0 Token:  for, logprob: -16.029243, linearprob: 0.0 Token:  the, logprob: -16.333767, linearprob: 0.0 Token:  this, logprob: -0.0006056951, linearprob: 99.94 Token:  data, logprob: -7.4232664, linearprob: 0.06 Token:  methodologies, logprob: -12.332324, linearprob: 0.0 Token:  the, logprob: -12.678248, linearprob: 0.0 Token:  research, logprob: -14.278302, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.053827, linearprob: 0.0 Token:  res, logprob: -16.959114, linearprob: 0.0 Token:  researched, logprob: -17.28955, linearprob: 0.0 Token:  Research, logprob: -17.309303, linearprob: 0.0 Token:  paper, logprob: -2.0100624e-05, linearprob: 100.0 Token:  papers, logprob: -10.852071, linearprob: 0.0 Token:  pap, logprob: -15.02909, linearprob: 0.0 Token:  p, logprob: -15.618962, linearprob: 0.0 Token: 's, logprob: -16.227264, linearprob: 0.0 Token: 's, logprob: -1.4378848e-05, linearprob: 100.0 Token: ’s, logprob: -11.162059, linearprob: 0.0 Token: \', logprob: -16.556805, linearprob: 0.0 Token: ´s, logprob: -17.524569, linearprob: 0.0 Token: ‘s, logprob: -17.564531, linearprob: 0.0 Token:  data, logprob: -5.5122365e-07, linearprob: 100.0 Token:  gathering, logprob: -15.215117, linearprob: 0.0 Token:  Data, logprob: -15.468112, linearprob: 0.0 Token:  , logprob: -16.87208, linearprob: 0.0 Token:  dat, logprob: -17.718897, linearprob: 0.0 Token:  gathering, logprob: -0.0002585978, linearprob: 99.97 Token: -g, logprob: -8.265231, linearprob: 0.03 Token:  g, logprob: -14.660535, linearprob: 0.0 Token:  gather, logprob: -14.853164, linearprob: 0.0 Token:  gathered, logprob: -15.927342, linearprob: 0.0 Token:  methodologies, logprob: -3.333223e-05, linearprob: 100.0 Token:  methodology, logprob: -10.60885, linearprob: 0.0 Token:  method, logprob: -12.658238, linearprob: 0.0 Token:  meth, logprob: -13.015428, linearprob: 0.0 Token:  methods, logprob: -13.758082, linearprob: 0.0 Token:  contains, logprob: -0.00022010603, linearprob: 99.98 Token:  contain, logprob: -8.615542, linearprob: 0.02 Token:  includes, logprob: -10.418891, linearprob: 0.0 Token:  contained, logprob: -13.17969, linearprob: 0.0 Token:  consists, logprob: -13.228972, linearprob: 0.0 Token:  brainstorm, logprob: -0.079564, linearprob: 92.35 Token:  interviews, logprob: -2.9536848, linearprob: 5.21 Token:  focus, logprob: -4.026122, linearprob: 1.78 Token: :, logprob: -6.066945, linearprob: 0.23 Token: :\n, logprob: -7.6573668, linearprob: 0.05 Token: ing, logprob: -6.1537958e-06, linearprob: 100.0 Token: ,, logprob: -13.163956, linearprob: 0.0 Token: ings, logprob: -13.634266, linearprob: 0.0 Token: in, logprob: -13.743368, linearprob: 0.0 Token:  and, logprob: -14.815695, linearprob: 0.0 Token: ,, logprob: -0.015270142, linearprob: 98.48 Token:  and, logprob: -4.204241, linearprob: 1.49 Token:  sessions, logprob: -8.523956, linearprob: 0.02 Token:  focus, logprob: -12.128782, linearprob: 0.0 Token:  discussions, logprob: -12.356882, linearprob: 0.0 Token:  focus, logprob: -0.21996933, linearprob: 80.25 Token:  interviews, logprob: -1.7537713, linearprob: 17.31 Token:  conceptual, logprob: -3.874064, linearprob: 2.08 Token:  concept, logprob: -6.028464, linearprob: 0.24 Token:  focused, logprob: -8.149789, linearprob: 0.03 Token:  groups, logprob: -5.419287e-05, linearprob: 99.99 Token:  group, logprob: -9.825135, linearprob: 0.01 Token:  Groups, logprob: -17.3108, linearprob: 0.0 Token: -groups, logprob: -18.188951, linearprob: 0.0 Token: groups, logprob: -18.640083, linearprob: 0.0 Token: ,, logprob: -0.00013476722, linearprob: 99.99 Token: ., logprob: -9.440768, linearprob: 0.01 Token:  and, logprob: -9.897206, linearprob: 0.01 Token:  interviews, logprob: -13.260992, linearprob: 0.0 Token:  (, logprob: -13.465094, linearprob: 0.0 Token:  interviews, logprob: -0.07293193, linearprob: 92.97 Token:  conceptual, logprob: -2.8659523, linearprob: 5.69 Token:  and, logprob: -5.5780187, linearprob: 0.38 Token:  concept, logprob: -6.143286, linearprob: 0.21 Token:  participant, logprob: -6.3753934, linearprob: 0.17 Token: ,, logprob: -0.0011958249, linearprob: 99.88 Token: ., logprob: -6.8468475, linearprob: 0.11 Token:  and, logprob: -9.516808, linearprob: 0.01 Token:  (, logprob: -9.869379, linearprob: 0.01 Token:  with, logprob: -12.808998, linearprob: 0.0 Token:  question, logprob: -0.36509803, linearprob: 69.41 Token:  and, logprob: -1.5895891, linearprob: 20.4 Token:  conceptual, logprob: -2.8923426, linearprob: 5.54 Token:  concept, logprob: -4.064582, linearprob: 1.72 Token:  shadow, logprob: -4.8872232, linearprob: 0.75 Token: naires, logprob: -1.6762922e-05, linearprob: 100.0 Token: aires, logprob: -11.185017, linearprob: 0.0 Token: n, logprob: -13.4672985, linearprob: 0.0 Token: aries, logprob: -14.479137, linearprob: 0.0 Token: na, logprob: -14.608515, linearprob: 0.0 Token: ,, logprob: -0.001578898, linearprob: 99.84 Token: ., logprob: -6.492328, linearprob: 0.15 Token:  and, logprob: -9.862844, linearprob: 0.01 Token:  as, logprob: -12.830252, linearprob: 0.0 Token:  concept, logprob: -13.273506, linearprob: 0.0 Token:  conceptual, logprob: -0.42079005, linearprob: 65.65 Token:  and, logprob: -1.151152, linearprob: 31.63 Token:  think, logprob: -4.481034, linearprob: 1.13 Token:  shadow, logprob: -5.0734143, linearprob: 0.63 Token:  concept, logprob: -5.9461427, linearprob: 0.26 Token:  modeling, logprob: -5.0974373e-05, linearprob: 99.99 Token:  modelling, logprob: -9.902417, linearprob: 0.01 Token:  model, logprob: -14.875735, linearprob: 0.0 Token:  , logprob: -15.938216, linearprob: 0.0 Token:  mapping, logprob: -16.066298, linearprob: 0.0 Token: ,, logprob: -0.010753734, linearprob: 98.93 Token: ., logprob: -4.5460625, linearprob: 1.06 Token:  and, logprob: -9.476162, linearprob: 0.01 Token: .\n, logprob: -13.013476, linearprob: 0.0 Token: .\n\n, logprob: -13.28271, linearprob: 0.0 Token:  work, logprob: -0.6061616, linearprob: 54.54 Token:  and, logprob: -1.626658, linearprob: 19.66 Token:  document, logprob: -2.0033288, linearprob: 13.49 Token:  think, logprob: -2.7280164, linearprob: 6.53 Token:  participant, logprob: -3.6137376, linearprob: 2.7 Token:  di, logprob: -1.2664457e-06, linearprob: 100.0 Token:  diary, logprob: -14.067973, linearprob: 0.0 Token:  d, logprob: -15.393071, linearprob: 0.0 Token: di, logprob: -16.110424, linearprob: 0.0 Token:  Di, logprob: -17.393787, linearprob: 0.0 Token: aries, logprob: -5.080963e-06, linearprob: 100.0 Token: ar, logprob: -13.442811, linearprob: 0.0 Token: aris, logprob: -13.766053, linearprob: 0.0 Token: arie, logprob: -14.120345, linearprob: 0.0 Token: ari, logprob: -14.256492, linearprob: 0.0 Token: ,, logprob: -0.002298705, linearprob: 99.77 Token: ., logprob: -6.0872197, linearprob: 0.23 Token:  and, logprob: -10.835011, linearprob: 0.0 Token: .\n, logprob: -13.775637, linearprob: 0.0 Token: .\n\n, logprob: -14.256245, linearprob: 0.0 Token:  think, logprob: -0.154034, linearprob: 85.72 Token:  and, logprob: -2.2194788, linearprob: 10.87 Token:  shadow, logprob: -3.9457228, linearprob: 1.93 Token:  participant, logprob: -4.7669377, linearprob: 0.85 Token:  fly, logprob: -5.736714, linearprob: 0.32 Token: -al, logprob: -5.4385737e-06, linearprob: 100.0 Token:  aloud, logprob: -12.313751, linearprob: 0.0 Token: -, logprob: -14.511417, linearprob: 0.0 Token: ‐, logprob: -15.275433, linearprob: 0.0 Token: -out, logprob: -16.66373, linearprob: 0.0 Token: oud, logprob: -5.4385737e-06, linearprob: 100.0 Token: ound, logprob: -12.2263565, linearprob: 0.0 Token: ong, logprob: -15.129732, linearprob: 0.0 Token: oad, logprob: -16.540543, linearprob: 0.0 Token: ould, logprob: -16.54478, linearprob: 0.0 Token:  sessions, logprob: -3.0545007e-06, linearprob: 100.0 Token: sessions, logprob: -13.651654, linearprob: 0.0 Token:  session, logprob: -14.403491, linearprob: 0.0 Token: s, logprob: -14.409205, linearprob: 0.0 Token: ,, logprob: -15.286651, linearprob: 0.0 Token: ,, logprob: -0.01621525, linearprob: 98.39 Token: ., logprob: -4.1350164, linearprob: 1.6 Token:  and, logprob: -9.58088, linearprob: 0.01 Token: .\n, logprob: -12.870819, linearprob: 0.0 Token: .\n\n, logprob: -13.202037, linearprob: 0.0 Token:  shadow, logprob: -0.6045222, linearprob: 54.63 Token:  and, logprob: -1.7513213, linearprob: 17.35 Token:  document, logprob: -1.9616313, linearprob: 14.06 Token:  participant, logprob: -2.3297744, linearprob: 9.73 Token:  instrument, logprob: -4.0811057, linearprob: 1.69 Token: ing, logprob: -2.1008714e-06, linearprob: 100.0 Token: in, logprob: -14.212456, linearprob: 0.0 Token: ,, logprob: -14.521963, linearprob: 0.0 Token: ings, logprob: -15.30051, linearprob: 0.0 Token: ling, logprob: -15.821108, linearprob: 0.0 Token: ,, logprob: -0.0010005363, linearprob: 99.9 Token: ., logprob: -6.9515295, linearprob: 0.1 Token:  and, logprob: -10.745721, linearprob: 0.0 Token:  participant, logprob: -11.026917, linearprob: 0.0 Token: Participant, logprob: -13.915977, linearprob: 0.0 Token:  participant, logprob: -0.25726703, linearprob: 77.32 Token:  and, logprob: -2.0331154, linearprob: 13.09 Token:  document, logprob: -2.8365822, linearprob: 5.86 Token:  instrument, logprob: -3.6982288, linearprob: 2.48 Token:  fly, logprob: -4.941799, linearprob: 0.71 Token: -, logprob: -2.3080716e-05, linearprob: 100.0 Token:  observer, logprob: -11.371693, linearprob: 0.0 Token:  observation, logprob: -11.615305, linearprob: 0.0 Token: -ob, logprob: -13.361534, linearprob: 0.0 Token: –, logprob: -15.421995, linearprob: 0.0 Token: observer, logprob: -0.0011910604, linearprob: 99.88 Token: observation, logprob: -6.7658243, linearprob: 0.12 Token: obs, logprob: -11.052079, linearprob: 0.0 Token:  observer, logprob: -12.0003, linearprob: 0.0 Token: observations, logprob: -12.161595, linearprob: 0.0 Token: ,, logprob: -0.00080914825, linearprob: 99.92 Token: ., logprob: -7.2942433, linearprob: 0.07 Token:  technique, logprob: -9.2850685, linearprob: 0.01 Token:  techniques, logprob: -11.255936, linearprob: 0.0 Token:  observation, logprob: -11.752363, linearprob: 0.0 Token:  instrument, logprob: -0.55126804, linearprob: 57.62 Token:  and, logprob: -1.0766664, linearprob: 34.07 Token:  document, logprob: -2.9996705, linearprob: 4.98 Token:  fly, logprob: -4.0100713, linearprob: 1.81 Token:  lab, logprob: -4.9077764, linearprob: 0.74 Token: ing, logprob: -8.89548e-06, linearprob: 100.0 Token: ed, logprob: -12.6270075, linearprob: 0.0 Token:  systems, logprob: -13.072181, linearprob: 0.0 Token: ating, logprob: -13.388465, linearprob: 0.0 Token:  system, logprob: -14.224284, linearprob: 0.0 Token:  systems, logprob: -4.1914896e-05, linearprob: 100.0 Token:  system, logprob: -10.136765, linearprob: 0.0 Token:  of, logprob: -14.607412, linearprob: 0.0 Token:  Systems, logprob: -14.7600765, linearprob: 0.0 Token: ,, logprob: -15.017122, linearprob: 0.0 Token: ,, logprob: -0.0006358316, linearprob: 99.94 Token: ., logprob: -7.387484, linearprob: 0.06 Token:  and, logprob: -11.269628, linearprob: 0.0 Token:  document, logprob: -13.997648, linearprob: 0.0 Token: /document, logprob: -14.978595, linearprob: 0.0 Token:  fly, logprob: -0.6814739, linearprob: 50.59 Token:  document, logprob: -1.0825074, linearprob: 33.87 Token:  and, logprob: -2.054292, linearprob: 12.82 Token:  lab, logprob: -3.6631958, linearprob: 2.57 Token:  , logprob: -7.9994707, linearprob: 0.03 Token:  on, logprob: -0.00018911675, linearprob: 99.98 Token: -on, logprob: -8.577044, linearprob: 0.02 Token:  in, logprob: -15.330589, linearprob: 0.0 Token:  the, logprob: -15.872074, linearprob: 0.0 Token:  of, logprob: -16.45508, linearprob: 0.0 Token:  the, logprob: -2.1008714e-06, linearprob: 100.0 Token: -the, logprob: -14.158581, linearprob: 0.0 Token:  a, logprob: -14.529381, linearprob: 0.0 Token:  wall, logprob: -15.3445835, linearprob: 0.0 Token: the, logprob: -15.766472, linearprob: 0.0 Token:  wall, logprob: -6.0345924e-06, linearprob: 100.0 Token:  Wall, logprob: -12.844002, linearprob: 0.0 Token:  walls, logprob: -13.512894, linearprob: 0.0 Token:  wal, logprob: -14.366386, linearprob: 0.0 Token:  ball, logprob: -14.923182, linearprob: 0.0 Token: ,, logprob: -0.00012666137, linearprob: 99.99 Token: ., logprob: -9.061423, linearprob: 0.01 Token:  and, logprob: -12.51397, linearprob: 0.0 Token:  technique, logprob: -12.98463, linearprob: 0.0 Token:  document, logprob: -13.714166, linearprob: 0.0 Token:  document, logprob: -0.03366633, linearprob: 96.69 Token:  and, logprob: -3.5353563, linearprob: 2.91 Token:  lab, logprob: -6.0976005, linearprob: 0.22 Token:  social, logprob: -6.5356803, linearprob: 0.15 Token:  image, logprob: -8.843721, linearprob: 0.01 Token:  corpus, logprob: -2.0935051e-05, linearprob: 100.0 Token:  corps, logprob: -11.36097, linearprob: 0.0 Token:  corp, logprob: -12.458481, linearprob: 0.0 Token:  corpor, logprob: -12.571926, linearprob: 0.0 Token:  cor, logprob: -14.137676, linearprob: 0.0 Token:  analysis, logprob: -1.3856493e-06, linearprob: 100.0 Token: ,, logprob: -14.222934, linearprob: 0.0 Token:  Analysis, logprob: -15.8819685, linearprob: 0.0 Token:  an, logprob: -15.94694, linearprob: 0.0 Token:  and, logprob: -16.1411, linearprob: 0.0 Token: ,, logprob: -0.094872445, linearprob: 90.95 Token: ., logprob: -2.4048834, linearprob: 9.03 Token:  and, logprob: -8.763133, linearprob: 0.02 Token: .\n, logprob: -10.68603, linearprob: 0.0 Token: <|end|>, logprob: -11.047844, linearprob: 0.0 Token:  social, logprob: -0.89453983, linearprob: 40.88 Token:  and, logprob: -1.0109854, linearprob: 36.39 Token:  lab, logprob: -1.7761583, linearprob: 16.93 Token:  image, logprob: -2.8760023, linearprob: 5.64 Token:  as, logprob: -7.957425, linearprob: 0.04 Token:  media, logprob: -9.0883464e-07, linearprob: 100.0 Token:  medial, logprob: -15.383403, linearprob: 0.0 Token: -media, logprob: -15.53231, linearprob: 0.0 Token: media, logprob: -15.783687, linearprob: 0.0 Token:  med, logprob: -16.465881, linearprob: 0.0 Token:  analysis, logprob: -1.7432603e-06, linearprob: 100.0 Token: ,, logprob: -14.317213, linearprob: 0.0 Token:  an, logprob: -14.949139, linearprob: 0.0 Token:  Analysis, logprob: -15.296215, linearprob: 0.0 Token:  analy, logprob: -16.061121, linearprob: 0.0 Token: ,, logprob: -0.008156956, linearprob: 99.19 Token: ., logprob: -4.8225527, linearprob: 0.8 Token:  and, logprob: -9.762729, linearprob: 0.01 Token: ,image, logprob: -11.56615, linearprob: 0.0 Token: .\n, logprob: -12.707153, linearprob: 0.0 Token:  image, logprob: -0.16810349, linearprob: 84.53 Token:  and, logprob: -1.9254963, linearprob: 14.58 Token:  lab, logprob: -4.7311053, linearprob: 0.88 Token:  as, logprob: -9.887607, linearprob: 0.01 Token:  audio, logprob: -10.541905, linearprob: 0.0 Token: ,, logprob: -0.3094278, linearprob: 73.39 Token:  analysis, logprob: -1.5778031, linearprob: 20.64 Token: /audio, logprob: -3.2272477, linearprob: 3.97 Token:  audio, logprob: -4.6702447, linearprob: 0.94 Token: -a, logprob: -5.1078095, linearprob: 0.6 Token:  audio, logprob: -2.749125e-05, linearprob: 100.0 Token:  and, logprob: -10.629716, linearprob: 0.0 Token:  video, logprob: -12.882472, linearprob: 0.0 Token:  Audio, logprob: -15.908755, linearprob: 0.0 Token: audio, logprob: -15.933301, linearprob: 0.0 Token: ,, logprob: -0.00064714864, linearprob: 99.94 Token:  and, logprob: -7.3465333, linearprob: 0.06 Token: ,and, logprob: -13.787202, linearprob: 0.0 Token: ,\n, logprob: -14.904607, linearprob: 0.0 Token:  &, logprob: -15.600175, linearprob: 0.0 Token:  and, logprob: -0.00022666223, linearprob: 99.98 Token:  video, logprob: -8.399019, linearprob: 0.02 Token:  &, logprob: -14.653687, linearprob: 0.0 Token:  , logprob: -15.087425, linearprob: 0.0 Token:  audio, logprob: -15.731176, linearprob: 0.0 Token:  video, logprob: -3.650519e-06, linearprob: 100.0 Token:  vide, logprob: -14.000736, linearprob: 0.0 Token: ,, logprob: -14.3011, linearprob: 0.0 Token:  Video, logprob: -14.602354, linearprob: 0.0 Token:  videos, logprob: -14.787931, linearprob: 0.0 Token:  library, logprob: -8.304016e-05, linearprob: 99.99 Token:  analysis, logprob: -9.91413, linearprob: 0.0 Token:  Library, logprob: -11.397473, linearprob: 0.0 Token: ,, logprob: -11.429229, linearprob: 0.0 Token:  libraries, logprob: -12.933316, linearprob: 0.0 Token:  analysis, logprob: -7.58424e-06, linearprob: 100.0 Token: ,, logprob: -12.563877, linearprob: 0.0 Token: ., logprob: -14.24119, linearprob: 0.0 Token:  analyses, logprob: -14.390812, linearprob: 0.0 Token: analysis, logprob: -14.570604, linearprob: 0.0 Token: ., logprob: -0.3678489, linearprob: 69.22 Token: ,, logprob: -1.1807113, linearprob: 30.71 Token:  and, logprob: -8.158805, linearprob: 0.03 Token: .\n, logprob: -9.06473, linearprob: 0.01 Token:  lab, logprob: -9.444835, linearprob: 0.01 ","Classification: Data gathering methodology - You are to perform a single or multiclass classification of the provided research paper's data gathering methodology. What are the data gathering methodologies of the provided research paper?Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: classification: data gathering methodology - the set of this research paper's data gathering methodologies contains brainstorming, focus groups, interviews, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, document corpus analysis, lab experiment.,"Token: Classification, logprob: -0.038841736, linearprob: 96.19 Token: The, logprob: -3.291388, linearprob: 3.72 Token: Based, logprob: -8.876868, linearprob: 0.01 Token:  Classification, logprob: -8.934939, linearprob: 0.01 Token: Class, logprob: -9.023892, linearprob: 0.01 Token: :, logprob: -0.0010623443, linearprob: 99.89 Token: :\n, logprob: -7.0321236, linearprob: 0.09 Token: :\n\n, logprob: -9.792994, linearprob: 0.01 Token:  result, logprob: -10.07597, linearprob: 0.0 Token:  Result, logprob: -10.444382, linearprob: 0.0 Token:  Data, logprob: -0.00074006146, linearprob: 99.93 Token:  \n\n, logprob: -7.659094, linearprob: 0.05 Token:  \n, logprob: -8.67753, linearprob: 0.02 Token:  The, logprob: -10.552954, linearprob: 0.0 Token: Data, logprob: -10.814512, linearprob: 0.0 Token:  gathering, logprob: -3.1782583e-05, linearprob: 100.0 Token:  Gathering, logprob: -10.554864, linearprob: 0.0 Token: -g, logprob: -12.315864, linearprob: 0.0 Token:  gather, logprob: -14.960958, linearprob: 0.0 Token:  gathered, logprob: -15.699786, linearprob: 0.0 Token:  methodology, logprob: -0.0005912788, linearprob: 99.94 Token:  methodologies, logprob: -7.4604435, linearprob: 0.06 Token:  method, logprob: -11.921257, linearprob: 0.0 Token:  Method, logprob: -12.285225, linearprob: 0.0 Token:  meth, logprob: -13.28788, linearprob: 0.0 Token:  -, logprob: -0.008132718, linearprob: 99.19 Token: \n\n, logprob: -5.501734, linearprob: 0.41 Token: \n, logprob: -6.250887, linearprob: 0.19 Token:  \n\n, logprob: -7.639515, linearprob: 0.05 Token:  -\n\n, logprob: -8.00322, linearprob: 0.03 Token:  The, logprob: -0.40897232, linearprob: 66.43 Token:  You, logprob: -1.1406999, linearprob: 31.96 Token:  \n, logprob: -5.0774064, linearprob: 0.62 Token:  \n\n, logprob: -5.160895, linearprob: 0.57 Token:  , logprob: -5.787295, linearprob: 0.31 Token:  set, logprob: -0.0012422557, linearprob: 99.88 Token:  data, logprob: -6.964192, linearprob: 0.09 Token:  research, logprob: -8.47701, linearprob: 0.02 Token:  provided, logprob: -9.739671, linearprob: 0.01 Token:  methodologies, logprob: -11.963685, linearprob: 0.0 Token:  of, logprob: -3.650519e-06, linearprob: 100.0 Token:  containing, logprob: -13.518116, linearprob: 0.0 Token:  for, logprob: -14.406559, linearprob: 0.0 Token:  data, logprob: -14.7017765, linearprob: 0.0 Token:  , logprob: -14.975172, linearprob: 0.0 Token:  this, logprob: -0.00012225083, linearprob: 99.99 Token:  data, logprob: -9.234268, linearprob: 0.01 Token:  methodologies, logprob: -11.464533, linearprob: 0.0 Token:  the, logprob: -11.686592, linearprob: 0.0 Token:  research, logprob: -12.299473, linearprob: 0.0 Token:  research, logprob: -9.0883464e-07, linearprob: 100.0 Token:  re, logprob: -15.063218, linearprob: 0.0 Token:  res, logprob: -15.850696, linearprob: 0.0 Token:  researched, logprob: -16.53456, linearprob: 0.0 Token:  paper, logprob: -16.618076, linearprob: 0.0 Token:  paper, logprob: -4.0961266e-05, linearprob: 100.0 Token:  papers, logprob: -10.284169, linearprob: 0.0 Token:  research, logprob: -12.162187, linearprob: 0.0 Token:  pap, logprob: -14.21012, linearprob: 0.0 Token:  p, logprob: -15.231633, linearprob: 0.0 Token: 's, logprob: -5.550411e-05, linearprob: 99.99 Token: ’s, logprob: -9.8046055, linearprob: 0.01 Token: \', logprob: -15.981319, linearprob: 0.0 Token: ´s, logprob: -16.549452, linearprob: 0.0 Token: ‘s, logprob: -17.345648, linearprob: 0.0 Token:  data, logprob: -5.5122365e-07, linearprob: 100.0 Token:  gathering, logprob: -15.180436, linearprob: 0.0 Token:  Data, logprob: -16.184923, linearprob: 0.0 Token:  , logprob: -16.278614, linearprob: 0.0 Token:  methodologies, logprob: -18.571222, linearprob: 0.0 Token:  gathering, logprob: -5.4669687e-05, linearprob: 99.99 Token: -g, logprob: -9.832595, linearprob: 0.01 Token:  methodologies, logprob: -14.897143, linearprob: 0.0 Token:  g, logprob: -15.450265, linearprob: 0.0 Token:  , logprob: -15.910098, linearprob: 0.0 Token:  methodologies, logprob: -3.23786e-05, linearprob: 100.0 Token:  methodology, logprob: -10.558786, linearprob: 0.0 Token:  method, logprob: -13.458405, linearprob: 0.0 Token:  meth, logprob: -13.691219, linearprob: 0.0 Token:  contains, logprob: -13.756332, linearprob: 0.0 Token:  contains, logprob: -0.00057221355, linearprob: 99.94 Token:  contain, logprob: -7.6251545, linearprob: 0.05 Token:  includes, logprob: -9.636087, linearprob: 0.01 Token:  consists, logprob: -12.291927, linearprob: 0.0 Token:  contained, logprob: -12.49519, linearprob: 0.0 Token:  brainstorm, logprob: -0.1259338, linearprob: 88.17 Token:  interviews, logprob: -2.3219368, linearprob: 9.81 Token:  focus, logprob: -4.9592943, linearprob: 0.7 Token:  conceptual, logprob: -6.03337, linearprob: 0.24 Token:  structured, logprob: -6.2458677, linearprob: 0.19 Token: ing, logprob: -6.392203e-06, linearprob: 100.0 Token: ,, logprob: -12.659215, linearprob: 0.0 Token: in, logprob: -13.874562, linearprob: 0.0 Token: ings, logprob: -14.117803, linearprob: 0.0 Token:  and, logprob: -15.189195, linearprob: 0.0 Token: ,, logprob: -0.00014215785, linearprob: 99.99 Token:  and, logprob: -8.932033, linearprob: 0.01 Token:  sessions, logprob: -12.356822, linearprob: 0.0 Token: ,f, logprob: -12.827575, linearprob: 0.0 Token: /f, logprob: -14.249894, linearprob: 0.0 Token:  focus, logprob: -0.5009208, linearprob: 60.6 Token:  interviews, logprob: -1.0510497, linearprob: 34.96 Token:  conceptual, logprob: -3.377294, linearprob: 3.41 Token:  concept, logprob: -5.1023803, linearprob: 0.61 Token:  question, logprob: -6.306957, linearprob: 0.18 Token:  groups, logprob: -7.362307e-05, linearprob: 99.99 Token:  group, logprob: -9.519475, linearprob: 0.01 Token:  Groups, logprob: -15.967004, linearprob: 0.0 Token: -groups, logprob: -17.464481, linearprob: 0.0 Token: groups, logprob: -18.788004, linearprob: 0.0 Token: ,, logprob: -1.1279553e-05, linearprob: 100.0 Token:  interviews, logprob: -13.082533, linearprob: 0.0 Token:  and, logprob: -13.307262, linearprob: 0.0 Token: ., logprob: -13.502541, linearprob: 0.0 Token: ,int, logprob: -13.537548, linearprob: 0.0 Token:  interviews, logprob: -0.23359439, linearprob: 79.17 Token:  conceptual, logprob: -1.6072745, linearprob: 20.04 Token:  concept, logprob: -6.0217776, linearprob: 0.24 Token:  participant, logprob: -6.4808803, linearprob: 0.15 Token:  semi, logprob: -6.734077, linearprob: 0.12 Token: ,, logprob: -0.025293412, linearprob: 97.5 Token:  (, logprob: -3.6940002, linearprob: 2.49 Token: ., logprob: -9.63966, linearprob: 0.01 Token:  with, logprob: -11.834112, linearprob: 0.0 Token:  structured, logprob: -12.105055, linearprob: 0.0 Token:  conceptual, logprob: -0.5908055, linearprob: 55.39 Token:  question, logprob: -0.9362797, linearprob: 39.21 Token:  concept, logprob: -3.684609, linearprob: 2.51 Token:  think, logprob: -4.8499246, linearprob: 0.78 Token:  work, logprob: -5.53056, linearprob: 0.4 Token:  modeling, logprob: -0.00012308525, linearprob: 99.99 Token:  modelling, logprob: -9.008218, linearprob: 0.01 Token:  model, logprob: -14.994714, linearprob: 0.0 Token:  mapping, logprob: -15.678203, linearprob: 0.0 Token:  , logprob: -16.148333, linearprob: 0.0 Token: ,, logprob: -0.00072244136, linearprob: 99.93 Token: ., logprob: -7.256931, linearprob: 0.07 Token:  and, logprob: -11.362488, linearprob: 0.0 Token:  (, logprob: -13.447453, linearprob: 0.0 Token: .\n\n, logprob: -14.065851, linearprob: 0.0 Token:  work, logprob: -0.07075122, linearprob: 93.17 Token:  document, logprob: -3.6348991, linearprob: 2.64 Token:  think, logprob: -4.1917114, linearprob: 1.51 Token:  participant, logprob: -4.347166, linearprob: 1.29 Token:  fly, logprob: -5.1585026, linearprob: 0.58 Token:  di, logprob: -1.504853e-06, linearprob: 100.0 Token:  diary, logprob: -13.722681, linearprob: 0.0 Token:  d, logprob: -15.478292, linearprob: 0.0 Token: di, logprob: -16.28744, linearprob: 0.0 Token:  Di, logprob: -18.741907, linearprob: 0.0 Token: aries, logprob: -4.365741e-06, linearprob: 100.0 Token: ar, logprob: -13.234059, linearprob: 0.0 Token: aris, logprob: -13.828242, linearprob: 0.0 Token: arie, logprob: -14.579885, linearprob: 0.0 Token: ari, logprob: -14.977237, linearprob: 0.0 Token: ,, logprob: -0.00013917775, linearprob: 99.99 Token: ., logprob: -8.951183, linearprob: 0.01 Token:  and, logprob: -11.780378, linearprob: 0.0 Token: ,\n, logprob: -14.56661, linearprob: 0.0 Token: .\n\n, logprob: -15.146255, linearprob: 0.0 Token:  think, logprob: -0.03450729, linearprob: 96.61 Token:  shadow, logprob: -4.261131, linearprob: 1.41 Token:  participant, logprob: -5.219091, linearprob: 0.54 Token:  fly, logprob: -5.321298, linearprob: 0.49 Token:  lab, logprob: -5.7337146, linearprob: 0.32 Token: -al, logprob: -1.0325924e-05, linearprob: 100.0 Token:  aloud, logprob: -11.800696, linearprob: 0.0 Token: -, logprob: -13.14878, linearprob: 0.0 Token: ‐, logprob: -15.076907, linearprob: 0.0 Token: -out, logprob: -15.86077, linearprob: 0.0 Token: oud, logprob: -7.89631e-07, linearprob: 100.0 Token: ound, logprob: -14.524329, linearprob: 0.0 Token: ong, logprob: -15.807052, linearprob: 0.0 Token: ould, logprob: -17.310234, linearprob: 0.0 Token: oad, logprob: -17.66969, linearprob: 0.0 Token:  sessions, logprob: -2.6968896e-06, linearprob: 100.0 Token:  session, logprob: -14.268397, linearprob: 0.0 Token:  protocol, logprob: -14.514804, linearprob: 0.0 Token: sessions, logprob: -14.586072, linearprob: 0.0 Token: s, logprob: -15.113894, linearprob: 0.0 Token: ,, logprob: -0.029649464, linearprob: 97.08 Token: ., logprob: -3.5383375, linearprob: 2.91 Token:  and, logprob: -9.354351, linearprob: 0.01 Token: <|end|>, logprob: -10.586914, linearprob: 0.0 Token: .\n\n, logprob: -11.047847, linearprob: 0.0 Token:  shadow, logprob: -0.662605, linearprob: 51.55 Token:  participant, logprob: -1.3997648, linearprob: 24.67 Token:  document, logprob: -2.283117, linearprob: 10.2 Token:  and, logprob: -2.8135202, linearprob: 6.0 Token:  lab, logprob: -3.0694635, linearprob: 4.64 Token: ing, logprob: -4.246537e-06, linearprob: 100.0 Token: ,, logprob: -13.355589, linearprob: 0.0 Token: in, logprob: -13.704374, linearprob: 0.0 Token: ings, logprob: -14.893089, linearprob: 0.0 Token: i, logprob: -15.420029, linearprob: 0.0 Token: ,, logprob: -0.0064469003, linearprob: 99.36 Token: ., logprob: -5.058079, linearprob: 0.64 Token:  and, logprob: -10.273933, linearprob: 0.0 Token:  participant, logprob: -11.55478, linearprob: 0.0 Token: .\n\n, logprob: -12.059481, linearprob: 0.0 Token:  participant, logprob: -0.62712944, linearprob: 53.41 Token:  document, logprob: -1.7713975, linearprob: 17.01 Token:  instrument, logprob: -1.7915353, linearprob: 16.67 Token:  lab, logprob: -2.9454088, linearprob: 5.26 Token:  and, logprob: -2.9508772, linearprob: 5.23 Token: -, logprob: -7.934485e-05, linearprob: 99.99 Token:  observer, logprob: -9.649171, linearprob: 0.01 Token:  observation, logprob: -11.279796, linearprob: 0.0 Token: -ob, logprob: -13.6783, linearprob: 0.0 Token:  observations, logprob: -15.244975, linearprob: 0.0 Token: observer, logprob: -0.00031806578, linearprob: 99.97 Token: observation, logprob: -8.147375, linearprob: 0.03 Token: obs, logprob: -11.58255, linearprob: 0.0 Token:  observer, logprob: -11.648544, linearprob: 0.0 Token: observ, logprob: -12.470827, linearprob: 0.0 Token: ,, logprob: -0.0023273635, linearprob: 99.77 Token: ., logprob: -6.658379, linearprob: 0.13 Token:  technique, logprob: -7.2586255, linearprob: 0.07 Token:  techniques, logprob: -8.699356, linearprob: 0.02 Token:  observations, logprob: -9.6309395, linearprob: 0.01 Token:  instrument, logprob: -0.78890926, linearprob: 45.43 Token:  and, logprob: -1.0644314, linearprob: 34.49 Token:  document, logprob: -2.2234013, linearprob: 10.82 Token:  lab, logprob: -3.268781, linearprob: 3.81 Token:  image, logprob: -3.9404519, linearprob: 1.94 Token: ing, logprob: -1.27099975e-05, linearprob: 100.0 Token:  systems, logprob: -11.837382, linearprob: 0.0 Token:  system, logprob: -13.256104, linearprob: 0.0 Token: ating, logprob: -13.468324, linearprob: 0.0 Token: ed, logprob: -13.856084, linearprob: 0.0 Token:  systems, logprob: -5.0020742e-05, linearprob: 99.99 Token:  system, logprob: -9.936607, linearprob: 0.0 Token:  of, logprob: -15.154039, linearprob: 0.0 Token:  Systems, logprob: -15.174343, linearprob: 0.0 Token: ,, logprob: -15.375193, linearprob: 0.0 Token: ,, logprob: -0.02745382, linearprob: 97.29 Token: ., logprob: -3.6135402, linearprob: 2.7 Token:  and, logprob: -9.666201, linearprob: 0.01 Token: <|end|>, logprob: -11.084773, linearprob: 0.0 Token: .\n, logprob: -11.463232, linearprob: 0.0 Token:  document, logprob: -0.5522822, linearprob: 57.56 Token:  and, logprob: -1.604021, linearprob: 20.11 Token:  lab, logprob: -2.005083, linearprob: 13.46 Token:  fly, logprob: -2.580553, linearprob: 7.57 Token:  image, logprob: -5.11322, linearprob: 0.6 Token:  corpus, logprob: -1.6166903e-05, linearprob: 100.0 Token:  corps, logprob: -11.806075, linearprob: 0.0 Token:  corpor, logprob: -12.441928, linearprob: 0.0 Token:  corp, logprob: -12.637128, linearprob: 0.0 Token:  analysis, logprob: -14.6095915, linearprob: 0.0 Token:  analysis, logprob: -6.704273e-07, linearprob: 100.0 Token:  an, logprob: -15.862105, linearprob: 0.0 Token: ,, logprob: -15.94328, linearprob: 0.0 Token:  analyses, logprob: -16.274378, linearprob: 0.0 Token:  analy, logprob: -16.298355, linearprob: 0.0 Token: ,, logprob: -0.48740432, linearprob: 61.42 Token: ., logprob: -0.95363855, linearprob: 38.53 Token: .\n, logprob: -8.833958, linearprob: 0.01 Token: .\n\n, logprob: -8.977279, linearprob: 0.01 Token:  and, logprob: -8.996035, linearprob: 0.01 Token:  lab, logprob: -0.4299306, linearprob: 65.06 Token:  and, logprob: -1.1785306, linearprob: 30.77 Token:  image, logprob: -3.7592669, linearprob: 2.33 Token:  social, logprob: -4.110154, linearprob: 1.64 Token:  no, logprob: -7.253579, linearprob: 0.07 Token:  experiment, logprob: -0.002654609, linearprob: 99.73 Token:  experiments, logprob: -5.9346156, linearprob: 0.26 Token:  experimentation, logprob: -12.591719, linearprob: 0.0 Token:  Experiment, logprob: -14.457563, linearprob: 0.0 Token:  exper, logprob: -15.031748, linearprob: 0.0 Token: ., logprob: -0.02605294, linearprob: 97.43 Token: ,, logprob: -3.718823, linearprob: 2.43 Token: <|end|>, logprob: -7.222523, linearprob: 0.07 Token: .\n, logprob: -7.6439033, linearprob: 0.05 Token: .\n\n, logprob: -8.822775, linearprob: 0.01 ","Classification: Data gathering methodology - You are to perform a single or multiclass classification of the provided research paper's data gathering methodology. What are the data gathering methodologies of the provided research paper?Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: the set of this research paper's data gathering methodologies contains brainstorming, focus groups, interviews, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, and lab experiment.,"Token: The, logprob: -0.114386186, linearprob: 89.19 Token: Classification, logprob: -2.2264771, linearprob: 10.79 Token: classification, logprob: -10.71532, linearprob: 0.0 Token: Based, logprob: -10.7858515, linearprob: 0.0 Token: In, logprob: -10.970733, linearprob: 0.0 Token:  set, logprob: -0.00035226622, linearprob: 99.96 Token:  data, logprob: -8.1328535, linearprob: 0.03 Token:  research, logprob: -10.066533, linearprob: 0.0 Token:  classification, logprob: -11.606978, linearprob: 0.0 Token:  provided, logprob: -12.096038, linearprob: 0.0 Token:  of, logprob: -9.0883464e-07, linearprob: 100.0 Token:  containing, logprob: -14.854926, linearprob: 0.0 Token:  data, logprob: -15.705268, linearprob: 0.0 Token:  for, logprob: -16.050505, linearprob: 0.0 Token:  the, logprob: -16.528515, linearprob: 0.0 Token:  this, logprob: -0.00023274162, linearprob: 99.98 Token:  data, logprob: -8.404564, linearprob: 0.02 Token:  methodologies, logprob: -12.216274, linearprob: 0.0 Token:  the, logprob: -12.871294, linearprob: 0.0 Token:  research, logprob: -13.756727, linearprob: 0.0 Token:  research, logprob: -4.3202e-07, linearprob: 100.0 Token:  re, logprob: -15.655138, linearprob: 0.0 Token:  res, logprob: -16.837908, linearprob: 0.0 Token:  Research, logprob: -17.11534, linearprob: 0.0 Token:  researched, logprob: -17.19412, linearprob: 0.0 Token:  paper, logprob: -2.5226382e-05, linearprob: 100.0 Token:  papers, logprob: -10.625673, linearprob: 0.0 Token:  pap, logprob: -14.777973, linearprob: 0.0 Token:  p, logprob: -15.493336, linearprob: 0.0 Token: 's, logprob: -16.159458, linearprob: 0.0 Token: 's, logprob: -1.2352386e-05, linearprob: 100.0 Token: ’s, logprob: -11.321164, linearprob: 0.0 Token: \', logprob: -16.15735, linearprob: 0.0 Token: ´s, logprob: -17.243267, linearprob: 0.0 Token: ‘s, logprob: -17.365063, linearprob: 0.0 Token:  data, logprob: -5.5122365e-07, linearprob: 100.0 Token:  Data, logprob: -15.437943, linearprob: 0.0 Token:  gathering, logprob: -15.473739, linearprob: 0.0 Token:  , logprob: -16.556719, linearprob: 0.0 Token:  dat, logprob: -17.417536, linearprob: 0.0 Token:  gathering, logprob: -0.00034559448, linearprob: 99.97 Token: -g, logprob: -7.975657, linearprob: 0.03 Token:  g, logprob: -14.262362, linearprob: 0.0 Token:  gather, logprob: -14.497927, linearprob: 0.0 Token:  gathered, logprob: -15.773016, linearprob: 0.0 Token:  methodologies, logprob: -2.5107178e-05, linearprob: 100.0 Token:  methodology, logprob: -10.922243, linearprob: 0.0 Token:  method, logprob: -12.8925705, linearprob: 0.0 Token:  meth, logprob: -13.205702, linearprob: 0.0 Token:  methods, logprob: -13.756884, linearprob: 0.0 Token:  contains, logprob: -0.00013917775, linearprob: 99.99 Token:  contain, logprob: -9.152882, linearprob: 0.01 Token:  includes, logprob: -10.615784, linearprob: 0.0 Token:  consists, logprob: -13.223381, linearprob: 0.0 Token:  contained, logprob: -13.381861, linearprob: 0.0 Token:  brainstorm, logprob: -0.04451311, linearprob: 95.65 Token:  interviews, logprob: -3.5260997, linearprob: 2.94 Token:  focus, logprob: -4.5827823, linearprob: 1.02 Token: :, logprob: -6.395311, linearprob: 0.17 Token: :\n, logprob: -7.351505, linearprob: 0.06 Token: ing, logprob: -5.4385737e-06, linearprob: 100.0 Token: ,, logprob: -13.414038, linearprob: 0.0 Token: ings, logprob: -13.609928, linearprob: 0.0 Token: in, logprob: -13.92276, linearprob: 0.0 Token:  and, logprob: -14.611305, linearprob: 0.0 Token: ,, logprob: -0.013607662, linearprob: 98.65 Token:  and, logprob: -4.3132977, linearprob: 1.34 Token:  sessions, logprob: -9.146838, linearprob: 0.01 Token:  focus, logprob: -12.043174, linearprob: 0.0 Token:  discussions, logprob: -12.662026, linearprob: 0.0 Token:  focus, logprob: -0.2236748, linearprob: 79.96 Token:  interviews, logprob: -1.6990662, linearprob: 18.29 Token:  conceptual, logprob: -4.2811146, linearprob: 1.38 Token:  concept, logprob: -6.100546, linearprob: 0.22 Token:  shadow, logprob: -7.919445, linearprob: 0.04 Token:  groups, logprob: -7.064298e-05, linearprob: 99.99 Token:  group, logprob: -9.560968, linearprob: 0.01 Token:  Groups, logprob: -16.923466, linearprob: 0.0 Token: -groups, logprob: -17.789204, linearprob: 0.0 Token: groups, logprob: -18.24357, linearprob: 0.0 Token: ,, logprob: -2.5226382e-05, linearprob: 100.0 Token:  and, logprob: -11.472342, linearprob: 0.0 Token: ., logprob: -11.724007, linearprob: 0.0 Token:  (, logprob: -12.501818, linearprob: 0.0 Token:  interviews, logprob: -13.331587, linearprob: 0.0 Token:  interviews, logprob: -0.062609605, linearprob: 93.93 Token:  conceptual, logprob: -3.0094824, linearprob: 4.93 Token:  participant, logprob: -5.9341784, linearprob: 0.26 Token:  concept, logprob: -6.25776, linearprob: 0.19 Token:  and, logprob: -6.520768, linearprob: 0.15 Token: ,, logprob: -0.000742684, linearprob: 99.93 Token: ., logprob: -7.4407396, linearprob: 0.06 Token:  (, logprob: -9.252849, linearprob: 0.01 Token:  and, logprob: -9.865485, linearprob: 0.01 Token:  with, logprob: -12.305299, linearprob: 0.0 Token:  question, logprob: -0.24887756, linearprob: 77.97 Token:  and, logprob: -2.2270536, linearprob: 10.78 Token:  conceptual, logprob: -2.5794764, linearprob: 7.58 Token:  concept, logprob: -4.342067, linearprob: 1.3 Token:  shadow, logprob: -4.7088118, linearprob: 0.9 Token: naires, logprob: -9.4914985e-06, linearprob: 100.0 Token: aires, logprob: -11.87908, linearprob: 0.0 Token: n, logprob: -13.44227, linearprob: 0.0 Token: aries, logprob: -14.745653, linearprob: 0.0 Token: na, logprob: -14.760063, linearprob: 0.0 Token: ,, logprob: -0.0005429041, linearprob: 99.95 Token: ., logprob: -7.635511, linearprob: 0.05 Token:  and, logprob: -9.769992, linearprob: 0.01 Token:  as, logprob: -13.93436, linearprob: 0.0 Token: .\n, logprob: -15.329331, linearprob: 0.0 Token:  conceptual, logprob: -0.33552954, linearprob: 71.5 Token:  and, logprob: -1.3483012, linearprob: 25.97 Token:  think, logprob: -4.8516006, linearprob: 0.78 Token:  shadow, logprob: -5.1641197, linearprob: 0.57 Token:  concept, logprob: -5.591793, linearprob: 0.37 Token:  modeling, logprob: -4.5491004e-05, linearprob: 100.0 Token:  modelling, logprob: -10.020416, linearprob: 0.0 Token:  model, logprob: -14.811205, linearprob: 0.0 Token:  mapping, logprob: -15.936907, linearprob: 0.0 Token:  , logprob: -16.056753, linearprob: 0.0 Token: ,, logprob: -0.0026817252, linearprob: 99.73 Token: ., logprob: -5.9359436, linearprob: 0.26 Token:  and, logprob: -10.351379, linearprob: 0.0 Token: .\n, logprob: -13.972058, linearprob: 0.0 Token: .\n\n, logprob: -14.58724, linearprob: 0.0 Token:  work, logprob: -0.779612, linearprob: 45.86 Token:  and, logprob: -1.3845048, linearprob: 25.04 Token:  document, logprob: -1.6635137, linearprob: 18.95 Token:  think, logprob: -3.0720887, linearprob: 4.63 Token:  shadow, logprob: -3.7594514, linearprob: 2.33 Token:  di, logprob: -2.1008714e-06, linearprob: 100.0 Token:  diary, logprob: -13.383194, linearprob: 0.0 Token:  d, logprob: -14.945412, linearprob: 0.0 Token: di, logprob: -15.951862, linearprob: 0.0 Token:  Di, logprob: -17.140535, linearprob: 0.0 Token: aries, logprob: -4.365741e-06, linearprob: 100.0 Token: ar, logprob: -13.690476, linearprob: 0.0 Token: aris, logprob: -13.886266, linearprob: 0.0 Token: arie, logprob: -14.308698, linearprob: 0.0 Token: ari, logprob: -14.813529, linearprob: 0.0 Token: ,, logprob: -0.00042090187, linearprob: 99.96 Token: ., logprob: -7.789823, linearprob: 0.04 Token:  and, logprob: -12.072011, linearprob: 0.0 Token: .\n, logprob: -15.199472, linearprob: 0.0 Token:  think, logprob: -15.810763, linearprob: 0.0 Token:  think, logprob: -0.14883845, linearprob: 86.17 Token:  and, logprob: -2.2861543, linearprob: 10.17 Token:  shadow, logprob: -3.9386277, linearprob: 1.95 Token:  participant, logprob: -4.8521466, linearprob: 0.78 Token:  fly, logprob: -5.4348207, linearprob: 0.44 Token: -al, logprob: -4.604148e-06, linearprob: 100.0 Token:  aloud, logprob: -12.5829115, linearprob: 0.0 Token: -, logprob: -14.36354, linearprob: 0.0 Token: ‐, logprob: -15.160968, linearprob: 0.0 Token: -out, logprob: -16.064024, linearprob: 0.0 Token: oud, logprob: -4.604148e-06, linearprob: 100.0 Token: ound, logprob: -12.466262, linearprob: 0.0 Token: ong, logprob: -14.817925, linearprob: 0.0 Token: ould, logprob: -16.428087, linearprob: 0.0 Token: oad, logprob: -16.632881, linearprob: 0.0 Token:  sessions, logprob: -3.888926e-06, linearprob: 100.0 Token: sessions, logprob: -13.675587, linearprob: 0.0 Token: s, logprob: -13.950462, linearprob: 0.0 Token:  session, logprob: -13.978815, linearprob: 0.0 Token: ,, logprob: -14.751505, linearprob: 0.0 Token: ,, logprob: -0.005918485, linearprob: 99.41 Token: ., logprob: -5.136673, linearprob: 0.59 Token:  and, logprob: -10.958969, linearprob: 0.0 Token: .\n, logprob: -12.925082, linearprob: 0.0 Token: .\n\n, logprob: -13.343693, linearprob: 0.0 Token:  shadow, logprob: -0.633244, linearprob: 53.09 Token:  and, logprob: -1.7488141, linearprob: 17.4 Token:  document, logprob: -2.1107202, linearprob: 12.12 Token:  participant, logprob: -2.2907739, linearprob: 10.12 Token:  instrument, logprob: -3.3441205, linearprob: 3.53 Token: ing, logprob: -2.3392786e-06, linearprob: 100.0 Token: in, logprob: -14.158594, linearprob: 0.0 Token: ,, logprob: -14.287418, linearprob: 0.0 Token: ings, logprob: -15.1821, linearprob: 0.0 Token: ling, logprob: -15.727835, linearprob: 0.0 Token: ,, logprob: -0.00027325618, linearprob: 99.97 Token: ., logprob: -8.247642, linearprob: 0.03 Token:  and, logprob: -11.717439, linearprob: 0.0 Token:  participant, logprob: -13.200364, linearprob: 0.0 Token: .\n, logprob: -15.045957, linearprob: 0.0 Token:  participant, logprob: -0.34303346, linearprob: 70.96 Token:  and, logprob: -2.0701778, linearprob: 12.62 Token:  instrument, logprob: -2.6017177, linearprob: 7.41 Token:  document, logprob: -2.6373317, linearprob: 7.16 Token:  fly, logprob: -4.4595056, linearprob: 1.16 Token: -, logprob: -2.5703197e-05, linearprob: 100.0 Token:  observer, logprob: -11.232387, linearprob: 0.0 Token:  observation, logprob: -11.549666, linearprob: 0.0 Token: -ob, logprob: -13.130122, linearprob: 0.0 Token:  observations, logprob: -15.543767, linearprob: 0.0 Token: observer, logprob: -0.0011193805, linearprob: 99.89 Token: observation, logprob: -6.8289547, linearprob: 0.11 Token: obs, logprob: -11.207407, linearprob: 0.0 Token:  observer, logprob: -11.753437, linearprob: 0.0 Token: observations, logprob: -12.208174, linearprob: 0.0 Token: ,, logprob: -0.000636666, linearprob: 99.94 Token: ., logprob: -7.457994, linearprob: 0.06 Token:  technique, logprob: -10.054825, linearprob: 0.0 Token:  and, logprob: -12.2956505, linearprob: 0.0 Token:  techniques, logprob: -12.634672, linearprob: 0.0 Token:  instrument, logprob: -0.73075414, linearprob: 48.15 Token:  and, logprob: -0.8825886, linearprob: 41.37 Token:  document, logprob: -2.8018124, linearprob: 6.07 Token:  fly, logprob: -3.5582783, linearprob: 2.85 Token:  lab, logprob: -4.8199797, linearprob: 0.81 Token: ing, logprob: -8.299462e-06, linearprob: 100.0 Token: ed, logprob: -12.58043, linearprob: 0.0 Token:  systems, logprob: -13.204528, linearprob: 0.0 Token: ating, logprob: -13.398288, linearprob: 0.0 Token:  system, logprob: -14.457107, linearprob: 0.0 Token:  systems, logprob: -4.0603656e-05, linearprob: 100.0 Token:  system, logprob: -10.166293, linearprob: 0.0 Token:  of, logprob: -14.454125, linearprob: 0.0 Token:  Systems, logprob: -14.715201, linearprob: 0.0 Token: ,, logprob: -14.833644, linearprob: 0.0 Token: ,, logprob: -0.00019769941, linearprob: 99.98 Token: ., logprob: -8.557906, linearprob: 0.02 Token:  and, logprob: -12.498248, linearprob: 0.0 Token:  document, logprob: -14.6223345, linearprob: 0.0 Token: ,\n, logprob: -15.514884, linearprob: 0.0 Token:  fly, logprob: -0.7433455, linearprob: 47.55 Token:  document, logprob: -0.9914744, linearprob: 37.1 Token:  and, logprob: -2.1896079, linearprob: 11.2 Token:  lab, logprob: -3.2472517, linearprob: 3.89 Token:  participant, logprob: -7.0343475, linearprob: 0.09 Token:  on, logprob: -0.00032974407, linearprob: 99.97 Token: -on, logprob: -8.021748, linearprob: 0.03 Token:  in, logprob: -14.686514, linearprob: 0.0 Token:  the, logprob: -15.186766, linearprob: 0.0 Token: ,, logprob: -15.39748, linearprob: 0.0 Token:  the, logprob: -2.8160932e-06, linearprob: 100.0 Token: -the, logprob: -13.843192, linearprob: 0.0 Token:  a, logprob: -14.252734, linearprob: 0.0 Token:  wall, logprob: -15.093001, linearprob: 0.0 Token: ,, logprob: -15.632146, linearprob: 0.0 Token:  wall, logprob: -6.9882217e-06, linearprob: 100.0 Token:  Wall, logprob: -12.81594, linearprob: 0.0 Token:  walls, logprob: -13.387204, linearprob: 0.0 Token:  wal, logprob: -14.111966, linearprob: 0.0 Token:  ball, logprob: -14.570111, linearprob: 0.0 Token: ,, logprob: -0.00011415232, linearprob: 99.99 Token: ., logprob: -9.195866, linearprob: 0.01 Token:  and, logprob: -12.664337, linearprob: 0.0 Token:  observation, logprob: -12.719412, linearprob: 0.0 Token:  technique, logprob: -12.7631, linearprob: 0.0 Token:  document, logprob: -0.06474984, linearprob: 93.73 Token:  and, logprob: -2.8608198, linearprob: 5.72 Token:  lab, logprob: -5.3891935, linearprob: 0.46 Token:  social, logprob: -7.8872957, linearprob: 0.04 Token:  image, logprob: -7.9994802, linearprob: 0.03 Token:  corpus, logprob: -2.2961513e-05, linearprob: 100.0 Token:  corps, logprob: -11.274202, linearprob: 0.0 Token:  corp, logprob: -12.3546715, linearprob: 0.0 Token:  corpor, logprob: -12.592669, linearprob: 0.0 Token:  cor, logprob: -14.124399, linearprob: 0.0 Token:  analysis, logprob: -1.6240566e-06, linearprob: 100.0 Token: ,, logprob: -14.239632, linearprob: 0.0 Token:  an, logprob: -15.589352, linearprob: 0.0 Token:  analy, logprob: -15.729921, linearprob: 0.0 Token:  Analysis, logprob: -15.814928, linearprob: 0.0 Token: ,, logprob: -0.037272062, linearprob: 96.34 Token: ., logprob: -3.31066, linearprob: 3.65 Token:  and, logprob: -9.697764, linearprob: 0.01 Token: .\n, logprob: -11.061311, linearprob: 0.0 Token: .\n\n, logprob: -11.987623, linearprob: 0.0 Token:  and, logprob: -1.0313997, linearprob: 35.65 Token:  image, logprob: -1.1730146, linearprob: 30.94 Token:  social, logprob: -1.6899137, linearprob: 18.45 Token:  lab, logprob: -1.9100142, linearprob: 14.81 Token:  as, logprob: -7.9138136, linearprob: 0.04 Token:  lab, logprob: -0.0611298, linearprob: 94.07 Token:  image, logprob: -3.417865, linearprob: 3.28 Token:  social, logprob: -3.799783, linearprob: 2.24 Token:  no, logprob: -6.588538, linearprob: 0.14 Token:  laboratory, logprob: -6.765277, linearprob: 0.12 Token:  experiment, logprob: -0.00180513, linearprob: 99.82 Token:  experiments, logprob: -6.320531, linearprob: 0.18 Token:  experimentation, logprob: -12.59472, linearprob: 0.0 Token:  Experiment, logprob: -14.831563, linearprob: 0.0 Token:  exper, logprob: -15.536565, linearprob: 0.0 Token: ., logprob: -0.0008893063, linearprob: 99.91 Token: <|end|>, logprob: -8.156255, linearprob: 0.03 Token:  techniques, logprob: -8.370971, linearprob: 0.02 Token: .\n, logprob: -8.907721, linearprob: 0.01 Token:  methods, logprob: -9.625428, linearprob: 0.01 ","Classification: Data gathering methodology - You are to perform a single or multiclass classification of the provided research paper's data gathering methodology. What are the data gathering methodologies of the provided research paper?Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: the set of this research paper's data gathering methodologies contains brainstorming, focus groups, interviews, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, document corpus analysis, and lab experiment.,"Token: The, logprob: -0.00018744789, linearprob: 99.98 Token:  The, logprob: -9.855252, linearprob: 0.01 Token: Classification, logprob: -10.051017, linearprob: 0.0 Token: According, logprob: -10.794564, linearprob: 0.0 Token: Based, logprob: -11.216041, linearprob: 0.0 Token:  set, logprob: -8.304016e-05, linearprob: 99.99 Token:  data, logprob: -9.552976, linearprob: 0.01 Token:  classification, logprob: -11.672591, linearprob: 0.0 Token:  research, logprob: -12.76995, linearprob: 0.0 Token: set, logprob: -16.31402, linearprob: 0.0 Token:  of, logprob: -1.3856493e-06, linearprob: 100.0 Token:  containing, logprob: -14.169124, linearprob: 0.0 Token:  this, logprob: -15.945569, linearprob: 0.0 Token:  data, logprob: -16.090532, linearprob: 0.0 Token:  for, logprob: -16.165855, linearprob: 0.0 Token:  this, logprob: -0.00023095356, linearprob: 99.98 Token:  data, logprob: -8.391993, linearprob: 0.02 Token:  methodologies, logprob: -12.802162, linearprob: 0.0 Token:  the, logprob: -13.837824, linearprob: 0.0 Token:  research, logprob: -14.950672, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -15.964579, linearprob: 0.0 Token:  res, logprob: -16.685476, linearprob: 0.0 Token:  Research, logprob: -17.517977, linearprob: 0.0 Token: research, logprob: -18.24751, linearprob: 0.0 Token:  paper, logprob: -1.5213274e-05, linearprob: 100.0 Token:  papers, logprob: -11.1339855, linearprob: 0.0 Token:  p, logprob: -15.627979, linearprob: 0.0 Token:  pap, logprob: -15.700199, linearprob: 0.0 Token: 's, logprob: -16.159094, linearprob: 0.0 Token: 's, logprob: -7.107425e-06, linearprob: 100.0 Token: ’s, logprob: -11.896321, linearprob: 0.0 Token: \', logprob: -15.925265, linearprob: 0.0 Token: &#, logprob: -17.793604, linearprob: 0.0 Token: &apos, logprob: -17.854233, linearprob: 0.0 Token:  data, logprob: -5.5122365e-07, linearprob: 100.0 Token:  gathering, logprob: -15.106765, linearprob: 0.0 Token:  Data, logprob: -16.530943, linearprob: 0.0 Token:  , logprob: -16.633675, linearprob: 0.0 Token:  dat, logprob: -18.221231, linearprob: 0.0 Token:  gathering, logprob: -0.00010556965, linearprob: 99.99 Token: -g, logprob: -9.161159, linearprob: 0.01 Token:  g, logprob: -15.758119, linearprob: 0.0 Token:  gather, logprob: -15.876317, linearprob: 0.0 Token: \n, logprob: -16.932741, linearprob: 0.0 Token:  methodologies, logprob: -1.4378848e-05, linearprob: 100.0 Token:  methodology, logprob: -11.591992, linearprob: 0.0 Token:  meth, logprob: -13.623261, linearprob: 0.0 Token:  method, logprob: -13.647042, linearprob: 0.0 Token:  methods, logprob: -13.730546, linearprob: 0.0 Token:  contains, logprob: -6.5278815e-05, linearprob: 99.99 Token:  contain, logprob: -10.101818, linearprob: 0.0 Token:  includes, logprob: -11.040199, linearprob: 0.0 Token:  according, logprob: -13.285042, linearprob: 0.0 Token:  consists, logprob: -13.622499, linearprob: 0.0 Token:  brainstorm, logprob: -0.21989585, linearprob: 80.26 Token:  interviews, logprob: -1.8926063, linearprob: 15.07 Token:  focus, logprob: -4.339397, linearprob: 1.3 Token: :, logprob: -4.9379897, linearprob: 0.72 Token:  think, logprob: -5.049213, linearprob: 0.64 Token: ing, logprob: -4.4849444e-06, linearprob: 100.0 Token: ,, logprob: -13.711663, linearprob: 0.0 Token: ings, logprob: -13.933852, linearprob: 0.0 Token: in, logprob: -14.202085, linearprob: 0.0 Token:  and, logprob: -14.75521, linearprob: 0.0 Token: ,, logprob: -0.073262125, linearprob: 92.94 Token:  and, logprob: -2.651143, linearprob: 7.06 Token:  sessions, logprob: -9.723706, linearprob: 0.01 Token: /f, logprob: -12.507806, linearprob: 0.0 Token: ,f, logprob: -12.860192, linearprob: 0.0 Token:  focus, logprob: -0.43000033, linearprob: 65.05 Token:  interviews, logprob: -1.0726624, linearprob: 34.21 Token:  conceptual, logprob: -5.189152, linearprob: 0.56 Token:  concept, logprob: -6.722824, linearprob: 0.12 Token:  think, logprob: -9.1219425, linearprob: 0.01 Token:  groups, logprob: -3.8338785e-05, linearprob: 100.0 Token:  group, logprob: -10.17284, linearprob: 0.0 Token:  Groups, logprob: -17.423054, linearprob: 0.0 Token: -groups, logprob: -18.301483, linearprob: 0.0 Token: groups, logprob: -18.707043, linearprob: 0.0 Token: ,, logprob: -4.1273333e-06, linearprob: 100.0 Token:  and, logprob: -13.617697, linearprob: 0.0 Token:  interviews, logprob: -13.738491, linearprob: 0.0 Token: ., logprob: -14.15843, linearprob: 0.0 Token:  (, logprob: -14.919228, linearprob: 0.0 Token:  interviews, logprob: -0.03886421, linearprob: 96.19 Token:  conceptual, logprob: -3.5206337, linearprob: 2.96 Token:  participant, logprob: -6.224354, linearprob: 0.2 Token:  concept, logprob: -6.335188, linearprob: 0.18 Token:  think, logprob: -6.5339985, linearprob: 0.15 Token: ,, logprob: -0.0007548354, linearprob: 99.92 Token:  (, logprob: -7.2395453, linearprob: 0.07 Token: ., logprob: -10.550875, linearprob: 0.0 Token:  and, logprob: -12.0085125, linearprob: 0.0 Token:  with, logprob: -12.99114, linearprob: 0.0 Token:  question, logprob: -0.16567494, linearprob: 84.73 Token:  and, logprob: -2.7924068, linearprob: 6.13 Token:  conceptual, logprob: -2.8662117, linearprob: 5.69 Token:  concept, logprob: -4.2337217, linearprob: 1.45 Token:  think, logprob: -4.3395567, linearprob: 1.3 Token: naires, logprob: -1.1994775e-05, linearprob: 100.0 Token: aires, logprob: -11.6163, linearprob: 0.0 Token: n, logprob: -13.614409, linearprob: 0.0 Token: aries, logprob: -14.305951, linearprob: 0.0 Token: na, logprob: -14.676335, linearprob: 0.0 Token: ,, logprob: -0.0003418992, linearprob: 99.97 Token: ., logprob: -8.031293, linearprob: 0.03 Token:  and, logprob: -11.227487, linearprob: 0.0 Token:  as, logprob: -13.679188, linearprob: 0.0 Token:  for, logprob: -15.0478735, linearprob: 0.0 Token:  conceptual, logprob: -0.29461953, linearprob: 74.48 Token:  and, logprob: -1.7333403, linearprob: 17.67 Token:  think, logprob: -2.9911718, linearprob: 5.02 Token:  shadow, logprob: -4.3660707, linearprob: 1.27 Token:  concept, logprob: -5.0920725, linearprob: 0.61 Token:  modeling, logprob: -2.3438328e-05, linearprob: 100.0 Token:  modelling, logprob: -10.681386, linearprob: 0.0 Token:  model, logprob: -15.730203, linearprob: 0.0 Token:  , logprob: -16.33673, linearprob: 0.0 Token: \n, logprob: -16.729935, linearprob: 0.0 Token: ,, logprob: -0.0012813031, linearprob: 99.87 Token: ., logprob: -6.6689944, linearprob: 0.13 Token:  and, logprob: -11.732407, linearprob: 0.0 Token: .\n, logprob: -14.392787, linearprob: 0.0 Token: ,\n, logprob: -15.197033, linearprob: 0.0 Token:  work, logprob: -0.58400893, linearprob: 55.77 Token:  think, logprob: -1.3091562, linearprob: 27.0 Token:  and, logprob: -2.5615327, linearprob: 7.72 Token:  participant, logprob: -3.1115663, linearprob: 4.45 Token:  shadow, logprob: -3.780506, linearprob: 2.28 Token:  di, logprob: -6.704273e-07, linearprob: 100.0 Token:  diary, logprob: -14.528668, linearprob: 0.0 Token:  d, logprob: -15.633534, linearprob: 0.0 Token: di, logprob: -16.982952, linearprob: 0.0 Token:  Di, logprob: -18.519985, linearprob: 0.0 Token: aries, logprob: -2.1008714e-06, linearprob: 100.0 Token: ar, logprob: -14.317543, linearprob: 0.0 Token: aris, logprob: -14.542004, linearprob: 0.0 Token: arie, logprob: -14.950727, linearprob: 0.0 Token: ari, logprob: -15.491732, linearprob: 0.0 Token: ,, logprob: -0.0034898352, linearprob: 99.65 Token: ., logprob: -5.6619716, linearprob: 0.35 Token:  and, logprob: -12.319824, linearprob: 0.0 Token: .\n, logprob: -13.566417, linearprob: 0.0 Token: .\n\n, logprob: -14.914958, linearprob: 0.0 Token:  think, logprob: -0.03686806, linearprob: 96.38 Token:  and, logprob: -3.8139753, linearprob: 2.21 Token:  shadow, logprob: -4.9101133, linearprob: 0.74 Token:  participant, logprob: -5.2994967, linearprob: 0.5 Token:  fly, logprob: -6.986638, linearprob: 0.09 Token: -al, logprob: -2.9352968e-06, linearprob: 100.0 Token:  aloud, logprob: -13.105594, linearprob: 0.0 Token: -, logprob: -14.677323, linearprob: 0.0 Token: ‐, logprob: -15.522023, linearprob: 0.0 Token: \-, logprob: -17.130352, linearprob: 0.0 Token: oud, logprob: -1.7432603e-06, linearprob: 100.0 Token: ound, logprob: -13.468838, linearprob: 0.0 Token: ong, logprob: -15.710224, linearprob: 0.0 Token: ould, logprob: -17.138056, linearprob: 0.0 Token: oad, logprob: -17.467186, linearprob: 0.0 Token:  sessions, logprob: -2.220075e-06, linearprob: 100.0 Token: sessions, logprob: -14.0587635, linearprob: 0.0 Token: s, logprob: -14.659933, linearprob: 0.0 Token:  session, logprob: -14.950151, linearprob: 0.0 Token: ,, logprob: -15.503893, linearprob: 0.0 Token: ,, logprob: -0.03865149, linearprob: 96.21 Token: ., logprob: -3.273919, linearprob: 3.79 Token:  and, logprob: -10.095728, linearprob: 0.0 Token: .\n, logprob: -12.219259, linearprob: 0.0 Token: <|end|>, logprob: -13.355167, linearprob: 0.0 Token:  shadow, logprob: -0.69068825, linearprob: 50.12 Token:  participant, logprob: -1.3628684, linearprob: 25.59 Token:  and, logprob: -2.0321856, linearprob: 13.1 Token:  document, logprob: -2.8612108, linearprob: 5.72 Token:  instrument, logprob: -3.7474852, linearprob: 2.36 Token: ing, logprob: -2.220075e-06, linearprob: 100.0 Token: in, logprob: -14.131231, linearprob: 0.0 Token: ,, logprob: -14.247454, linearprob: 0.0 Token: ings, logprob: -15.631041, linearprob: 0.0 Token: ling, logprob: -15.836128, linearprob: 0.0 Token: ,, logprob: -0.0014247603, linearprob: 99.86 Token: ., logprob: -6.584944, linearprob: 0.14 Token:  participant, logprob: -10.368212, linearprob: 0.0 Token:  and, logprob: -11.885618, linearprob: 0.0 Token: .\n, logprob: -13.853567, linearprob: 0.0 Token:  participant, logprob: -0.22132365, linearprob: 80.15 Token:  and, logprob: -2.3192985, linearprob: 9.83 Token:  document, logprob: -2.7424476, linearprob: 6.44 Token:  instrument, logprob: -3.6635768, linearprob: 2.56 Token:  fly, logprob: -4.9776807, linearprob: 0.69 Token: -, logprob: -2.5345586e-05, linearprob: 100.0 Token:  observation, logprob: -11.105653, linearprob: 0.0 Token:  observer, logprob: -11.687453, linearprob: 0.0 Token: -ob, logprob: -13.556556, linearprob: 0.0 Token:  observations, logprob: -15.634332, linearprob: 0.0 Token: observer, logprob: -0.001534501, linearprob: 99.85 Token: observation, logprob: -6.5003405, linearprob: 0.15 Token: obs, logprob: -11.31837, linearprob: 0.0 Token:  observer, logprob: -12.106124, linearprob: 0.0 Token: observations, logprob: -12.428681, linearprob: 0.0 Token: ,, logprob: -0.0005924708, linearprob: 99.94 Token: ., logprob: -7.901302, linearprob: 0.04 Token:  technique, logprob: -8.92954, linearprob: 0.01 Token:  techniques, logprob: -9.7930155, linearprob: 0.01 Token:  observation, logprob: -11.22786, linearprob: 0.0 Token:  instrument, logprob: -0.5252936, linearprob: 59.14 Token:  and, logprob: -1.2566054, linearprob: 28.46 Token:  document, logprob: -2.5343363, linearprob: 7.93 Token:  fly, logprob: -3.7137988, linearprob: 2.44 Token:  lab, logprob: -4.1752415, linearprob: 1.54 Token: ing, logprob: -4.604148e-06, linearprob: 100.0 Token:  systems, logprob: -13.307883, linearprob: 0.0 Token: ed, logprob: -13.583962, linearprob: 0.0 Token: ating, logprob: -14.1867895, linearprob: 0.0 Token:  system, logprob: -14.257522, linearprob: 0.0 Token:  systems, logprob: -2.7848862e-05, linearprob: 100.0 Token:  system, logprob: -10.538997, linearprob: 0.0 Token:  of, logprob: -14.667563, linearprob: 0.0 Token:  Systems, logprob: -15.544845, linearprob: 0.0 Token:  the, logprob: -15.6018305, linearprob: 0.0 Token: ,, logprob: -0.0031390938, linearprob: 99.69 Token: ., logprob: -5.7686095, linearprob: 0.31 Token:  and, logprob: -12.511297, linearprob: 0.0 Token:  (, logprob: -13.9006815, linearprob: 0.0 Token: .\n, logprob: -13.942022, linearprob: 0.0 Token:  document, logprob: -0.5724202, linearprob: 56.42 Token:  fly, logprob: -1.2450943, linearprob: 28.79 Token:  and, logprob: -2.2465477, linearprob: 10.58 Token:  lab, logprob: -3.195856, linearprob: 4.09 Token:  image, logprob: -8.025417, linearprob: 0.03 Token:  corpus, logprob: -1.2590794e-05, linearprob: 100.0 Token:  corps, logprob: -11.906583, linearprob: 0.0 Token:  corpor, logprob: -12.889409, linearprob: 0.0 Token:  corp, logprob: -12.941966, linearprob: 0.0 Token:  cor, logprob: -14.942277, linearprob: 0.0 Token:  analysis, logprob: -6.704273e-07, linearprob: 100.0 Token: ,, logprob: -15.299058, linearprob: 0.0 Token:  an, logprob: -16.20447, linearprob: 0.0 Token:  analy, logprob: -16.401285, linearprob: 0.0 Token:  Analysis, logprob: -16.632868, linearprob: 0.0 Token: ,, logprob: -0.20246539, linearprob: 81.67 Token: ., logprob: -1.6973995, linearprob: 18.32 Token:  and, logprob: -9.959424, linearprob: 0.0 Token: .\n, logprob: -10.048082, linearprob: 0.0 Token: <|end|>, logprob: -11.657853, linearprob: 0.0 Token:  and, logprob: -0.41924962, linearprob: 65.75 Token:  lab, logprob: -1.8685561, linearprob: 15.43 Token:  image, logprob: -2.231492, linearprob: 10.74 Token:  social, logprob: -2.551692, linearprob: 7.79 Token:  fly, logprob: -6.9700975, linearprob: 0.09 Token:  lab, logprob: -0.025093582, linearprob: 97.52 Token:  social, logprob: -4.377732, linearprob: 1.26 Token:  image, logprob: -4.6234593, linearprob: 0.98 Token:  no, logprob: -7.04209, linearprob: 0.09 Token:  laboratory, logprob: -7.3454804, linearprob: 0.06 Token:  experiment, logprob: -0.0031322055, linearprob: 99.69 Token:  experiments, logprob: -5.768452, linearprob: 0.31 Token:  experimentation, logprob: -12.916, linearprob: 0.0 Token:  Experiment, logprob: -16.11259, linearprob: 0.0 Token:  experimental, logprob: -17.005508, linearprob: 0.0 Token: ., logprob: -0.0013102402, linearprob: 99.87 Token:  techniques, logprob: -7.590149, linearprob: 0.05 Token:  methodologies, logprob: -8.271999, linearprob: 0.03 Token:  methods, logprob: -8.809042, linearprob: 0.01 Token: <|end|>, logprob: -8.841124, linearprob: 0.01 ","Classification: Data gathering methodology - What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence, filling in the [MASK] token with a set of any length containing methodologies that match best. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,DNE: the set of this research paper's data gathering methodologies contains brainstorming, interviews, questionnaires, conceptual modeling, work diaries, think-aloud sessions, shadowing, participant-observer, instrumenting systems, fly on the wall, document corpus analysis, and lab experiment.,"Token: The, logprob: -0.00060164585, linearprob: 99.94 Token: Classification, logprob: -7.8870325, linearprob: 0.04 Token:  The, logprob: -9.318316, linearprob: 0.01 Token: The, logprob: -10.388463, linearprob: 0.0 Token: I, logprob: -10.722621, linearprob: 0.0 Token:  set, logprob: -1.7716551e-05, linearprob: 100.0 Token:  data, logprob: -11.041165, linearprob: 0.0 Token:  research, logprob: -13.804524, linearprob: 0.0 Token:  classification, logprob: -15.432685, linearprob: 0.0 Token:  , logprob: -16.715397, linearprob: 0.0 Token:  of, logprob: -7.89631e-07, linearprob: 100.0 Token:  data, logprob: -14.949645, linearprob: 0.0 Token:  this, logprob: -16.076023, linearprob: 0.0 Token:  for, logprob: -16.365011, linearprob: 0.0 Token:  , logprob: -16.877441, linearprob: 0.0 Token:  this, logprob: -0.00018649426, linearprob: 99.98 Token:  data, logprob: -8.614841, linearprob: 0.02 Token:  the, logprob: -12.64946, linearprob: 0.0 Token:  research, logprob: -13.42082, linearprob: 0.0 Token:  these, logprob: -15.959835, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -15.600645, linearprob: 0.0 Token:  res, logprob: -16.738218, linearprob: 0.0 Token:  Research, logprob: -17.547874, linearprob: 0.0 Token: research, logprob: -17.886496, linearprob: 0.0 Token:  paper, logprob: -2.1769476e-05, linearprob: 100.0 Token:  papers, logprob: -10.769229, linearprob: 0.0 Token:  pap, logprob: -15.494212, linearprob: 0.0 Token:  p, logprob: -15.675546, linearprob: 0.0 Token: 's, logprob: -16.617582, linearprob: 0.0 Token: 's, logprob: -1.7597347e-05, linearprob: 100.0 Token: ’s, logprob: -10.959291, linearprob: 0.0 Token: \', logprob: -16.19449, linearprob: 0.0 Token:  data, logprob: -17.48719, linearprob: 0.0 Token: ‘s, logprob: -17.953785, linearprob: 0.0 Token:  data, logprob: -6.704273e-07, linearprob: 100.0 Token:  gathering, logprob: -14.648791, linearprob: 0.0 Token:  Data, logprob: -16.352541, linearprob: 0.0 Token:  , logprob: -16.999407, linearprob: 0.0 Token:  **, logprob: -18.702223, linearprob: 0.0 Token:  gathering, logprob: -0.0002751598, linearprob: 99.97 Token: -g, logprob: -8.202664, linearprob: 0.03 Token:  g, logprob: -14.734324, linearprob: 0.0 Token:  gather, logprob: -14.835105, linearprob: 0.0 Token:  gathered, logprob: -15.923219, linearprob: 0.0 Token:  methodologies, logprob: -1.6286107e-05, linearprob: 100.0 Token:  methodology, logprob: -11.414723, linearprob: 0.0 Token:  methods, logprob: -13.139686, linearprob: 0.0 Token:  meth, logprob: -13.621675, linearprob: 0.0 Token:  method, logprob: -13.676225, linearprob: 0.0 Token:  contains, logprob: -0.00026968008, linearprob: 99.97 Token:  contain, logprob: -8.543487, linearprob: 0.02 Token:  includes, logprob: -9.720761, linearprob: 0.01 Token:  consists, logprob: -12.179137, linearprob: 0.0 Token:  contained, logprob: -12.409654, linearprob: 0.0 Token:  brainstorm, logprob: -0.31352702, linearprob: 73.09 Token:  demonstration, logprob: -2.5258074, linearprob: 8.0 Token:  demonstrations, logprob: -2.9068022, linearprob: 5.47 Token:  interviews, logprob: -3.128625, linearprob: 4.38 Token:  focus, logprob: -4.119748, linearprob: 1.62 Token: ing, logprob: -4.4849444e-06, linearprob: 100.0 Token: ings, logprob: -13.735204, linearprob: 0.0 Token:  and, logprob: -13.889964, linearprob: 0.0 Token: ,, logprob: -14.024852, linearprob: 0.0 Token: in, logprob: -14.532054, linearprob: 0.0 Token: ,, logprob: -0.08993371, linearprob: 91.4 Token:  and, logprob: -2.456465, linearprob: 8.57 Token:  sessions, logprob: -8.629843, linearprob: 0.02 Token: /f, logprob: -9.863451, linearprob: 0.01 Token:  focus, logprob: -10.705044, linearprob: 0.0 Token:  interviews, logprob: -0.44258448, linearprob: 64.24 Token:  focus, logprob: -1.1020999, linearprob: 33.22 Token:  conceptual, logprob: -3.8849673, linearprob: 2.05 Token:  concept, logprob: -5.8166785, linearprob: 0.3 Token:  participant, logprob: -7.5151973, linearprob: 0.05 Token: ,, logprob: -8.685468e-05, linearprob: 99.99 Token:  and, logprob: -9.535417, linearprob: 0.01 Token: ., logprob: -11.683005, linearprob: 0.0 Token:  (, logprob: -12.872361, linearprob: 0.0 Token:  with, logprob: -13.77144, linearprob: 0.0 Token:  question, logprob: -0.24319327, linearprob: 78.41 Token:  conceptual, logprob: -2.451365, linearprob: 8.62 Token:  think, logprob: -3.2290578, linearprob: 3.96 Token:  concept, logprob: -3.695374, linearprob: 2.48 Token:  shadow, logprob: -3.9101892, linearprob: 2.0 Token: naires, logprob: -1.962381e-05, linearprob: 100.0 Token: aires, logprob: -11.137244, linearprob: 0.0 Token: n, logprob: -13.139092, linearprob: 0.0 Token: aries, logprob: -13.747634, linearprob: 0.0 Token: na, logprob: -14.100472, linearprob: 0.0 Token: ,, logprob: -4.441817e-05, linearprob: 100.0 Token: ., logprob: -10.756795, linearprob: 0.0 Token:  and, logprob: -10.767663, linearprob: 0.0 Token:  concept, logprob: -14.5900545, linearprob: 0.0 Token:  as, logprob: -14.736672, linearprob: 0.0 Token:  conceptual, logprob: -0.35978425, linearprob: 69.78 Token:  and, logprob: -1.3104564, linearprob: 26.97 Token:  think, logprob: -4.1384325, linearprob: 1.59 Token:  shadow, logprob: -5.087987, linearprob: 0.62 Token:  concept, logprob: -5.3598547, linearprob: 0.47 Token:  modeling, logprob: -4.561021e-05, linearprob: 100.0 Token:  modelling, logprob: -10.019236, linearprob: 0.0 Token:  model, logprob: -14.49198, linearprob: 0.0 Token:  , logprob: -15.863344, linearprob: 0.0 Token: \n, logprob: -16.15389, linearprob: 0.0 Token: ,, logprob: -0.00019042798, linearprob: 99.98 Token: ., logprob: -8.659782, linearprob: 0.02 Token:  and, logprob: -11.059969, linearprob: 0.0 Token: ,\n, logprob: -15.653622, linearprob: 0.0 Token: .\n, logprob: -15.73629, linearprob: 0.0 Token:  work, logprob: -0.38457805, linearprob: 68.07 Token:  think, logprob: -1.5602677, linearprob: 21.01 Token:  and, logprob: -3.1284287, linearprob: 4.38 Token:  participant, logprob: -3.7372887, linearprob: 2.38 Token:  shadow, logprob: -4.062841, linearprob: 1.72 Token:  di, logprob: -2.577686e-06, linearprob: 100.0 Token:  diary, logprob: -13.050452, linearprob: 0.0 Token:  d, logprob: -15.146762, linearprob: 0.0 Token: di, logprob: -16.102251, linearprob: 0.0 Token:  Di, logprob: -17.745735, linearprob: 0.0 Token: aries, logprob: -5.080963e-06, linearprob: 100.0 Token: ar, logprob: -13.094754, linearprob: 0.0 Token: aris, logprob: -13.863017, linearprob: 0.0 Token: arie, logprob: -14.023665, linearprob: 0.0 Token: ari, logprob: -14.781249, linearprob: 0.0 Token: ,, logprob: -0.00020890454, linearprob: 99.98 Token: ., logprob: -8.516641, linearprob: 0.02 Token:  and, logprob: -11.781314, linearprob: 0.0 Token:  think, logprob: -14.82968, linearprob: 0.0 Token: ,\n, logprob: -16.053492, linearprob: 0.0 Token:  think, logprob: -0.057017248, linearprob: 94.46 Token:  and, logprob: -3.2390642, linearprob: 3.92 Token:  shadow, logprob: -4.5868425, linearprob: 1.02 Token:  participant, logprob: -5.582078, linearprob: 0.38 Token:  instrument, logprob: -6.715499, linearprob: 0.12 Token: -al, logprob: -5.080963e-06, linearprob: 100.0 Token:  aloud, logprob: -12.520898, linearprob: 0.0 Token: -, logprob: -14.304833, linearprob: 0.0 Token: ‐, logprob: -14.824317, linearprob: 0.0 Token: -out, logprob: -16.198534, linearprob: 0.0 Token: oud, logprob: -8.89548e-06, linearprob: 100.0 Token: ound, logprob: -11.870288, linearprob: 0.0 Token: ong, logprob: -13.461448, linearprob: 0.0 Token: ould, logprob: -15.298808, linearprob: 0.0 Token: ough, logprob: -16.71949, linearprob: 0.0 Token:  sessions, logprob: -3.1737043e-06, linearprob: 100.0 Token:  protocol, logprob: -13.700902, linearprob: 0.0 Token:  session, logprob: -14.090882, linearprob: 0.0 Token: s, logprob: -14.558568, linearprob: 0.0 Token: sessions, logprob: -14.706893, linearprob: 0.0 Token: ,, logprob: -0.004613234, linearprob: 99.54 Token: ., logprob: -5.388597, linearprob: 0.46 Token:  and, logprob: -10.461778, linearprob: 0.0 Token: .\n, logprob: -12.888463, linearprob: 0.0 Token: .\n\n, logprob: -14.402132, linearprob: 0.0 Token:  shadow, logprob: -0.8634928, linearprob: 42.17 Token:  participant, logprob: -1.3033693, linearprob: 27.16 Token:  and, logprob: -1.4521997, linearprob: 23.41 Token:  document, logprob: -3.4703748, linearprob: 3.11 Token:  instrument, logprob: -3.867605, linearprob: 2.09 Token: ing, logprob: -4.00813e-06, linearprob: 100.0 Token: ,, logprob: -13.330259, linearprob: 0.0 Token: in, logprob: -13.7393, linearprob: 0.0 Token: ings, logprob: -14.85817, linearprob: 0.0 Token: ling, logprob: -15.530174, linearprob: 0.0 Token: ,, logprob: -0.0005232465, linearprob: 99.95 Token: ., logprob: -7.6255174, linearprob: 0.05 Token:  participant, logprob: -11.012298, linearprob: 0.0 Token:  and, logprob: -11.065818, linearprob: 0.0 Token: .\n, logprob: -14.271689, linearprob: 0.0 Token:  participant, logprob: -0.53141487, linearprob: 58.78 Token:  and, logprob: -1.3278016, linearprob: 26.51 Token:  instrument, logprob: -2.6373625, linearprob: 7.15 Token:  document, logprob: -2.8486853, linearprob: 5.79 Token:  fly, logprob: -4.3019915, linearprob: 1.35 Token: -, logprob: -3.4405064e-05, linearprob: 100.0 Token:  observer, logprob: -10.971302, linearprob: 0.0 Token:  observation, logprob: -11.159479, linearprob: 0.0 Token: -ob, logprob: -13.108072, linearprob: 0.0 Token: –, logprob: -15.160721, linearprob: 0.0 Token: observer, logprob: -0.00053099106, linearprob: 99.95 Token: observation, logprob: -7.583852, linearprob: 0.05 Token:  observer, logprob: -11.566236, linearprob: 0.0 Token: obs, logprob: -12.275733, linearprob: 0.0 Token: observ, logprob: -12.8373785, linearprob: 0.0 Token: ,, logprob: -0.0008925212, linearprob: 99.91 Token:  technique, logprob: -7.224554, linearprob: 0.07 Token:  techniques, logprob: -9.603003, linearprob: 0.01 Token: ., logprob: -10.425262, linearprob: 0.0 Token:  observation, logprob: -10.452562, linearprob: 0.0 Token:  instrument, logprob: -0.59798545, linearprob: 54.99 Token:  and, logprob: -0.90049094, linearprob: 40.64 Token:  document, logprob: -3.9816315, linearprob: 1.87 Token:  fly, logprob: -4.141652, linearprob: 1.59 Token:  lab, logprob: -5.4876394, linearprob: 0.41 Token: ing, logprob: -1.4259645e-05, linearprob: 100.0 Token:  systems, logprob: -11.990796, linearprob: 0.0 Token: ed, logprob: -12.426761, linearprob: 0.0 Token:  system, logprob: -13.213531, linearprob: 0.0 Token: ating, logprob: -13.3830595, linearprob: 0.0 Token:  systems, logprob: -8.518583e-05, linearprob: 99.99 Token:  system, logprob: -9.427421, linearprob: 0.01 Token:  of, logprob: -13.022126, linearprob: 0.0 Token:  the, logprob: -14.00265, linearprob: 0.0 Token:  Systems, logprob: -14.49261, linearprob: 0.0 Token: ,, logprob: -0.011440464, linearprob: 98.86 Token: ., logprob: -4.4822674, linearprob: 1.13 Token:  and, logprob: -9.760895, linearprob: 0.01 Token: .\n, logprob: -13.06759, linearprob: 0.0 Token:  (, logprob: -13.7653675, linearprob: 0.0 Token:  fly, logprob: -0.4125498, linearprob: 66.2 Token:  and, logprob: -1.7487848, linearprob: 17.4 Token:  document, logprob: -1.8786504, linearprob: 15.28 Token:  lab, logprob: -4.640938, linearprob: 0.96 Token:  participant, logprob: -7.4216003, linearprob: 0.06 Token:  on, logprob: -0.0006809804, linearprob: 99.93 Token: -on, logprob: -7.295677, linearprob: 0.07 Token:  in, logprob: -14.1663, linearprob: 0.0 Token:  the, logprob: -14.489939, linearprob: 0.0 Token:  of, logprob: -15.115166, linearprob: 0.0 Token:  the, logprob: -4.4849444e-06, linearprob: 100.0 Token: -the, logprob: -13.233506, linearprob: 0.0 Token:  a, logprob: -14.091823, linearprob: 0.0 Token:  wall, logprob: -14.092767, linearprob: 0.0 Token: \n, logprob: -14.889426, linearprob: 0.0 Token:  wall, logprob: -8.657073e-06, linearprob: 100.0 Token:  Wall, logprob: -12.5395975, linearprob: 0.0 Token:  walls, logprob: -13.521512, linearprob: 0.0 Token:  wal, logprob: -13.775077, linearprob: 0.0 Token:  ball, logprob: -14.084807, linearprob: 0.0 Token: ,, logprob: -0.00080319535, linearprob: 99.92 Token:  technique, logprob: -7.5228863, linearprob: 0.05 Token:  observation, logprob: -8.987066, linearprob: 0.01 Token: ., logprob: -9.218586, linearprob: 0.01 Token:  method, logprob: -11.289278, linearprob: 0.0 Token:  document, logprob: -0.057266183, linearprob: 94.43 Token:  and, logprob: -2.9544792, linearprob: 5.21 Token:  lab, logprob: -6.011297, linearprob: 0.25 Token:  social, logprob: -7.019598, linearprob: 0.09 Token:  image, logprob: -9.168203, linearprob: 0.01 Token:  corpus, logprob: -1.92662e-05, linearprob: 100.0 Token:  corps, logprob: -11.464277, linearprob: 0.0 Token:  corp, logprob: -12.462133, linearprob: 0.0 Token:  corpor, logprob: -12.674353, linearprob: 0.0 Token:  corpse, logprob: -14.407579, linearprob: 0.0 Token:  analysis, logprob: -2.4584822e-06, linearprob: 100.0 Token: ,, logprob: -13.765725, linearprob: 0.0 Token:  an, logprob: -14.997031, linearprob: 0.0 Token:  Analysis, logprob: -15.582032, linearprob: 0.0 Token:  and, logprob: -15.603234, linearprob: 0.0 Token: ,, logprob: -0.050299007, linearprob: 95.09 Token: ., logprob: -3.0187647, linearprob: 4.89 Token:  and, logprob: -8.771492, linearprob: 0.02 Token: .\n, logprob: -10.713957, linearprob: 0.0 Token: .\n\n, logprob: -12.508819, linearprob: 0.0 Token:  and, logprob: -0.8498124, linearprob: 42.75 Token:  social, logprob: -0.9683827, linearprob: 37.97 Token:  lab, logprob: -2.0241995, linearprob: 13.21 Token:  image, logprob: -2.82125, linearprob: 5.95 Token:  as, logprob: -7.841239, linearprob: 0.04 Token:  lab, logprob: -0.03147467, linearprob: 96.9 Token:  social, logprob: -4.0029635, linearprob: 1.83 Token:  image, logprob: -4.6657863, linearprob: 0.94 Token:  no, logprob: -6.8671565, linearprob: 0.1 Token:  laboratory, logprob: -7.1399283, linearprob: 0.08 Token:  experiment, logprob: -0.002556996, linearprob: 99.74 Token:  experiments, logprob: -5.9718485, linearprob: 0.25 Token:  experimentation, logprob: -12.506028, linearprob: 0.0 Token:  Experiment, logprob: -15.447171, linearprob: 0.0 Token:  experimental, logprob: -16.39299, linearprob: 0.0 Token: ., logprob: -0.0012455897, linearprob: 99.88 Token:  techniques, logprob: -7.6825523, linearprob: 0.05 Token: .\n, logprob: -8.149237, linearprob: 0.03 Token: ,, logprob: -9.083353, linearprob: 0.01 Token:  methods, logprob: -9.72996, linearprob: 0.01 ","Classification: Data gathering methodology - What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,DNE: the set of this research paper's data gathering methodologies contains lab experiment, interviews, and questionnaires.,"Token: The, logprob: -0.00037824165, linearprob: 99.96 Token: Classification, logprob: -8.4941225, linearprob: 0.02 Token:  The, logprob: -10.1554575, linearprob: 0.0 Token: As, logprob: -10.642072, linearprob: 0.0 Token: \n, logprob: -11.073365, linearprob: 0.0 Token:  set, logprob: -2.7133641e-05, linearprob: 100.0 Token:  data, logprob: -10.5881195, linearprob: 0.0 Token:  research, logprob: -13.632021, linearprob: 0.0 Token:  classification, logprob: -14.867374, linearprob: 0.0 Token:  , logprob: -16.889482, linearprob: 0.0 Token:  of, logprob: -3.1281633e-07, linearprob: 100.0 Token:  data, logprob: -15.753044, linearprob: 0.0 Token:  , logprob: -17.111916, linearprob: 0.0 Token:  for, logprob: -17.310434, linearprob: 0.0 Token:  this, logprob: -17.401062, linearprob: 0.0 Token:  this, logprob: -0.00092539575, linearprob: 99.91 Token:  data, logprob: -6.998015, linearprob: 0.09 Token:  the, logprob: -11.869146, linearprob: 0.0 Token:  research, logprob: -13.001993, linearprob: 0.0 Token:  methodologies, logprob: -13.214853, linearprob: 0.0 Token:  research, logprob: -3.1281633e-07, linearprob: 100.0 Token:  re, logprob: -16.663399, linearprob: 0.0 Token:  CH, logprob: -16.679401, linearprob: 0.0 Token:  Research, logprob: -17.088863, linearprob: 0.0 Token:  res, logprob: -17.584808, linearprob: 0.0 Token:  paper, logprob: -2.057744e-05, linearprob: 100.0 Token:  papers, logprob: -10.81646, linearprob: 0.0 Token:  pap, logprob: -16.020893, linearprob: 0.0 Token: 's, logprob: -16.239067, linearprob: 0.0 Token:  p, logprob: -16.267376, linearprob: 0.0 Token: 's, logprob: -3.0113732e-05, linearprob: 100.0 Token: ’s, logprob: -10.42116, linearprob: 0.0 Token: \', logprob: -15.734877, linearprob: 0.0 Token: ‘s, logprob: -17.003092, linearprob: 0.0 Token: ´s, logprob: -17.087667, linearprob: 0.0 Token:  data, logprob: -6.704273e-07, linearprob: 100.0 Token:  gathering, logprob: -14.752791, linearprob: 0.0 Token:  Data, logprob: -15.746369, linearprob: 0.0 Token:  , logprob: -17.060566, linearprob: 0.0 Token:  **, logprob: -18.325472, linearprob: 0.0 Token:  gathering, logprob: -0.00021366903, linearprob: 99.98 Token: -g, logprob: -8.454853, linearprob: 0.02 Token:  gather, logprob: -14.791479, linearprob: 0.0 Token:  g, logprob: -15.680117, linearprob: 0.0 Token:  gathered, logprob: -16.328382, linearprob: 0.0 Token:  methodologies, logprob: -2.4630364e-05, linearprob: 100.0 Token:  methodology, logprob: -10.798824, linearprob: 0.0 Token:  methods, logprob: -13.1612015, linearprob: 0.0 Token:  method, logprob: -13.621529, linearprob: 0.0 Token:  meth, logprob: -14.332678, linearprob: 0.0 Token:  contains, logprob: -0.00039051592, linearprob: 99.96 Token:  includes, logprob: -8.445266, linearprob: 0.02 Token:  contain, logprob: -8.790328, linearprob: 0.02 Token:  consists, logprob: -11.560256, linearprob: 0.0 Token:  contained, logprob: -12.326144, linearprob: 0.0 Token:  lab, logprob: -0.25344312, linearprob: 77.61 Token:  brainstorm, logprob: -2.5130734, linearprob: 8.1 Token:  interviews, logprob: -2.6319432, linearprob: 7.19 Token:  sem, logprob: -3.3762937, linearprob: 3.42 Token:  in, logprob: -3.90445, linearprob: 2.02 Token:  experiment, logprob: -0.17585953, linearprob: 83.87 Token:  study, logprob: -2.0651798, linearprob: 12.68 Token:  experiments, logprob: -3.6085606, linearprob: 2.71 Token: -based, logprob: -5.3096867, linearprob: 0.49 Token:  studies, logprob: -6.047941, linearprob: 0.24 Token: ,, logprob: -0.39308307, linearprob: 67.5 Token:  and, logprob: -1.365108, linearprob: 25.54 Token: ., logprob: -2.726047, linearprob: 6.55 Token:  as, logprob: -6.5601306, linearprob: 0.14 Token:  for, logprob: -7.0606246, linearprob: 0.09 Token:  interviews, logprob: -0.2532743, linearprob: 77.63 Token:  in, logprob: -3.248359, linearprob: 3.88 Token:  interview, logprob: -3.5842106, linearprob: 2.78 Token:  brainstorm, logprob: -3.607999, linearprob: 2.71 Token:  focus, logprob: -3.6733124, linearprob: 2.54 Token: ,, logprob: -0.0022394701, linearprob: 99.78 Token:  (, logprob: -6.4563355, linearprob: 0.16 Token:  and, logprob: -7.4259934, linearprob: 0.06 Token:  with, logprob: -10.523283, linearprob: 0.0 Token: ., logprob: -10.829769, linearprob: 0.0 Token:  and, logprob: -0.37379372, linearprob: 68.81 Token:  question, logprob: -1.9522239, linearprob: 14.2 Token:  think, logprob: -2.1717482, linearprob: 11.4 Token:  focus, logprob: -4.3603315, linearprob: 1.28 Token:  concept, logprob: -4.392804, linearprob: 1.24 Token:  question, logprob: -0.7077351, linearprob: 49.28 Token:  think, logprob: -1.5878506, linearprob: 20.44 Token:  online, logprob: -2.0315628, linearprob: 13.11 Token:  questionnaire, logprob: -2.7498074, linearprob: 6.39 Token:  no, logprob: -3.0613785, linearprob: 4.68 Token: naires, logprob: -3.8100377e-05, linearprob: 100.0 Token: aires, logprob: -10.796406, linearprob: 0.0 Token: n, logprob: -11.689333, linearprob: 0.0 Token: aries, logprob: -12.601826, linearprob: 0.0 Token: na, logprob: -13.149296, linearprob: 0.0 Token: ., logprob: -0.042308785, linearprob: 95.86 Token:  according, logprob: -3.4008217, linearprob: 3.33 Token:  based, logprob: -5.8521843, linearprob: 0.29 Token: ,, logprob: -5.9055443, linearprob: 0.27 Token:  as, logprob: -6.669327, linearprob: 0.13 ","Classification: Data gathering methodology - What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,DNE: the set of this research paper's data gathering methodologies contains lab experiment, interviews, and think-aloud sessions.,"Token: The, logprob: -0.00031079436, linearprob: 99.97 Token: I, logprob: -8.972761, linearprob: 0.01 Token:  The, logprob: -10.023832, linearprob: 0.0 Token: Based, logprob: -10.643513, linearprob: 0.0 Token: The, logprob: -10.978243, linearprob: 0.0 Token:  set, logprob: -4.3941356e-05, linearprob: 100.0 Token:  data, logprob: -10.330673, linearprob: 0.0 Token:  research, logprob: -11.747063, linearprob: 0.0 Token:  classification, logprob: -12.843857, linearprob: 0.0 Token:  gathering, logprob: -16.184973, linearprob: 0.0 Token:  of, logprob: -3.1281633e-07, linearprob: 100.0 Token:  data, logprob: -15.919687, linearprob: 0.0 Token:  , logprob: -16.521372, linearprob: 0.0 Token:  this, logprob: -17.277195, linearprob: 0.0 Token:  for, logprob: -17.453104, linearprob: 0.0 Token:  this, logprob: -0.00014632997, linearprob: 99.99 Token:  data, logprob: -8.872342, linearprob: 0.01 Token:  the, logprob: -12.410604, linearprob: 0.0 Token:  research, logprob: -13.50702, linearprob: 0.0 Token:  methodologies, logprob: -14.9289465, linearprob: 0.0 Token:  research, logprob: -1.9361265e-07, linearprob: 100.0 Token:  re, logprob: -16.510202, linearprob: 0.0 Token:  res, logprob: -17.302164, linearprob: 0.0 Token:  , logprob: -17.791138, linearprob: 0.0 Token:  CH, logprob: -18.356602, linearprob: 0.0 Token:  paper, logprob: -1.9027791e-05, linearprob: 100.0 Token:  papers, logprob: -10.894171, linearprob: 0.0 Token:  pap, logprob: -16.183313, linearprob: 0.0 Token:  p, logprob: -16.219355, linearprob: 0.0 Token: 's, logprob: -16.595932, linearprob: 0.0 Token: 's, logprob: -1.7954959e-05, linearprob: 100.0 Token: ’s, logprob: -10.937863, linearprob: 0.0 Token: \', logprob: -16.050804, linearprob: 0.0 Token: ‘s, logprob: -17.554546, linearprob: 0.0 Token:  data, logprob: -18.140179, linearprob: 0.0 Token:  data, logprob: -6.704273e-07, linearprob: 100.0 Token:  gathering, logprob: -14.675309, linearprob: 0.0 Token:  Data, logprob: -15.801871, linearprob: 0.0 Token:  , logprob: -16.76755, linearprob: 0.0 Token: data, logprob: -18.669022, linearprob: 0.0 Token:  gathering, logprob: -0.0001501445, linearprob: 99.98 Token: -g, logprob: -8.81017, linearprob: 0.01 Token:  gather, logprob: -14.5753355, linearprob: 0.0 Token:  g, logprob: -15.604823, linearprob: 0.0 Token:  , logprob: -16.443033, linearprob: 0.0 Token:  methodologies, logprob: -2.3438328e-05, linearprob: 100.0 Token:  methodology, logprob: -10.941173, linearprob: 0.0 Token:  methods, logprob: -12.882915, linearprob: 0.0 Token:  meth, logprob: -13.650577, linearprob: 0.0 Token:  method, logprob: -13.655435, linearprob: 0.0 Token:  contains, logprob: -0.00032604876, linearprob: 99.97 Token:  includes, logprob: -8.740189, linearprob: 0.02 Token:  contain, logprob: -8.832958, linearprob: 0.01 Token:  consists, logprob: -11.763914, linearprob: 0.0 Token:  according, logprob: -12.398769, linearprob: 0.0 Token:  lab, logprob: -0.33474082, linearprob: 71.55 Token:  interviews, logprob: -1.4918549, linearprob: 22.5 Token:  sem, logprob: -3.7762806, linearprob: 2.29 Token:  brainstorm, logprob: -3.9014008, linearprob: 2.02 Token:  in, logprob: -5.0915575, linearprob: 0.61 Token:  experiment, logprob: -0.07698111, linearprob: 92.59 Token:  study, logprob: -2.6735234, linearprob: 6.9 Token:  experiments, logprob: -5.4493527, linearprob: 0.43 Token: -based, logprob: -7.4645696, linearprob: 0.06 Token:  studies, logprob: -8.577336, linearprob: 0.02 Token: ,, logprob: -0.32868382, linearprob: 71.99 Token:  and, logprob: -1.4203568, linearprob: 24.16 Token: ., logprob: -3.2646465, linearprob: 3.82 Token:  interviews, logprob: -9.563652, linearprob: 0.01 Token:  as, logprob: -10.335171, linearprob: 0.0 Token:  interviews, logprob: -0.14023817, linearprob: 86.92 Token:  no, logprob: -3.9403634, linearprob: 1.94 Token:  interview, logprob: -4.062859, linearprob: 1.72 Token:  in, logprob: -4.1075616, linearprob: 1.64 Token:  semi, logprob: -4.320605, linearprob: 1.33 Token: ,, logprob: -0.0013305949, linearprob: 99.87 Token:  and, logprob: -6.8735304, linearprob: 0.1 Token: ., logprob: -8.77979, linearprob: 0.02 Token:  (, logprob: -8.981759, linearprob: 0.01 Token:  with, logprob: -11.97207, linearprob: 0.0 Token:  and, logprob: -0.25800657, linearprob: 77.26 Token:  question, logprob: -2.1424823, linearprob: 11.74 Token:  think, logprob: -2.3909526, linearprob: 9.15 Token:  concept, logprob: -5.015306, linearprob: 0.66 Token:  questionnaire, logprob: -5.6144576, linearprob: 0.36 Token:  think, logprob: -0.75903773, linearprob: 46.81 Token:  question, logprob: -1.164273, linearprob: 31.21 Token:  questionnaire, logprob: -2.5674026, linearprob: 7.67 Token:  no, logprob: -3.2112224, linearprob: 4.03 Token:  focus, logprob: -3.2943256, linearprob: 3.71 Token: -al, logprob: -2.1173457e-05, linearprob: 100.0 Token:  aloud, logprob: -10.981575, linearprob: 0.0 Token: al, logprob: -13.745605, linearprob: 0.0 Token: ‐, logprob: -13.946812, linearprob: 0.0 Token: -, logprob: -14.218756, linearprob: 0.0 Token: oud, logprob: -5.3193703e-06, linearprob: 100.0 Token: ound, logprob: -12.559111, linearprob: 0.0 Token: ong, logprob: -13.585804, linearprob: 0.0 Token: oad, logprob: -15.402506, linearprob: 0.0 Token: ould, logprob: -15.461714, linearprob: 0.0 Token:  sessions, logprob: -8.8404326e-05, linearprob: 99.99 Token:  protocol, logprob: -9.605823, linearprob: 0.01 Token:  session, logprob: -11.223193, linearprob: 0.0 Token: sessions, logprob: -12.26773, linearprob: 0.0 Token: s, logprob: -13.508279, linearprob: 0.0 Token: ., logprob: -0.0023323556, linearprob: 99.77 Token:  according, logprob: -6.2629333, linearprob: 0.19 Token: ,, logprob: -9.144903, linearprob: 0.01 Token:  based, logprob: -9.153147, linearprob: 0.01 Token:  as, logprob: -9.329851, linearprob: 0.01 ","Classification: Data gathering methodology - What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,DNE: the set of this research paper's data gathering methodologies contains lab experiment, interviews, and think-aloud sessions.,"Token: The, logprob: -0.00026288547, linearprob: 99.97 Token: The, logprob: -9.365938, linearprob: 0.01 Token:  The, logprob: -9.423979, linearprob: 0.01 Token: **, logprob: -10.438, linearprob: 0.0 Token: According, logprob: -11.073683, linearprob: 0.0 Token:  set, logprob: -0.00024358915, linearprob: 99.98 Token:  data, logprob: -8.368054, linearprob: 0.02 Token:  research, logprob: -11.645198, linearprob: 0.0 Token:  classification, logprob: -13.416802, linearprob: 0.0 Token:  paper, logprob: -15.759913, linearprob: 0.0 Token:  of, logprob: -6.704273e-07, linearprob: 100.0 Token:  data, logprob: -15.247651, linearprob: 0.0 Token:  , logprob: -16.19043, linearprob: 0.0 Token:  for, logprob: -16.276642, linearprob: 0.0 Token:  this, logprob: -16.51453, linearprob: 0.0 Token:  this, logprob: -0.00016897499, linearprob: 99.98 Token:  data, logprob: -8.734174, linearprob: 0.02 Token:  research, logprob: -12.605092, linearprob: 0.0 Token:  the, logprob: -12.649657, linearprob: 0.0 Token:  methodologies, logprob: -13.900931, linearprob: 0.0 Token:  research, logprob: -1.9361265e-07, linearprob: 100.0 Token:  re, logprob: -16.493462, linearprob: 0.0 Token:  CH, logprob: -17.665756, linearprob: 0.0 Token:  , logprob: -17.770214, linearprob: 0.0 Token: research, logprob: -17.780415, linearprob: 0.0 Token:  paper, logprob: -2.8802491e-05, linearprob: 100.0 Token:  papers, logprob: -10.472923, linearprob: 0.0 Token:  pap, logprob: -15.636698, linearprob: 0.0 Token:  p, logprob: -16.030735, linearprob: 0.0 Token: 's, logprob: -16.881386, linearprob: 0.0 Token: 's, logprob: -1.9504607e-05, linearprob: 100.0 Token: ’s, logprob: -10.861572, linearprob: 0.0 Token: \', logprob: -15.55715, linearprob: 0.0 Token: ‘s, logprob: -16.918297, linearprob: 0.0 Token:  data, logprob: -17.306469, linearprob: 0.0 Token:  data, logprob: -6.704273e-07, linearprob: 100.0 Token:  gathering, logprob: -14.744147, linearprob: 0.0 Token:  Data, logprob: -15.830852, linearprob: 0.0 Token:  , logprob: -16.335955, linearprob: 0.0 Token: data, logprob: -17.881655, linearprob: 0.0 Token:  gathering, logprob: -0.0001136755, linearprob: 99.99 Token: -g, logprob: -9.095302, linearprob: 0.01 Token:  gather, logprob: -14.49462, linearprob: 0.0 Token:  g, logprob: -14.932027, linearprob: 0.0 Token:  , logprob: -15.62614, linearprob: 0.0 Token:  methodologies, logprob: -2.6060809e-05, linearprob: 100.0 Token:  methodology, logprob: -10.80877, linearprob: 0.0 Token:  methods, logprob: -12.81701, linearprob: 0.0 Token:  method, logprob: -13.827935, linearprob: 0.0 Token:  meth, logprob: -13.987311, linearprob: 0.0 Token:  contains, logprob: -0.0006857486, linearprob: 99.93 Token:  contain, logprob: -7.5254545, linearprob: 0.05 Token:  includes, logprob: -8.995975, linearprob: 0.01 Token:  contained, logprob: -11.833138, linearprob: 0.0 Token:  consists, logprob: -11.901416, linearprob: 0.0 Token:  lab, logprob: -0.41253513, linearprob: 66.2 Token:  interviews, logprob: -1.1344972, linearprob: 32.16 Token:  semi, logprob: -5.5607533, linearprob: 0.38 Token:  sem, logprob: -5.7247357, linearprob: 0.33 Token:  in, logprob: -6.6400495, linearprob: 0.13 Token:  experiment, logprob: -0.021437714, linearprob: 97.88 Token:  study, logprob: -4.2739515, linearprob: 1.39 Token:  experiments, logprob: -4.967143, linearprob: 0.7 Token: -based, logprob: -8.49657, linearprob: 0.02 Token:  studies, logprob: -9.31659, linearprob: 0.01 Token: ,, logprob: -0.45711973, linearprob: 63.31 Token:  and, logprob: -1.1127384, linearprob: 32.87 Token: ., logprob: -3.2734554, linearprob: 3.79 Token:  interviews, logprob: -9.470511, linearprob: 0.01 Token:  for, logprob: -9.8785715, linearprob: 0.01 Token:  interviews, logprob: -0.12585223, linearprob: 88.17 Token:  in, logprob: -3.107555, linearprob: 4.47 Token:  sem, logprob: -4.0306926, linearprob: 1.78 Token:  semi, logprob: -4.23356, linearprob: 1.45 Token:  interview, logprob: -4.669275, linearprob: 0.94 Token: ,, logprob: -0.0018947219, linearprob: 99.81 Token:  and, logprob: -6.6723356, linearprob: 0.13 Token:  (, logprob: -7.5717154, linearprob: 0.05 Token: ., logprob: -9.649486, linearprob: 0.01 Token:  with, logprob: -11.170733, linearprob: 0.0 Token:  and, logprob: -0.22318402, linearprob: 80.0 Token:  think, logprob: -2.260036, linearprob: 10.43 Token:  question, logprob: -2.4789023, linearprob: 8.38 Token:  questionnaire, logprob: -5.7952094, linearprob: 0.3 Token:  concept, logprob: -6.1379867, linearprob: 0.22 Token:  think, logprob: -0.36612937, linearprob: 69.34 Token:  question, logprob: -1.6042275, linearprob: 20.1 Token:  questionnaire, logprob: -3.105845, linearprob: 4.48 Token:  online, logprob: -3.8683515, linearprob: 2.09 Token:  document, logprob: -4.619824, linearprob: 0.99 Token: -al, logprob: -4.143808e-05, linearprob: 100.0 Token:  aloud, logprob: -10.2578535, linearprob: 0.0 Token: al, logprob: -13.230111, linearprob: 0.0 Token: ‐, logprob: -13.409181, linearprob: 0.0 Token: -, logprob: -14.039256, linearprob: 0.0 Token: oud, logprob: -6.6306106e-06, linearprob: 100.0 Token: ound, logprob: -12.410695, linearprob: 0.0 Token: ong, logprob: -13.369021, linearprob: 0.0 Token: oad, logprob: -14.551461, linearprob: 0.0 Token: ould, logprob: -15.204543, linearprob: 0.0 Token:  sessions, logprob: -4.036525e-05, linearprob: 100.0 Token:  protocol, logprob: -10.789654, linearprob: 0.0 Token:  session, logprob: -11.485058, linearprob: 0.0 Token: sessions, logprob: -12.0628395, linearprob: 0.0 Token: s, logprob: -12.976551, linearprob: 0.0 Token: ., logprob: -0.0002518032, linearprob: 99.97 Token: .\n, logprob: -9.492451, linearprob: 0.01 Token: ,, logprob: -9.809835, linearprob: 0.01 Token:  based, logprob: -10.68151, linearprob: 0.0 Token:  as, logprob: -10.692873, linearprob: 0.0 ","What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,DNE: the set of this research paper's data gathering methodologies contains lab experiment, interviews, and think-aloud sessions.,"Token: The, logprob: -0.0002485847, linearprob: 99.98 Token:  The, logprob: -9.090459, linearprob: 0.01 Token: **, logprob: -10.130044, linearprob: 0.0 Token: The, logprob: -10.381667, linearprob: 0.0 Token: ###, logprob: -11.016383, linearprob: 0.0 Token:  set, logprob: -0.00046129356, linearprob: 99.95 Token:  data, logprob: -7.704023, linearprob: 0.05 Token:  research, logprob: -11.90959, linearprob: 0.0 Token:  classification, logprob: -13.200352, linearprob: 0.0 Token:  methodologies, logprob: -15.104832, linearprob: 0.0 Token:  of, logprob: -6.704273e-07, linearprob: 100.0 Token:  data, logprob: -15.172883, linearprob: 0.0 Token:  this, logprob: -16.267162, linearprob: 0.0 Token:  , logprob: -16.355076, linearprob: 0.0 Token:  for, logprob: -16.519978, linearprob: 0.0 Token:  this, logprob: -0.00026073982, linearprob: 99.97 Token:  data, logprob: -8.293366, linearprob: 0.03 Token:  research, logprob: -12.143239, linearprob: 0.0 Token:  the, logprob: -12.496602, linearprob: 0.0 Token:  methodologies, logprob: -13.784047, linearprob: 0.0 Token:  research, logprob: -1.9361265e-07, linearprob: 100.0 Token:  re, logprob: -16.522242, linearprob: 0.0 Token:  CH, logprob: -17.414207, linearprob: 0.0 Token:  , logprob: -17.55877, linearprob: 0.0 Token:  Research, logprob: -17.705816, linearprob: 0.0 Token:  paper, logprob: -2.0219828e-05, linearprob: 100.0 Token:  papers, logprob: -10.828563, linearprob: 0.0 Token:  pap, logprob: -15.897944, linearprob: 0.0 Token:  p, logprob: -16.0527, linearprob: 0.0 Token: 's, logprob: -17.11062, linearprob: 0.0 Token: 's, logprob: -1.7716551e-05, linearprob: 100.0 Token: ’s, logprob: -10.956626, linearprob: 0.0 Token: \', logprob: -15.862075, linearprob: 0.0 Token: ‘s, logprob: -17.054125, linearprob: 0.0 Token: &#, logprob: -17.458868, linearprob: 0.0 Token:  data, logprob: -7.89631e-07, linearprob: 100.0 Token:  gathering, logprob: -14.478328, linearprob: 0.0 Token:  , logprob: -16.010725, linearprob: 0.0 Token:  Data, logprob: -16.450462, linearprob: 0.0 Token: data, logprob: -17.498936, linearprob: 0.0 Token:  gathering, logprob: -8.6497064e-05, linearprob: 99.99 Token: -g, logprob: -9.374585, linearprob: 0.01 Token:  gather, logprob: -14.389924, linearprob: 0.0 Token:  g, logprob: -14.755349, linearprob: 0.0 Token:  , logprob: -15.443238, linearprob: 0.0 Token:  methodologies, logprob: -2.7610455e-05, linearprob: 100.0 Token:  methodology, logprob: -10.74602, linearprob: 0.0 Token:  methods, logprob: -12.904851, linearprob: 0.0 Token:  method, logprob: -13.741929, linearprob: 0.0 Token:  meth, logprob: -13.941539, linearprob: 0.0 Token:  contains, logprob: -0.00092694536, linearprob: 99.91 Token:  contain, logprob: -7.3200474, linearprob: 0.07 Token:  includes, logprob: -8.36887, linearprob: 0.02 Token:  consists, logprob: -11.281305, linearprob: 0.0 Token:  contained, logprob: -11.644094, linearprob: 0.0 Token:  lab, logprob: -0.1452998, linearprob: 86.48 Token:  interviews, logprob: -2.1248684, linearprob: 11.94 Token:  Lab, logprob: -5.6327477, linearprob: 0.36 Token:  semi, logprob: -6.331274, linearprob: 0.18 Token:  sem, logprob: -6.557476, linearprob: 0.14 Token:  experiment, logprob: -0.039543726, linearprob: 96.12 Token:  study, logprob: -3.4392514, linearprob: 3.21 Token:  experiments, logprob: -5.1211705, linearprob: 0.6 Token: -based, logprob: -7.570925, linearprob: 0.05 Token:  studies, logprob: -8.673549, linearprob: 0.02 Token: ,, logprob: -0.34406325, linearprob: 70.89 Token:  and, logprob: -1.3249631, linearprob: 26.58 Token: ., logprob: -3.6892724, linearprob: 2.5 Token:  interviews, logprob: -9.657309, linearprob: 0.01 Token:  with, logprob: -10.257885, linearprob: 0.0 Token:  interviews, logprob: -0.29788944, linearprob: 74.24 Token:  in, logprob: -1.9280717, linearprob: 14.54 Token:  sem, logprob: -2.7950037, linearprob: 6.11 Token:  semi, logprob: -4.2890906, linearprob: 1.37 Token:  interview, logprob: -4.45454, linearprob: 1.16 Token: ,, logprob: -0.0040818434, linearprob: 99.59 Token:  and, logprob: -6.002186, linearprob: 0.25 Token:  (, logprob: -6.8369217, linearprob: 0.11 Token: ., logprob: -7.6607056, linearprob: 0.05 Token:  with, logprob: -10.875305, linearprob: 0.0 Token:  and, logprob: -0.1286832, linearprob: 87.93 Token:  think, logprob: -2.3635387, linearprob: 9.41 Token:  question, logprob: -4.004421, linearprob: 1.82 Token:  questionnaire, logprob: -5.9767494, linearprob: 0.25 Token:  focus, logprob: -6.3207436, linearprob: 0.18 Token:  think, logprob: -0.3730957, linearprob: 68.86 Token:  question, logprob: -1.7860501, linearprob: 16.76 Token:  questionnaire, logprob: -2.7361062, linearprob: 6.48 Token:  document, logprob: -3.4904835, linearprob: 3.05 Token:  online, logprob: -4.290267, linearprob: 1.37 Token: -al, logprob: -5.0616763e-05, linearprob: 99.99 Token:  aloud, logprob: -10.025893, linearprob: 0.0 Token: al, logprob: -13.563554, linearprob: 0.0 Token: ‐, logprob: -13.626002, linearprob: 0.0 Token: -, logprob: -13.660266, linearprob: 0.0 Token: oud, logprob: -5.080963e-06, linearprob: 100.0 Token: ound, logprob: -12.771686, linearprob: 0.0 Token: ong, logprob: -13.392407, linearprob: 0.0 Token: oad, logprob: -14.861108, linearprob: 0.0 Token: ould, logprob: -15.470487, linearprob: 0.0 Token:  sessions, logprob: -7.1954215e-05, linearprob: 99.99 Token:  protocol, logprob: -10.030103, linearprob: 0.0 Token:  session, logprob: -11.334283, linearprob: 0.0 Token: sessions, logprob: -11.678723, linearprob: 0.0 Token: s, logprob: -12.180835, linearprob: 0.0 Token: ., logprob: -0.00016980943, linearprob: 99.98 Token: .\n, logprob: -9.869455, linearprob: 0.01 Token: ,, logprob: -10.369977, linearprob: 0.0 Token:  as, logprob: -11.027985, linearprob: 0.0 Token: <|end|>, logprob: -11.259368, linearprob: 0.0 ","What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,DNE: the set of this research paper's data gathering methodologies contains interviews, lab experiment, and think-aloud sessions.,"Token: The, logprob: -0.00019316968, linearprob: 99.98 Token:  The, logprob: -9.580476, linearprob: 0.01 Token: **, logprob: -10.238521, linearprob: 0.0 Token: The, logprob: -10.541766, linearprob: 0.0 Token: Based, logprob: -11.255034, linearprob: 0.0 Token:  set, logprob: -0.00039194638, linearprob: 99.96 Token:  data, logprob: -7.8591795, linearprob: 0.04 Token:  research, logprob: -12.721175, linearprob: 0.0 Token:  classification, logprob: -13.754308, linearprob: 0.0 Token:  methodologies, logprob: -15.660377, linearprob: 0.0 Token:  of, logprob: -4.3202e-07, linearprob: 100.0 Token:  data, logprob: -15.064236, linearprob: 0.0 Token:  , logprob: -16.14509, linearprob: 0.0 Token:  this, logprob: -16.786367, linearprob: 0.0 Token:  for, logprob: -16.904943, linearprob: 0.0 Token:  this, logprob: -0.00016802136, linearprob: 99.98 Token:  data, logprob: -8.72554, linearprob: 0.02 Token:  the, logprob: -12.930838, linearprob: 0.0 Token:  research, logprob: -13.002632, linearprob: 0.0 Token:  methodologies, logprob: -14.159559, linearprob: 0.0 Token:  research, logprob: -1.9361265e-07, linearprob: 100.0 Token:  re, logprob: -16.698814, linearprob: 0.0 Token:  CH, logprob: -16.774853, linearprob: 0.0 Token:  , logprob: -17.912657, linearprob: 0.0 Token:  res, logprob: -17.950861, linearprob: 0.0 Token:  paper, logprob: -2.808727e-05, linearprob: 100.0 Token:  papers, logprob: -10.5047865, linearprob: 0.0 Token:  pap, logprob: -15.707948, linearprob: 0.0 Token:  p, logprob: -15.78765, linearprob: 0.0 Token: 's, logprob: -16.946241, linearprob: 0.0 Token: 's, logprob: -2.1769476e-05, linearprob: 100.0 Token: ’s, logprob: -10.748805, linearprob: 0.0 Token: \', logprob: -15.7501745, linearprob: 0.0 Token: ‘s, logprob: -16.693693, linearprob: 0.0 Token:  data, logprob: -17.24805, linearprob: 0.0 Token:  data, logprob: -9.0883464e-07, linearprob: 100.0 Token:  gathering, logprob: -14.180615, linearprob: 0.0 Token:  , logprob: -16.161049, linearprob: 0.0 Token:  Data, logprob: -16.961708, linearprob: 0.0 Token: data, logprob: -17.757732, linearprob: 0.0 Token:  gathering, logprob: -0.000102708764, linearprob: 99.99 Token: -g, logprob: -9.192682, linearprob: 0.01 Token:  g, logprob: -15.042957, linearprob: 0.0 Token:  gather, logprob: -15.208601, linearprob: 0.0 Token:  , logprob: -15.937992, linearprob: 0.0 Token:  methodologies, logprob: -2.3676734e-05, linearprob: 100.0 Token:  methodology, logprob: -10.913533, linearprob: 0.0 Token:  methods, logprob: -13.041057, linearprob: 0.0 Token:  method, logprob: -13.655757, linearprob: 0.0 Token:  meth, logprob: -13.8577795, linearprob: 0.0 Token:  contains, logprob: -0.0006447646, linearprob: 99.94 Token:  contain, logprob: -7.6150665, linearprob: 0.05 Token:  includes, logprob: -8.952665, linearprob: 0.01 Token:  consists, logprob: -11.7272625, linearprob: 0.0 Token:  contained, logprob: -12.0172825, linearprob: 0.0 Token:  interviews, logprob: -0.63385916, linearprob: 53.05 Token:  lab, logprob: -0.80035543, linearprob: 44.92 Token:  focus, logprob: -5.3853927, linearprob: 0.46 Token:  semi, logprob: -5.4718127, linearprob: 0.42 Token:  Interviews, logprob: -6.5605845, linearprob: 0.14 Token: ,, logprob: -0.5966279, linearprob: 55.07 Token:  and, logprob: -0.85775536, linearprob: 42.41 Token: ., logprob: -3.7040231, linearprob: 2.46 Token:  lab, logprob: -9.234142, linearprob: 0.01 Token:  (, logprob: -9.377159, linearprob: 0.01 Token:  lab, logprob: -0.2707413, linearprob: 76.28 Token:  think, logprob: -1.992339, linearprob: 13.64 Token:  online, logprob: -3.7194374, linearprob: 2.42 Token:  focus, logprob: -3.9465568, linearprob: 1.93 Token:  conceptual, logprob: -4.045451, linearprob: 1.75 Token:  experiment, logprob: -0.05309086, linearprob: 94.83 Token:  experiments, logprob: -2.9633424, linearprob: 5.16 Token:  study, logprob: -10.599148, linearprob: 0.0 Token:  experimentation, logprob: -11.021528, linearprob: 0.0 Token: -based, logprob: -11.342724, linearprob: 0.0 Token: ,, logprob: -0.082473256, linearprob: 92.08 Token: ., logprob: -2.5695891, linearprob: 7.66 Token:  and, logprob: -5.9788184, linearprob: 0.25 Token: .\n, logprob: -10.831375, linearprob: 0.0 Token: .\n\n, logprob: -12.266518, linearprob: 0.0 Token:  and, logprob: -0.05568394, linearprob: 94.58 Token:  think, logprob: -3.5603168, linearprob: 2.84 Token:  document, logprob: -5.1355886, linearprob: 0.59 Token:  question, logprob: -5.21852, linearprob: 0.54 Token:  questionnaire, logprob: -5.6946135, linearprob: 0.34 Token:  think, logprob: -0.42303067, linearprob: 65.51 Token:  no, logprob: -2.5017755, linearprob: 8.19 Token:  document, logprob: -2.9292028, linearprob: 5.34 Token:  question, logprob: -3.0081651, linearprob: 4.94 Token:  questionnaire, logprob: -3.184591, linearprob: 4.14 Token: -al, logprob: -4.1318875e-05, linearprob: 100.0 Token:  aloud, logprob: -10.231711, linearprob: 0.0 Token: al, logprob: -13.670531, linearprob: 0.0 Token: ‐, logprob: -13.864247, linearprob: 0.0 Token: -, logprob: -14.029147, linearprob: 0.0 Token: oud, logprob: -4.4849444e-06, linearprob: 100.0 Token: ound, logprob: -12.961654, linearprob: 0.0 Token: ong, logprob: -13.630839, linearprob: 0.0 Token: oad, logprob: -14.295987, linearprob: 0.0 Token: ould, logprob: -15.460547, linearprob: 0.0 Token:  sessions, logprob: -4.406056e-05, linearprob: 100.0 Token:  session, logprob: -10.828985, linearprob: 0.0 Token:  protocol, logprob: -11.0846405, linearprob: 0.0 Token: sessions, logprob: -12.040581, linearprob: 0.0 Token: s, logprob: -13.353439, linearprob: 0.0 Token: ., logprob: -0.000102827966, linearprob: 99.99 Token: .\n, logprob: -10.189001, linearprob: 0.0 Token: ,, logprob: -10.770834, linearprob: 0.0 Token: <|end|>, logprob: -11.416625, linearprob: 0.0 Token:  according, logprob: -11.949203, linearprob: 0.0 ","What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 6250,2018,'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,"interviews,think-aloud sessions,lab experiment,","Token: The, logprob: -0.0002576442, linearprob: 99.97 Token:  The, logprob: -9.477495, linearprob: 0.01 Token: The, logprob: -9.678541, linearprob: 0.01 Token: **, logprob: -10.079332, linearprob: 0.0 Token: Based, logprob: -10.862463, linearprob: 0.0 Token:  set, logprob: -0.00026944166, linearprob: 99.97 Token:  data, logprob: -8.232327, linearprob: 0.03 Token:  research, logprob: -13.306272, linearprob: 0.0 Token:  classification, logprob: -13.919713, linearprob: 0.0 Token:  dataset, logprob: -15.956475, linearprob: 0.0 Token:  of, logprob: -7.89631e-07, linearprob: 100.0 Token:  data, logprob: -14.752984, linearprob: 0.0 Token:  for, logprob: -15.910308, linearprob: 0.0 Token:  this, logprob: -15.957828, linearprob: 0.0 Token:  , logprob: -16.244776, linearprob: 0.0 Token:  this, logprob: -0.000101993544, linearprob: 99.99 Token:  data, logprob: -9.244124, linearprob: 0.01 Token:  the, logprob: -12.908307, linearprob: 0.0 Token:  research, logprob: -13.014806, linearprob: 0.0 Token:  methodologies, logprob: -14.773971, linearprob: 0.0 Token:  research, logprob: -1.9361265e-07, linearprob: 100.0 Token:  re, logprob: -16.588478, linearprob: 0.0 Token:  CH, logprob: -17.546112, linearprob: 0.0 Token:  res, logprob: -17.91896, linearprob: 0.0 Token:  , logprob: -18.03468, linearprob: 0.0 Token:  paper, logprob: -2.4272753e-05, linearprob: 100.0 Token:  papers, logprob: -10.643319, linearprob: 0.0 Token:  pap, logprob: -15.998403, linearprob: 0.0 Token:  p, logprob: -16.126413, linearprob: 0.0 Token: 's, logprob: -17.30325, linearprob: 0.0 Token: 's, logprob: -1.6762922e-05, linearprob: 100.0 Token: ’s, logprob: -11.009575, linearprob: 0.0 Token: \', logprob: -16.259314, linearprob: 0.0 Token: ‘s, logprob: -17.187538, linearprob: 0.0 Token:  data, logprob: -17.442728, linearprob: 0.0 Token:  data, logprob: -9.0883464e-07, linearprob: 100.0 Token:  gathering, logprob: -14.234689, linearprob: 0.0 Token:  , logprob: -16.398369, linearprob: 0.0 Token:  Data, logprob: -17.082884, linearprob: 0.0 Token: data, logprob: -17.784147, linearprob: 0.0 Token:  gathering, logprob: -7.481511e-05, linearprob: 99.99 Token: -g, logprob: -9.511145, linearprob: 0.01 Token:  g, logprob: -15.154253, linearprob: 0.0 Token:  gather, logprob: -15.193769, linearprob: 0.0 Token:  , logprob: -16.28033, linearprob: 0.0 Token:  methodologies, logprob: -3.5716304e-05, linearprob: 100.0 Token:  methodology, logprob: -10.424001, linearprob: 0.0 Token:  methods, logprob: -12.915385, linearprob: 0.0 Token:  method, logprob: -13.498811, linearprob: 0.0 Token:  meth, logprob: -13.86667, linearprob: 0.0 Token:  contains, logprob: -0.0005966393, linearprob: 99.94 Token:  contain, logprob: -7.6267543, linearprob: 0.05 Token:  includes, logprob: -9.304272, linearprob: 0.01 Token:  contained, logprob: -11.99128, linearprob: 0.0 Token:  consists, logprob: -12.072271, linearprob: 0.0 Token:  lab, logprob: -0.31031066, linearprob: 73.32 Token:  interviews, logprob: -1.3800395, linearprob: 25.16 Token:  focus, logprob: -5.9453397, linearprob: 0.26 Token:  semi, logprob: -5.9799485, linearprob: 0.25 Token:  Lab, logprob: -6.462992, linearprob: 0.16 Token:  experiment, logprob: -0.013466538, linearprob: 98.66 Token:  experiments, logprob: -4.8224187, linearprob: 0.8 Token:  study, logprob: -5.2750134, linearprob: 0.51 Token: -based, logprob: -8.93233, linearprob: 0.01 Token:  studies, logprob: -9.727168, linearprob: 0.01 Token: ,, logprob: -0.3198322, linearprob: 72.63 Token:  and, logprob: -1.3768123, linearprob: 25.24 Token: ., logprob: -3.863894, linearprob: 2.1 Token:  with, logprob: -9.542185, linearprob: 0.01 Token:  interviews, logprob: -9.778078, linearprob: 0.01 Token:  interviews, logprob: -0.19312769, linearprob: 82.44 Token:  in, logprob: -2.6098146, linearprob: 7.35 Token:  sem, logprob: -3.349536, linearprob: 3.51 Token:  semi, logprob: -4.084008, linearprob: 1.68 Token:  interview, logprob: -4.46824, linearprob: 1.15 Token: ,, logprob: -0.0019876366, linearprob: 99.8 Token:  and, logprob: -6.541176, linearprob: 0.14 Token:  (, logprob: -7.9883537, linearprob: 0.03 Token: ., logprob: -8.703878, linearprob: 0.02 Token:  with, logprob: -10.788164, linearprob: 0.0 Token:  and, logprob: -0.19458316, linearprob: 82.32 Token:  think, logprob: -2.1630223, linearprob: 11.5 Token:  question, logprob: -3.1782715, linearprob: 4.17 Token:  focus, logprob: -5.0336447, linearprob: 0.65 Token:  questionnaire, logprob: -5.370694, linearprob: 0.47 Token:  think, logprob: -0.4288123, linearprob: 65.13 Token:  question, logprob: -1.7740958, linearprob: 16.96 Token:  questionnaire, logprob: -2.4999602, linearprob: 8.21 Token:  focus, logprob: -3.6731837, linearprob: 2.54 Token:  document, logprob: -3.8609812, linearprob: 2.1 Token: -al, logprob: -4.048445e-05, linearprob: 100.0 Token:  aloud, logprob: -10.234898, linearprob: 0.0 Token: ‐, logprob: -13.888086, linearprob: 0.0 Token: -, logprob: -13.908285, linearprob: 0.0 Token: al, logprob: -14.146179, linearprob: 0.0 Token: oud, logprob: -4.604148e-06, linearprob: 100.0 Token: ound, logprob: -12.941539, linearprob: 0.0 Token: ong, logprob: -13.459176, linearprob: 0.0 Token: oad, logprob: -14.646457, linearprob: 0.0 Token: ould, logprob: -15.699222, linearprob: 0.0 Token:  sessions, logprob: -2.2246291e-05, linearprob: 100.0 Token:  protocol, logprob: -11.631455, linearprob: 0.0 Token:  session, logprob: -12.037852, linearprob: 0.0 Token: sessions, logprob: -12.265092, linearprob: 0.0 Token: s, logprob: -13.165887, linearprob: 0.0 Token: ., logprob: -0.00016229959, linearprob: 99.98 Token: .\n, logprob: -10.162092, linearprob: 0.0 Token: ,, logprob: -10.290573, linearprob: 0.0 Token:  based, logprob: -10.885117, linearprob: 0.0 Token:  according, logprob: -10.984713, linearprob: 0.0 ","What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 882,1996,Pavlov: programming by stimulus-response demonstration,"interviews,conceptual modeling,think-aloud sessions,document corpus analysis,","Token: The, logprob: -0.0002385826, linearprob: 99.98 Token:  The, logprob: -9.348253, linearprob: 0.01 Token: The, logprob: -9.50362, linearprob: 0.01 Token: **, logprob: -11.148075, linearprob: 0.0 Token: Based, logprob: -11.23662, linearprob: 0.0 Token:  set, logprob: -0.00046522732, linearprob: 99.95 Token:  data, logprob: -7.6909313, linearprob: 0.05 Token:  research, logprob: -12.671261, linearprob: 0.0 Token:  classification, logprob: -12.733007, linearprob: 0.0 Token:  , logprob: -15.43144, linearprob: 0.0 Token:  of, logprob: -1.6240566e-06, linearprob: 100.0 Token:  data, logprob: -14.228437, linearprob: 0.0 Token:  this, logprob: -15.308418, linearprob: 0.0 Token:  for, logprob: -15.415419, linearprob: 0.0 Token:  , logprob: -15.567278, linearprob: 0.0 Token:  this, logprob: -0.000232384, linearprob: 99.98 Token:  data, logprob: -8.408356, linearprob: 0.02 Token:  the, logprob: -11.838282, linearprob: 0.0 Token:  research, logprob: -13.457401, linearprob: 0.0 Token:  methodologies, logprob: -15.025667, linearprob: 0.0 Token:  research, logprob: -4.3202e-07, linearprob: 100.0 Token:  re, logprob: -15.777418, linearprob: 0.0 Token:  res, logprob: -16.588985, linearprob: 0.0 Token:  Research, logprob: -16.657566, linearprob: 0.0 Token:  , logprob: -17.247215, linearprob: 0.0 Token:  paper, logprob: -3.5477897e-05, linearprob: 100.0 Token:  papers, logprob: -10.270541, linearprob: 0.0 Token:  p, logprob: -15.207261, linearprob: 0.0 Token:  pap, logprob: -15.322989, linearprob: 0.0 Token:  data, logprob: -16.236868, linearprob: 0.0 Token: 's, logprob: -2.4391957e-05, linearprob: 100.0 Token: ’s, logprob: -10.645088, linearprob: 0.0 Token: \', logprob: -15.393166, linearprob: 0.0 Token:  data, logprob: -15.834394, linearprob: 0.0 Token: ´s, logprob: -16.522825, linearprob: 0.0 Token:  data, logprob: -1.7432603e-06, linearprob: 100.0 Token:  gathering, logprob: -13.620771, linearprob: 0.0 Token:  , logprob: -15.64526, linearprob: 0.0 Token:  Data, logprob: -16.396114, linearprob: 0.0 Token: data, logprob: -17.622131, linearprob: 0.0 Token:  gathering, logprob: -0.00012296606, linearprob: 99.99 Token: -g, logprob: -9.015492, linearprob: 0.01 Token:  g, logprob: -14.291194, linearprob: 0.0 Token:  gather, logprob: -14.842061, linearprob: 0.0 Token:  gathered, logprob: -15.723672, linearprob: 0.0 Token:  methodologies, logprob: -5.407367e-05, linearprob: 99.99 Token:  methodology, logprob: -10.00101, linearprob: 0.0 Token:  methods, logprob: -12.722787, linearprob: 0.0 Token:  meth, logprob: -13.216588, linearprob: 0.0 Token:  method, logprob: -13.302724, linearprob: 0.0 Token:  contains, logprob: -0.00075685815, linearprob: 99.92 Token:  contain, logprob: -7.3941455, linearprob: 0.06 Token:  includes, logprob: -9.072573, linearprob: 0.01 Token:  contained, logprob: -11.644482, linearprob: 0.0 Token:  consists, logprob: -11.862097, linearprob: 0.0 Token:  conceptual, logprob: -0.8291464, linearprob: 43.64 Token:  interviews, logprob: -2.1048765, linearprob: 12.19 Token:  brainstorm, logprob: -2.119317, linearprob: 12.01 Token:  concept, logprob: -2.2986765, linearprob: 10.04 Token:  think, logprob: -3.2016535, linearprob: 4.07 Token:  modeling, logprob: -9.388769e-05, linearprob: 99.99 Token:  modelling, logprob: -9.321581, linearprob: 0.01 Token:  model, logprob: -12.683804, linearprob: 0.0 Token:  , logprob: -15.002068, linearprob: 0.0 Token:  Modeling, logprob: -15.138971, linearprob: 0.0 Token: ,, logprob: -0.14982145, linearprob: 86.09 Token:  and, logprob: -2.083646, linearprob: 12.45 Token: ., logprob: -4.234992, linearprob: 1.45 Token:  as, logprob: -9.6829195, linearprob: 0.01 Token:  for, logprob: -10.331946, linearprob: 0.0 Token:  interviews, logprob: -0.921645, linearprob: 39.79 Token:  instrument, logprob: -1.264777, linearprob: 28.23 Token:  think, logprob: -1.63922, linearprob: 19.41 Token:  participant, logprob: -3.0377662, linearprob: 4.79 Token:  shadow, logprob: -3.0503242, linearprob: 4.73 Token: ,, logprob: -0.00068431813, linearprob: 99.93 Token:  and, logprob: -7.642156, linearprob: 0.05 Token: ., logprob: -8.727041, linearprob: 0.02 Token:  with, logprob: -10.603735, linearprob: 0.0 Token:  (, logprob: -11.253815, linearprob: 0.0 Token:  think, logprob: -0.9114212, linearprob: 40.2 Token:  and, logprob: -1.095816, linearprob: 33.43 Token:  instrument, logprob: -2.2956567, linearprob: 10.07 Token:  shadow, logprob: -2.4471898, linearprob: 8.65 Token:  work, logprob: -3.4765038, linearprob: 3.09 Token: -al, logprob: -9.729906e-06, linearprob: 100.0 Token:  aloud, logprob: -11.685438, linearprob: 0.0 Token: -, logprob: -14.551922, linearprob: 0.0 Token: -out, logprob: -15.267723, linearprob: 0.0 Token: ‐, logprob: -15.31716, linearprob: 0.0 Token: oud, logprob: -2.8160932e-06, linearprob: 100.0 Token: ound, logprob: -13.103358, linearprob: 0.0 Token: ong, logprob: -14.664058, linearprob: 0.0 Token: ould, logprob: -15.905591, linearprob: 0.0 Token: oad, logprob: -16.25896, linearprob: 0.0 Token:  sessions, logprob: -2.1769476e-05, linearprob: 100.0 Token:  session, logprob: -11.674005, linearprob: 0.0 Token:  protocol, logprob: -11.7537565, linearprob: 0.0 Token: s, logprob: -13.133653, linearprob: 0.0 Token: sessions, logprob: -13.245059, linearprob: 0.0 Token: ,, logprob: -0.12040341, linearprob: 88.66 Token: ., logprob: -2.1821918, linearprob: 11.28 Token:  and, logprob: -7.47246, linearprob: 0.06 Token: .\n, logprob: -9.972006, linearprob: 0.0 Token:  as, logprob: -12.395186, linearprob: 0.0 Token:  and, logprob: -0.21660063, linearprob: 80.53 Token:  instrument, logprob: -2.7389357, linearprob: 6.46 Token:  participant, logprob: -2.8233378, linearprob: 5.94 Token:  shadow, logprob: -3.4589674, linearprob: 3.15 Token:  document, logprob: -3.7081206, linearprob: 2.45 Token:  document, logprob: -0.64247316, linearprob: 52.6 Token:  instrument, logprob: -1.7955875, linearprob: 16.6 Token:  lab, logprob: -1.84449, linearprob: 15.81 Token:  participant, logprob: -2.6693058, linearprob: 6.93 Token:  shadow, logprob: -3.268715, linearprob: 3.81 Token:  corpus, logprob: -1.6524515e-05, linearprob: 100.0 Token:  corps, logprob: -12.141503, linearprob: 0.0 Token:  corpor, logprob: -12.318444, linearprob: 0.0 Token:  corp, logprob: -12.326724, linearprob: 0.0 Token:  cor, logprob: -13.622699, linearprob: 0.0 Token:  analysis, logprob: -1.2664457e-06, linearprob: 100.0 Token:  analyses, logprob: -14.843135, linearprob: 0.0 Token:  Analysis, logprob: -15.509967, linearprob: 0.0 Token:  an, logprob: -15.618945, linearprob: 0.0 Token: ., logprob: -15.725184, linearprob: 0.0 Token: ., logprob: -0.00023631772, linearprob: 99.98 Token: .\n, logprob: -9.190504, linearprob: 0.01 Token: ,, logprob: -10.23308, linearprob: 0.0 Token:  according, logprob: -10.2801485, linearprob: 0.0 Token:  based, logprob: -11.353987, linearprob: 0.0 ","What are the data gathering methodologies of this research paper according to our taxonomy?Answer in the format of the following sentence. The set of this research paper's data gathering methodologies contains [MASK]."
 